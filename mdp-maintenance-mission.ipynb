{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previous work (scroll down and start below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug(i, j, k, K, n, t, v):\n",
    "    first = \"i: {}\\tj: {}\\tn: {}\\tT(k,j,n): {}\\tV(k): {}\".format(i, j, n, t, v)\n",
    "    others = \"T(k,j,n): {}\\tV(k): {}\".format(t, v)\n",
    "    if(k == 0):\n",
    "        print(first, end=\"\")\n",
    "    else:\n",
    "        print(others, end=\"\")\n",
    "    if(k != K):\n",
    "        print(\" +\\t\", end=\"\")\n",
    "    else:\n",
    "        print(\"\")\n",
    "\n",
    "def value_iteration(states, actions, T, rewards, gamma, beta, values=[], verbose=False):\n",
    "    \"\"\"\n",
    "        @brief: implements Bellman's equation to assign the max expected state values\n",
    "        \n",
    "        @input:\n",
    "            states: a list of states (length is j), type should be string if verbose=True i.e. [\"s1\", \"s2\", \"living room\" ... \"etc\"]\n",
    "            actions: a list of actions (length is n), type should be string if verbose=True i.e. [\"a1\", \"a2\", \"take picture\" ... \"etc\"]\n",
    "            T: a jxjxn matrix of transition probabilities P(Sj | Si, An) for all Sj, Si, and An, j=i is valid (self transitions)\n",
    "            rewards: an array of length j specifying the reward for each state. The reward value never changes.\n",
    "            gamma: the discount factor, i.e. how much do we care about future rewards\n",
    "            beta: the stopping criteria, i.e. minimum amount of improvement between iterations before stopping\n",
    "            values: [optional] current values of the states, set to the states reward value if not supplied\n",
    "            verbose: [optional] set to false \n",
    "        \n",
    "        @output:\n",
    "            values: an array of state values where V(S(i)) = V[i]\n",
    "            policy: a dictionary mapping states as keys to the best action as values, i.e. policy[\"some state\"] -> \"some action\"\n",
    "    \"\"\"\n",
    "    if(len(values) == 0):\n",
    "        values = rewards[:]\n",
    "    old_values = np.ones(len(rewards))*-999\n",
    "    #stopping_criteria = .005\n",
    "    policy = dict.fromkeys(states, \"\")\n",
    "    i = 0\n",
    "    while(abs(sum(values) - sum(old_values)) > beta):\n",
    "        old_values = values[:]\n",
    "        for j in range(0, len(states)): # all states in S\n",
    "            vals = np.zeros(len(actions))\n",
    "            for n in range(0, len(actions)): # valid actions in Sj\n",
    "                assert vals[n] == 0\n",
    "                for k in range(0, len(states)): # reachabe states from Sj\n",
    "                    vals[n] = vals[n] + T[k][j][n]*values[k]\n",
    "                    if(verbose):\n",
    "                        debug(i, j, k, len(states)-1, n, T[k][j][n], values[k])\n",
    "            values[j] = rewards[j] + gamma * np.amax(vals)\n",
    "            policy[states[j]] = actions[np.argmax(vals)]\n",
    "            if(verbose):\n",
    "                print(\"Vals: {}\".format(vals))\n",
    "                print(\"V({0}) = R({0}) + gamma*max({1})\".format(j, vals))\n",
    "                print(\"V({0}) = {1} + {2}*{3}\".format(j, rewards[j], gamma, np.amax(vals)))\n",
    "                print(\"V(\" + str(j) + \"): \" + str(values[j]))\n",
    "        i = i + 1\n",
    "    print(\"Stopping criteria met. state values have been permanently assigned.\")\n",
    "    print(\"V(s1) = {:.2f}\\tV(s2) = {:.2f}\".format(values[0], values[1]))\n",
    "    print(\"R(s1) = {:.2f}\\tR(s2) = {:.2f}\".format(rewards[0], rewards[1]))\n",
    "    print(\"Optimal policy: \")\n",
    "    for key, value in policy.items():\n",
    "        print(\"in state (\" + key + \") take action (\" + value+ \")\")\n",
    "    return values, policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### contrived example for understanding value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping criteria met. state values have been permanently assigned.\n",
      "V(s1) = 4.40\tV(s2) = 1.20\n",
      "R(s1) = 3.00\tR(s2) = -1.00\n",
      "Optimal policy: \n",
      "in state (s1) take action (a2)\n",
      "in state (s2) take action (a1)\n",
      "[4.399061441421509, 1.1995307207107544]\n",
      "{'s1': 'a2', 's2': 'a1'}\n"
     ]
    }
   ],
   "source": [
    "#### dont change anything in this cell, just comment out if needed.\n",
    "states = [\"s1\", \"s2\"]\n",
    "actions = [\"a1\", \"a2\"]\n",
    "rewards = [3, -1]\n",
    "\n",
    "# initialize the values\n",
    "# possible choices are random, zeros, or set to reward value\n",
    "values = rewards[:]\n",
    "\n",
    "# T is a jxjxn matrix of transition probabilities P(Sj | Si, An) for all Sj, Si, and An\n",
    "# P(s1 | s1, a1) = 0.0\n",
    "# P(s1 | s1, a2) = 0.5 \n",
    "# .... \n",
    "# P(s2 | s2, a2) = 1.0\n",
    "# This information must be given (or learned, but that is a different problem)\n",
    "T = [[[0, .5], [1.0, 0]], [[1.0, .5], [0, 1.0]]]\n",
    "\n",
    "# the weight factor for how much we care about future rewards\n",
    "gamma = .5\n",
    "\n",
    "# the stopping criteria, i.e. if improvement is less than this value then stop\n",
    "beta = .005\n",
    "values, policy = value_iteration(states, actions, T, rewards[:], gamma, beta, values=rewards[:], verbose=False)\n",
    "print(values)\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-value iteration for prognostics scheduling\n",
    "#### valid actions are \n",
    "    - maintenance \n",
    "    - short mission\n",
    "    - long mission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1647,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actions(state, actions):\n",
    "    \"\"\"\n",
    "        @brief: maps a given state to the valid actions in that state\n",
    "        \n",
    "        @input:\n",
    "            states: a list of states (length is i), type should be string if verbose=True i.e. [\"s1\", \"s2\", \"living room\" ... \"etc\"]\n",
    "            actions: a list of actions (length is j), type should be string if verbose=True i.e. [\"a1\", \"a2\", \"take picture\" ... \"etc\"]\n",
    "        \n",
    "        @output:\n",
    "            valid_actions: an array of valid actions for a given state\n",
    "    \"\"\"\n",
    "    valid_actions = []\n",
    "    if(state == \"dead\"):\n",
    "        valid_actions = [\"maintenance\"]\n",
    "    else:\n",
    "        valid_actions = actions\n",
    "    return valid_actions\n",
    "\n",
    "\n",
    "\n",
    "def build_T(states, actions, verbose=False):\n",
    "    \"\"\"\n",
    "        @brief: constructs the transition matrix\n",
    "        \n",
    "        @input:\n",
    "            states: a list of states (length is i and k where i==k), [\"healthy\", \"good\", \"degraded\", \"dead\"]\n",
    "            actions: a list of actions (length is j), [\"maintenance\", \"short-mission\", \"long-mission\"]\n",
    "        \n",
    "        @output:\n",
    "            T: an [i,j,k] shape matrix where T[i,j,k] = probability of transition to state k from state i taking action j\n",
    "    \"\"\"\n",
    "    T = np.zeros((len(states), len(actions), len(states)))\n",
    "    for i in range(0, len(states)):\n",
    "        current_state = states[i]\n",
    "        valid_actions = get_actions(current_state, actions)\n",
    "        for j in range(0, len(valid_actions)):\n",
    "            action = valid_actions[j]\n",
    "            for k in range(0, len(states)):\n",
    "                next_state = states[k]\n",
    "                if(current_state == \"healthy\"):\n",
    "                    if(action == \"maintenance\"):\n",
    "                        if(next_state == \"healthy\"):\n",
    "                            T[i,j,k] = 1.0\n",
    "                    elif(action == \"short-mission\"):\n",
    "                        if(next_state == \"healthy\"):\n",
    "                            T[i,j,k] = .99\n",
    "                        elif(next_state == \"good\"):\n",
    "                            T[i,j,k] = .01\n",
    "                    elif(action == \"long-mission\"):\n",
    "                        if(next_state == \"healthy\"):\n",
    "                            T[i,j,k] = .9\n",
    "                        elif(next_state == \"good\"):\n",
    "                            T[i,j,k] = .09\n",
    "                        elif(next_state == \"degraded\"):\n",
    "                            T[i,j,k] = .01\n",
    "                            \n",
    "                elif(current_state == \"good\"):\n",
    "                    if(action == \"maintenance\"):\n",
    "                        if(next_state == \"healthy\"):\n",
    "                            T[i,j,k] = .9\n",
    "                        elif(next_state == \"good\"):\n",
    "                            T[i,j,k] = .1\n",
    "                    elif(action == \"short-mission\"):\n",
    "                        if(next_state == \"good\"):\n",
    "                            T[i,j,k] = .9\n",
    "                        elif(next_state == \"degraded\"):\n",
    "                            T[i,j,k] = .1\n",
    "                    elif(action == \"long-mission\"):\n",
    "                        if(next_state == \"good\"):\n",
    "                            T[i,j,k] = .8\n",
    "                        elif(next_state == \"degraded\"):\n",
    "                            T[i,j,k] = .2\n",
    "\n",
    "                elif(current_state == \"degraded\"):\n",
    "                    if(action == \"maintenance\"):\n",
    "                        if(next_state == \"healthy\"):\n",
    "                            T[i,j,k] = .05\n",
    "                        elif(next_state == \"good\"):\n",
    "                            T[i,j,k] = .75\n",
    "                        elif(next_state == \"degraded\"):\n",
    "                            T[i,j,k] = .2\n",
    "                    elif(action == \"short-mission\"):\n",
    "                        if(next_state == \"degraded\"):\n",
    "                            T[i,j,k] = .7\n",
    "                        elif(next_state == \"dead\"):\n",
    "                            T[i,j,k] = .3\n",
    "                    elif(action == \"long-mission\"):\n",
    "                        if(next_state == \"degraded\"):\n",
    "                            T[i,j,k] = .05\n",
    "                        elif(next_state == \"dead\"):\n",
    "                            T[i,j,k] = .95\n",
    "\n",
    "                elif(current_state == \"dead\"):\n",
    "                    if(action == \"maintenance\"):\n",
    "                        if(next_state == \"dead\"):\n",
    "                            T[i,j,k] = .2\n",
    "                        elif(next_state == \"degraded\"):\n",
    "                            T[i,j,k] = .65\n",
    "                        elif(next_state == \"good\"):\n",
    "                            T[i,j,k] = .15\n",
    "                                     \n",
    "                if(verbose):\n",
    "                    print(\"P({} | {} , {}): {}\".format(next_state, current_state, action, T[i,j,k]))\n",
    "    return T\n",
    "    \n",
    "    \n",
    "def build_R(states, actions, rewards, verbose=False):\n",
    "    \"\"\"\n",
    "        @brief: constructs the reward matrix\n",
    "        \n",
    "        @input:\n",
    "            states: a list of states (length is i and k where i==k), [\"healthy\", \"good\", \"degraded\", \"dead\"]\n",
    "            actions: a list of actions (length is j), [\"maintenance\", \"short-mission\", \"long-mission\"]\n",
    "            rewards: a dictionary of rewards mapping quantitative values to qualitative expressions\n",
    "        \n",
    "        @output:\n",
    "            R: an [i,j] shape matrix where R[i,j] = reward for being in state i and taking action j\n",
    "    \"\"\"\n",
    "    R = np.zeros((len(states), len(actions)))\n",
    "    for i in range(0, len(states)):\n",
    "        state = states[i]\n",
    "        for j in range(0, len(actions)):\n",
    "            action = actions[j]\n",
    "            if(state == \"healthy\"):\n",
    "                if(action == \"maintenance\"):\n",
    "                    R[i,j] = rewards[\"bad\"]\n",
    "                elif(action == \"short-mission\"):\n",
    "                    R[i,j] = rewards[\"good\"]\n",
    "                elif(action == \"long-mission\"):\n",
    "                    R[i,j] = rewards[\"best\"]\n",
    "\n",
    "            elif(state == \"good\"):\n",
    "                if(action == \"maintenance\"):\n",
    "                    R[i,j] = rewards[\"none\"]\n",
    "                elif(action == \"short-mission\"):\n",
    "                    R[i,j] = rewards[\"best\"]\n",
    "                elif(action == \"long-mission\"):\n",
    "                    R[i,j] = rewards[\"good\"]\n",
    "  \n",
    "            elif(state == \"degraded\"):\n",
    "                if(action == \"maintenance\"):\n",
    "                    R[i,j] = rewards[\"best\"]\n",
    "                elif(action == \"short-mission\"):\n",
    "                    R[i,j] = rewards[\"bad\"]\n",
    "                elif(action == \"long-mission\"):\n",
    "                    R[i,j] = rewards[\"worst\"]\n",
    "  \n",
    "            elif(state == \"dead\"):\n",
    "                if(action == \"maintenance\"):\n",
    "                    R[i,j] = rewards[\"none\"]\n",
    "                else:\n",
    "                    R[i,j] = -99\n",
    "                    \n",
    "            if(verbose):\n",
    "                print(\"R({} + {}): {}\".format(state, action, R[i,j]))\n",
    "    return R\n",
    "\n",
    "def get_reward(state, action, rewards):\n",
    "    \"\"\"\n",
    "        @brief: returns the reward value for a given state-action pair\n",
    "        \n",
    "        @input:\n",
    "            states: a list of states (length is i and k where i==k), [\"healthy\", \"good\", \"degraded\", \"dead\"]\n",
    "            actions: a list of actions (length is j), [\"maintenance\", \"short-mission\", \"long-mission\"]\n",
    "            rewards: a dictionary of rewards mapping quantitative values to qualitative expressions\n",
    "        \n",
    "        @output:\n",
    "            a real number reward value\n",
    "    \"\"\"\n",
    "    if(state == \"healthy\"):\n",
    "        if(action == \"maintenance\"):\n",
    "            return rewards[\"bad\"]\n",
    "        elif(action == \"short-mission\"):\n",
    "            return rewards[\"good\"]\n",
    "        elif(action == \"long-mission\"):\n",
    "            return rewards[\"best\"]\n",
    "        else:\n",
    "            return -9999 \n",
    "    elif(state == \"good\"):\n",
    "        if(action == \"maintenance\"):\n",
    "            return rewards[\"none\"]\n",
    "        elif(action == \"short-mission\"):\n",
    "            return rewards[\"best\"]\n",
    "        elif(action == \"long-mission\"):\n",
    "            return rewards[\"good\"]\n",
    "        else:\n",
    "            return -9999    \n",
    "    elif(state == \"degraded\"):\n",
    "        if(action == \"maintenance\"):\n",
    "            return rewards[\"best\"]\n",
    "        elif(action == \"short-mission\"):\n",
    "            return rewards[\"bad\"]\n",
    "        elif(action == \"long-mission\"):\n",
    "            return rewards[\"worst\"]\n",
    "        else:\n",
    "            return -9999   \n",
    "    elif(state == \"dead\"):\n",
    "        if(action == \"maintenance\"):\n",
    "            return rewards[\"none\"]\n",
    "        elif(action == \"short-mission\"):\n",
    "            return rewards[\"worst\"]\n",
    "        elif(action == \"long-mission\"):\n",
    "            return rewards[\"worst\"]\n",
    "        else:\n",
    "            return -9999       \n",
    "    else:\n",
    "        return -9999\n",
    "\n",
    "\n",
    "def q_iteration(states, actions, T, R, Q, gamma, epsilon, verbose=False):\n",
    "    \"\"\"\n",
    "        @brief: implements Bellman's equation to solve for Q\n",
    "        \n",
    "        @input:\n",
    "            states: a list of states (length is i and k where i==k), [\"healthy\", \"good\", \"degraded\", \"dead\"]\n",
    "            actions: a list of actions (length is j), [\"maintenance\", \"short-mission\", \"long-mission\"]\n",
    "            T: a ixjxk matrix of transition probabilities P(Sk | Si, An) for all Sk, Si, and An\n",
    "            R: an ixj matrix of reward values \n",
    "            Q: an ixj matrix of Q values for a given state action pair (typically 0s to start)\n",
    "            gamma: the discount factor, i.e. how much do we care about future rewards\n",
    "            epsilon: the stopping criteria, i.e. minimum amount of improvement between iterations before stopping\n",
    "        \n",
    "        @output:\n",
    "            Q: an ixj matrix of Q values \n",
    "            policy: a dictionary mapping states as keys to the best action as values, i.e. policy[\"some state\"] -> \"some action\"\n",
    "    \"\"\"\n",
    "    old_Q = np.ones(R.shape)* -999\n",
    "    epsilon = .1\n",
    "    c = 0\n",
    "    policy = dict.fromkeys(states, \"\")\n",
    "    while(abs(sum(sum(Q)) - sum(sum(old_Q))) > epsilon):\n",
    "        old_Q[:] = Q[:]\n",
    "        for i in range(0, len(states)):\n",
    "            current_state = states[i]\n",
    "            actions = get_actions(state, actions)\n",
    "            for j in range(0, len(actions)):\n",
    "                current_action = actions[j]\n",
    "                q_vals =np.zeros(len(states))\n",
    "                for k in range(0, len(states)):\n",
    "                    next_state = states[k]\n",
    "                    reward = get_reward(current_state, current_action, rewards)\n",
    "                    if(verbose):\n",
    "                        print(\"s[{}]: {}, a[{}]: {}, R: {} \\t s'[{}]: {}, oldQ[s']: {}\".format(i, current_state, j, current_action, reward, k, next_state, old_Q[k]))\n",
    "                    q_vals[k] = T[i,j,k] * (reward + gamma * np.amax(old_Q[k]))\n",
    "                Q[i,j] = np.sum(q_vals)\n",
    "                if(verbose):\n",
    "                    print(\"Q[{},{}]: {}\".format(current_state, current_action, Q[i,j]))\n",
    "            policy[states[i]] = actions[np.argmax(Q[i])]\n",
    "        c+=1\n",
    "    if(verbose):\n",
    "        print(\"Optimal policy: \")\n",
    "        for key, value in policy.items():\n",
    "            print(\"in state (\" + key + \") take action (\" + value+ \")\")\n",
    "    return Q, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True  True  True]\n",
      " [ True False False]]\n",
      "[[ 1.99414062  3.99414062  5.99414062]\n",
      " [ 2.99414062  5.99414062  3.99414062]\n",
      " [ 5.99414062  1.49414063 -1.58919271]\n",
      " [ 2.66080729  0.          0.        ]]\n",
      "in state (healthy) take action (long-mission)\n",
      "in state (good) take action (short-mission)\n",
      "in state (degraded) take action (maintenance)\n",
      "in state (dead) take action (maintenance)\n"
     ]
    }
   ],
   "source": [
    "states = [\"healthy\", \"good\", \"degraded\", \"dead\"]\n",
    "actions = [\"maintenance\", \"short-mission\", \"long-mission\"]\n",
    "# best action, ok action, nothing, crappy action, worst action\n",
    "rewards = {\n",
    "    \"best\"  :  3,\n",
    "    \"good\"  :  1,\n",
    "    \"none\"   :  0,\n",
    "    \"bad\"   : -1,\n",
    "    \"worst\" : -3\n",
    "}\n",
    "# discount factor\n",
    "gamma = .5\n",
    "\n",
    "# stopping criteria\n",
    "epsilon = .01\n",
    "\n",
    "# transition matrix\n",
    "T = build_T(states, actions)\n",
    "# for this case, ensure all probabilities add to 1 except for the last 2\n",
    "# in the 'dead' state, the only valid action is maintenance\n",
    "print(T.sum(axis=2) == 1)\n",
    "\n",
    "# reward matrix\n",
    "R = build_R(states, actions, rewards)\n",
    "\n",
    "# state-action value matrix\n",
    "Q = np.zeros(R.shape)\n",
    "\n",
    "# solve the MDP\n",
    "Q, policy = q_iteration(states, actions, T, R, Q, gamma, epsilon, verbose=False)\n",
    "print(Q)\n",
    "for key, value in policy.items():\n",
    "    print(\"in state (\" + key + \") take action (\" + value+ \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Simulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1676,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max iterations reached\n",
      "('healthy', 'long-mission')\n",
      "('healthy', 'long-mission')\n",
      "('healthy', 'long-mission')\n",
      "('healthy', 'long-mission')\n",
      "('healthy', 'long-mission')\n",
      "('healthy', 'long-mission')\n",
      "('healthy', 'long-mission')\n",
      "('healthy', 'long-mission')\n",
      "('healthy', 'long-mission')\n",
      "('healthy', 'long-mission')\n",
      "('healthy', 'long-mission')\n",
      "('healthy', 'long-mission')\n",
      "('healthy', 'long-mission')\n",
      "('healthy', 'long-mission')\n",
      "('healthy', 'long-mission')\n",
      "('healthy', 'long-mission')\n",
      "('healthy', 'long-mission')\n",
      "('healthy', 'long-mission')\n",
      "('healthy', 'long-mission')\n",
      "('healthy', 'long-mission')\n",
      "('healthy', 'long-mission')\n",
      "('healthy', 'long-mission')\n",
      "('healthy', 'long-mission')\n",
      "('healthy', 'long-mission')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('degraded', 'maintenance')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('degraded', 'maintenance')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('degraded', 'maintenance')\n",
      "('degraded', 'maintenance')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('degraded', 'maintenance')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('degraded', 'maintenance')\n",
      "('degraded', 'maintenance')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('degraded', 'maintenance')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('degraded', 'maintenance')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('degraded', 'maintenance')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n",
      "('good', 'short-mission')\n"
     ]
    }
   ],
   "source": [
    "# define max iterations\n",
    "max_iterations = 1000\n",
    "\n",
    "# start in healthy state\n",
    "state = states[0]\n",
    "\n",
    "# create an empty history list (tuple of state-action pairs)\n",
    "history = []\n",
    "i = 0\n",
    "\n",
    "### -----------------------------\n",
    "\n",
    "# simulate until the craft dies (hopefully, it should never happen!)\n",
    "while(state is not \"dead\"):\n",
    "    # get the current best action\n",
    "    action = policy[state]\n",
    "    \n",
    "    # stochasticity\n",
    "    r = np.random.uniform(0.000, 1.0000001)\n",
    "    \n",
    "    # grab the transition probabilities for the current state-action pair\n",
    "    # P is an array where the indicies are the indicies of the next state\n",
    "    # and the values are the probabiliity of transitioning into that state\n",
    "    P = T[states.index(state), actions.index(action)]\n",
    "    \n",
    "    # put them in order for the probability calculate\n",
    "    Ps = np.flip(np.argsort(P))\n",
    "\n",
    "    # find out what state the system transitions to\n",
    "    if(r <= P[Ps[0]]):\n",
    "        next_state = states[Ps[0]]\n",
    "    elif(r <= (P[Ps[0]] + P[Ps[1]])):\n",
    "        next_state = states[Ps[1]]\n",
    "    elif(r <= (P[Ps[0]] + P[Ps[1]] + P[Ps[2]])):\n",
    "        next_state = states[Ps[2]]\n",
    "    else:\n",
    "        next_state = states[Ps[3]]\n",
    "        \n",
    "    # append the current state-action pair to the history\n",
    "    history.append((state, action))\n",
    "    \n",
    "    # assign to current state\n",
    "    state = next_state\n",
    "    \n",
    "    # useful to stop the loop as the model is defined in such a way as to have a\n",
    "    # very small probability of reaching the dead state\n",
    "    i += 1\n",
    "    if(i == max_iterations):\n",
    "        break\n",
    "\n",
    "### -----------------------------\n",
    "\n",
    "if(i == max_iterations):\n",
    "    print(\"max iterations reached\")\n",
    "else:\n",
    "    print(\"the uav died!\")\n",
    "for i in range(0, min(100, max_iterations)):\n",
    "    print(history[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous MDP    \n",
    "# WIP\n",
    "- continuous time state space, discrete action space\n",
    "- is that valid?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1607,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(soh):\n",
    "    state = \"\"\n",
    "    if(soh >= 80 and soh <= 100):\n",
    "        state = \"healthy\"\n",
    "    elif(soh >= 65 and soh < 80):\n",
    "        state = \"good\"\n",
    "    elif(soh >= 50 and soh < 65):\n",
    "        state = \"degraded\"\n",
    "    else:\n",
    "        state = \"dead\"\n",
    "    return state\n",
    "        \n",
    "def update_soh(soh, action):\n",
    "    if(action == \"maintenance\"):\n",
    "        soh = soh + 15\n",
    "    elif(action == \"short-mission\"):\n",
    "        soh = soh - .1\n",
    "    elif(action == \"long-mission\"):\n",
    "        soh = soh - .2\n",
    "    else:\n",
    "        soh = soh\n",
    "    return soh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1d3b146d908>]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# placeholder to track the SOH over time\n",
    "vals = np.zeros((steps))\n",
    "\n",
    "# simulation\n",
    "for i in range(0, steps):\n",
    "    if(soh >= 50):\n",
    "        idx = np.random.randint(1, 3)\n",
    "    else:\n",
    "        idx = np.random.randint(0, 3)\n",
    "    vals[i] = soh\n",
    "    soh = update_soh(soh, actions[idx])\n",
    "    state = get_state(soh)\n",
    "plt.plot(vals)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
