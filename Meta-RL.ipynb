{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "pyStateSpace_path = os.path.abspath('../pyStateSpace')\n",
    "pyTorchBridge_path = os.path.abspath('../pyTorchBridge')\n",
    "try:\n",
    "    import pystatespace\n",
    "except ImportError:\n",
    "    if pyStateSpace_path not in sys.path:\n",
    "        sys.path.append(pyStateSpace_path)\n",
    "try:\n",
    "    import pytorchbridge\n",
    "except ImportError:\n",
    "    if pyTorchBridge_path not in sys.path:\n",
    "        sys.path.append(pyTorchBridge_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from copy import deepcopy\n",
    "from multiprocessing import Pool\n",
    "import warnings\n",
    "warnings.filterwarnings(\"error\", category=UserWarning)\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # disable tensorflow warning messages\n",
    "os.makedirs(os.path.expanduser('~/Data/tensorboard/'), exist_ok=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gym\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score, make_scorer\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines import PPO2\n",
    "from tqdm.auto import tqdm, trange\n",
    "from pytorchbridge import TorchEstimator\n",
    "\n",
    "from utils import cache_function, cache_to_episodic_rewards, cache_to_episodes\n",
    "\n",
    "SMALL_SIZE = 15\n",
    "MEDIUM_SIZE = 17\n",
    "BIGGER_SIZE = 19\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "plt.rc('lines', linewidth = 2.5)\n",
    "\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(2, 4, 1, batch_first=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(4,4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 1))\n",
    "    def forward(self, x):\n",
    "        x, h = self.lstm(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_meta_network(estimator, hyperparameters, rewards):\n",
    "    # Train the network to predict rewards in training.\n",
    "    # This ensures that the meta-network can accurately\n",
    "    # model rewards from hyperparameters and time.\n",
    "    hyp = np.asarray(hyperparameters).reshape(1, len(hyperparameters), -1)\n",
    "    rewards = np.asarray(rewards).reshape(1, -1, 1)\n",
    "    estimator.fit(hyp, rewards)\n",
    "\n",
    "def get_relative_gradients(network, hyperparameters):\n",
    "    # Calculate gradients of change in reward w.r.t\n",
    "    # hyperparameters. Since the reward is being accurately\n",
    "    # modelled, the gradients will reflect meaningful changes\n",
    "    # in hyperparameters instead of prediction error.\n",
    "    hyp = np.asarray(hyperparameters).reshape(1, len(hyperparameters), -1)\n",
    "    hyp = torch.tensor(hyp, requires_grad=True)\n",
    "    rewards = torch.squeeze(network(hyp))\n",
    "    relative = rewards[-1] - rewards[-2]\n",
    "    relative.backward()\n",
    "    grad = hyp[0, -1].detach().cpu().numpy()\n",
    "    return grad\n",
    "    \n",
    "def deduce_hyperparameters_from_gradients(grad, hyperparameters):\n",
    "    curr_hyp = hyperparameters[-1]\n",
    "    new_hyp = np.asarray(curr_hyp + 0.1*grad)\n",
    "    return new_hyp\n",
    "\n",
    "def set_hyperparameters(agent, hyp):\n",
    "    agent.learning_rate = np.clip(hyp[0], a_min=1e-5, a_max=None)\n",
    "    agent.cliprange = np.clip(hyp[1], a_min=0, a_max=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial(factor, periods, period_length, agent, env,\n",
    "          hyp_init=None, metaest=None,\n",
    "          return_caches=False, degrade_fn=None,\n",
    "          rtype='episodic'):\n",
    "    if return_caches:\n",
    "        cache = {'state': [], 'action': [], 'reward': [], 'done': []}\n",
    "    rewards_rl = []\n",
    "    \n",
    "    # Initial hyperparameters:\n",
    "    if metaest is not None:\n",
    "        # For first two iterations, initial hyperparameters\n",
    "        # are used.\n",
    "        hyp = [np.asarray(hyp_init), np.asarray(hyp_init)]\n",
    "    \n",
    "    for period in trange(periods):\n",
    "        if degrade_fn is not None:\n",
    "            degrade_fn(env.envs[0], factor, period)\n",
    "\n",
    "        # Online-learning + control\n",
    "        agent.set_env(env)\n",
    "        x_cache, u_cache, d_cache, r_cache = [], [], [], []\n",
    "        agent.learn(total_timesteps=period_length,\n",
    "                    callback=cache_function(x_cache, u_cache, d_cache, r_cache))\n",
    "\n",
    "        # Accumulate rewards per period and other stats\n",
    "        if rtype == 'episodic':\n",
    "            rewards_rl.append(np.mean(cache_to_episodic_rewards(r_cache, d_cache)))\n",
    "        elif rtype == 'temporal':\n",
    "            rewards_rl.append(np.sum(r_cache))\n",
    "        if return_caches:\n",
    "            cache['state'].extend(x_cache)\n",
    "            cache['action'].extend(u_cache)\n",
    "            cache['reward'].extend(r_cache)\n",
    "            cache['done'].extend(d_cache)\n",
    "\n",
    "        # Meta-learning\n",
    "        # At least 2 reward accumulations\n",
    "        if metaest is not None:\n",
    "            if period == 0:\n",
    "                train_meta_network(metaest, hyp[:1], rewards_rl)\n",
    "            if period >= 1:\n",
    "                print('Period', period)\n",
    "                train_meta_network(metaest, hyp, rewards_rl)\n",
    "                grad = get_relative_gradients(metaest.module, hyp)\n",
    "                hyp.append(deduce_hyperparameters_from_gradients(grad, hyp))\n",
    "                set_hyperparameters(agent, hyp[-1])\n",
    "        \n",
    "        # Change environment\n",
    "    \n",
    "    if return_caches:\n",
    "        if metaest is not None:\n",
    "            return (cache, hyp)\n",
    "        return cache\n",
    "    if metaest is not None:\n",
    "        return np.asarray(rewards_rl), hyp\n",
    "    return np.asarray(rewards_rl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach - 1\n",
    "\n",
    "# Training/Testing Loop:\n",
    "#  Train agent for N steps,\n",
    "#  Train metanetwork\n",
    "#  Get gradients of reward w.r.t hyperparameters\n",
    "#  Change hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_interval = 20\n",
    "period_length = 500\n",
    "minibatch = 10\n",
    "policy_learning_rate = 1e-3\n",
    "policy_arch=[128, 128, dict(vf=[32, 32], pi=[32, 32])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def degrade_cartpole(env, factor, time):\n",
    "    env.masscart = 1.0 + time*factor*0.1\n",
    "    env.force_mag = 10.0 - time*factor*1\n",
    "\n",
    "# env_ = DummyVecEnv([lambda: gym.make('BipedalWalker-v2')])\n",
    "env_ = DummyVecEnv([lambda: gym.make('CartPole-v1')])\n",
    "\n",
    "agent = PPO2(MlpPolicy, env_, verbose=0, n_steps=update_interval,\n",
    "             nminibatches=minibatch, learning_rate=policy_learning_rate,\n",
    "             policy_kwargs=dict(net_arch=policy_arch), seed=seed,\n",
    "             tensorboard_log=os.path.expanduser('~/Data/tensorboard/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metanet = MetaNet().double()  # Numpy uses float64 by default\n",
    "metaparams = metanet.parameters()\n",
    "metaoptim = optim.Adam(metaparams, lr=1e-1)\n",
    "metaloss = nn.MSELoss()\n",
    "metaest = TorchEstimator(metanet, metaoptim, metaloss, epochs=1000,\n",
    "                         tol=0.001, max_tol_iter=4, batch_size=1, verbose=True)\n",
    "\n",
    "cache, hyp = trial(factor=1, periods=5, period_length=period_length,\n",
    "                   agent=agent, env=env_, \n",
    "                   hyp_init=(1e-3, 0.2), metaest=metaest, return_caches=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metaest.predict(np.asarray(hyp).reshape(1, -1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "er = cache_to_episodic_rewards(cache['reward'], cache['done'])\n",
    "plt.plot(er)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.asarray([h[0] for h in hyp]))\n",
    "# plt.plot(np.asarray([h[1] for h in hyp]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "e = env_.envs[0]\n",
    "# degrade(e, 1, 10)\n",
    "s = e.reset()\n",
    "while True:\n",
    "    a = agent.predict(s)\n",
    "    print(a[0])\n",
    "    s,r,d,_ = e.step(a[0])\n",
    "    if d:\n",
    "        e.close()\n",
    "        break\n",
    "    try:\n",
    "        e.render()\n",
    "        time.sleep(0.02)\n",
    "    except:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach - 2\n",
    "\n",
    "# Training Loop:\n",
    "#  Train agent to convergence,\n",
    "#  Train metanetwork\n",
    "#  Change environment i.e. task\n",
    "\n",
    "# Testing:\n",
    "#  Apply agent for N \n",
    "#  Get gradients of reward w.r.t hyperparameters\n",
    "#  Change hyperparameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
