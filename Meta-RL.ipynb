{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import notebook_setup\n",
    "\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"error\", category=UserWarning)\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # disable tensorflow warning messages\n",
    "os.makedirs(os.path.expanduser('~/Data/tensorboard/'), exist_ok=True)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gym\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV, ParameterGrid\n",
    "from sklearn.metrics import make_scorer, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "# from stable_baselines.common.policies import MlpPolicy\n",
    "# from stable_baselines.common.vec_env import DummyVecEnv\n",
    "# from stable_baselines.common.evaluation import evaluate_policy\n",
    "# from stable_baselines import PPO2\n",
    "from tqdm.auto import tqdm, trange\n",
    "from pytorchbridge import TorchEstimator\n",
    "from pystatespace import Trapezoid\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from utils import cache_function, cache_to_episodic_rewards, cache_to_episodes, copy_tensor, copy_mlp_regressor, sanitize_filename\n",
    "from systems.tanks import TanksFactory, TanksPhysicalEnv, TanksDataEnv, plot_tanks\n",
    "from ppo import PPO, Memory, ActorCriticBinary\n",
    "from meta import learn_env_model, meta_update, kl_div, prune_library, plot_adaption\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED = 0\n",
    "NCPU = os.cpu_count() // 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tanks = 6\n",
    "n_engines = 2\n",
    "tstep = 1e0\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "nominal_config = {\n",
    "    'heights': np.ones(n_tanks),\n",
    "    'cross_section':np.ones(n_tanks),\n",
    "    'valves_min':np.zeros(n_tanks),\n",
    "    'valves_max':np.ones(n_tanks),\n",
    "    'resistances':np.ones(n_tanks) * 1e2,\n",
    "    'pumps':np.ones(n_tanks) * 0.1,\n",
    "    'engines':np.ones(n_engines) * 0.05\n",
    "}\n",
    "# this tanks object is global and modified in-place by degrade function\n",
    "tanks = TanksFactory(n = n_tanks, e = n_engines, **nominal_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tanks_ = TanksFactory(n = n_tanks, e = n_engines, **nominal_config)\n",
    "env_ = TanksPhysicalEnv(tanks_, tstep)\n",
    "env_.reset()\n",
    "d = False\n",
    "i = 0\n",
    "R = 0\n",
    "while not d and i < 200:\n",
    "    i += 1\n",
    "    s, r, d, _ = env_.step(env_.action_space.sample())\n",
    "    R += r\n",
    "    print(i, s, '{:.2f}'.format(sum(s)), r)\n",
    "print(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tanks(env_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System model\n",
    "\n",
    "* TODO: Check if training data tuples are actually causal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random actions which are repeated for a random number\n",
    "# of time steps\n",
    "def rand_u(length: int, dim: int):\n",
    "    i = 0\n",
    "    u = np.zeros((length, dim))\n",
    "    while i < length - 1:\n",
    "        subseq = min(length - i, np.random.randint(1, int(length / 2)))\n",
    "        u_ = np.random.choice(2, (1, dim))\n",
    "        u[i:i+subseq] = u_\n",
    "        i += subseq\n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data\n",
    "def generate_training_data(tanks, episodes=50):\n",
    "    system = Trapezoid(dims=n_tanks, outputs=n_tanks,\n",
    "                       dx=tanks.dxdt, out=tanks.y, tstep=tstep)\n",
    "    episode_duration = sum(tanks.heights * tanks.cross_section) \\\n",
    "                       / min(sum(tanks.pumps), sum(tanks.engines))\n",
    "    episode_length = int(episode_duration / system.tstep)\n",
    "    \n",
    "    # An episode of length n will have n-1\n",
    "    # state[i], action[i], state[i+1] tuples\n",
    "    arr_len = episode_length - 1\n",
    "    Xtrain = np.empty((episodes * arr_len, n_tanks * 2))\n",
    "    Ytrain = np.empty((episodes * arr_len, n_tanks))\n",
    "    for e in range(episodes):\n",
    "        u = rand_u(episode_length, n_tanks)\n",
    "        t = np.linspace(0, episode_duration, num=episode_length, endpoint=False)\n",
    "        x, _ = system.predict(t, tanks.heights, u)\n",
    "        Xtrain[e * arr_len: (e+1) * arr_len, :n_tanks] = x[:-1]\n",
    "        Xtrain[e * arr_len: (e+1) * arr_len, n_tanks:] = u[:-1]\n",
    "        Ytrain[e * arr_len: (e+1) * arr_len] = x[1:]\n",
    "    return Xtrain, Ytrain\n",
    "\n",
    "Xtrain, Ytrain = generate_training_data(tanks, episodes=50)\n",
    "print(Xtrain.shape, Ytrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(MLPRegressor(), scoring=make_scorer(r2_score, multioutput='uniform_average'),\n",
    "                   param_grid={\n",
    "                       'hidden_layer_sizes': ((64, 64), (128, 128)),\n",
    "                       'activation': ('relu', 'logistic'),\n",
    "                       'learning_rate_init': (1e-2, 1e-3),\n",
    "                       'warm_start': (True,)\n",
    "                   },\n",
    "                   n_jobs=12, verbose=1)\n",
    "grid.fit(Xtrain, Ytrain)\n",
    "pd.DataFrame(grid.cv_results_).sort_values(by='rank_test_score', ascending=True, axis=0).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train on episodes-1, validate on 1 episode worth of instances:\n",
    "est = grid.best_estimator_\n",
    "est.set_params(random_state=seed)\n",
    "# Plot performance\n",
    "env_data = TanksDataEnv(tanks, est, tstep)\n",
    "plot_tanks(env_data, plot='closed')\n",
    "plt.suptitle('Modeled fuel tank levels over time');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module = nn.Sequential(\n",
    "    nn.Linear(12, 64),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(64, 6),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "o = optim.Adam(module.parameters(), lr=50e-5)\n",
    "l = nn.MSELoss()\n",
    "e = TorchEstimator(module, o, l, epochs=100, batch_size=32, early_stopping=True, verbose=True, max_tol_iter=10, tol=1e-5)\n",
    "e.fit(Xtrain, Ytrain);\n",
    "est=e # replace scklearn's estimator with pyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torche = TanksDataEnv(tanks, e, tstep)\n",
    "plot_tanks(torche)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tanks(TanksPhysicalEnv(tanks, tstep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degradation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def degrade(tanks: TanksFactory, time: float, tfactor: np.ndarray,\n",
    "            efactor: np.ndarray, **nominal):\n",
    "    if not isinstance(tfactor, (list, tuple, np.ndarray)):\n",
    "        # If a single degradation factor given, assume it is\n",
    "        # identical for all tanks.\n",
    "        tfactor = np.ones(n_tanks) * tfactor\n",
    "    if not isinstance(efactor, (list, tuple, np.ndarray)):\n",
    "        # If a single degradation factor given, assume it is\n",
    "        # identical for all engines.\n",
    "        efactor = np.ones(n_engines) * efactor\n",
    "    for i in range(n_tanks):\n",
    "        tanks.pumps[i] = nominal['pumps'][i] * (1 - time / tfactor[i])\n",
    "        tanks.resistances[i] = nominal['resistances'][i] + \\\n",
    "                               nominal['resistances'][i] * time / tfactor[i]\n",
    "    for i in range(n_engines):\n",
    "        tanks.engines[i] = nominal['engines'][i] + \\\n",
    "                           nominal['engines'][i] * time / efactor[i]\n",
    "\n",
    "\n",
    "def random_degrade(tanks: TanksFactory=None, tfactors=(10, 20), efactors=(10, 20),\n",
    "                   atmost_tanks=1, atmost_engines=1, random=np.random):\n",
    "    n_tanks = len(tanks.heights)\n",
    "    tfactor = np.ones(n_tanks) * np.inf\n",
    "    if atmost_tanks > 0:\n",
    "        tanks_affected = random.randint(1, atmost_tanks + 1)\n",
    "        idx_affected = random.choice(n_tanks, size=tanks_affected, replace=False)\n",
    "        tfactor[idx_affected] = random.randint(*tfactors, size=tanks_affected)\n",
    "        \n",
    "    efactor = np.ones(n_engines) * np.inf\n",
    "    if atmost_engines > 0:\n",
    "        engines_affected = random.randint(1, atmost_engines + 1)\n",
    "        idx_affected = random.choice(n_engines, size=engines_affected, replace=False)\n",
    "        efactor[idx_affected] = random.randint(*efactors, size=engines_affected)\n",
    "    \n",
    "    if tanks is not None:\n",
    "        degrade(tanks, min([t if t != np.inf else 0 for t in tfactor]), tfactor, efactor, **nominal_config)\n",
    "    return tfactor, efactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach - 1\n",
    "\n",
    "# Training/Testing Loop:\n",
    "#  Train agent for N steps,\n",
    "#  Train metanetwork\n",
    "#  Get gradients of reward w.r.t hyperparameters\n",
    "#  Change hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complementary MAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tanks = TanksFactory(n = n_tanks, e = n_engines, **nominal_config)\n",
    "env = TanksPhysicalEnv(tanks, tstep=tstep)\n",
    "timesteps = 50000 #50000           # max timesteps in one episode\n",
    "\n",
    "ppo_params = dict(\n",
    "    state_dim = env.observation_space.shape[0],\n",
    "    action_dim = 6,\n",
    "    epochs = 5,                  # update policy for K epochs\n",
    "    lr = 0.02,                  # learning rate\n",
    "    n_latent_var = 64,           # number of variables in hidden layer\n",
    "    betas = (0.9, 0.999),\n",
    "    gamma = 0.99,                # discount factor\n",
    "    eps_clip = 0.2,              # clip parameter for PPO\n",
    "    update_interval = 2000      # update policy every n timesteps\n",
    ")\n",
    "library_size = 3\n",
    "data_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Nominal agent trained without fault\n",
    "agent = PPO(env, **ppo_params)\n",
    "r = agent.learn(timesteps)\n",
    "library = [copy_tensor(agent.policy.state_dict())]   # initialize library with policy\n",
    "plt.plot(r)\n",
    "plt.title('Rewards on system under nominal conditions');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate library of policies\n",
    "library_rewards = []\n",
    "del library[1:]\n",
    "\n",
    "tfactors = [\n",
    "    [10, np.inf, np.inf, np.inf, np.inf, np.inf],\n",
    "    [np.inf, np.inf, np.inf, np.inf, 10, np.inf],\n",
    "    [np.inf, np.inf, 10, np.inf, np.inf, np.inf]\n",
    "]\n",
    "efactors = [np.inf, np.inf, np.inf]\n",
    "\n",
    "for tfactor, efactor in tqdm(zip(tfactors, efactors), total=len(tfactors), leave=False):\n",
    "    tanks_ = TanksFactory(n = n_tanks, e = n_engines, **nominal_config)\n",
    "    # introduce some fault and learn data-driven model\n",
    "    degrade(tanks_, time=5, tfactor=tfactor, efactor=efactor, **nominal_config)\n",
    "    if data_model:\n",
    "        est_ = deepcopy(est)  # copy estimator hyperparameters etc.\n",
    "        x, y = generate_training_data(tanks_, episodes=50)  # random actions!\n",
    "        est_.fit(x, y)\n",
    "        # Train agent on data-driven model\n",
    "        env_ = TanksDataEnv(tanks_, est_, tstep)\n",
    "    else:\n",
    "        env_ = TanksPhysicalEnv(tanks_, tstep)\n",
    "    agent_ = PPO(env_, **ppo_params)\n",
    "    rewards = agent_.learn(30000)\n",
    "    library.append(copy_tensor(agent_.policy.state_dict()))\n",
    "    library_rewards.append(rewards)\n",
    "\n",
    "# PLot library rewards\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i, rewards in enumerate(library_rewards):\n",
    "    plt.plot(rewards, label='Policy#{}'.format(i))\n",
    "plt.title('Episodic rewards on model with faults')\n",
    "plt.legend()\n",
    "plt.grid(True, 'both')\n",
    "\n",
    "pth = './bin/library/'\n",
    "os.makedirs(pth, exist_ok=True)\n",
    "plt.savefig(pth+'library_rewards.png')\n",
    "with open(pth + 'lib_rwards.pickle', 'wb') as f:\n",
    "    pickle.dump(dict(library_rewards=library_rewards, tfactors=tfactors, efactors=efactors), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt(agent, est, memory, library, data_model=True,\n",
    "          n_inner=1, n_outer=1, alpha_inner=0.01, alpha_outer=0.1,\n",
    "          **ppo_params):\n",
    "    if data_model:\n",
    "        est_ = learn_env_model(memory, est)\n",
    "        env_ = TanksDataEnv(agent.env.tanks, est_, tstep)\n",
    "    else:\n",
    "        env_ = agent.env\n",
    "    params = meta_update(agent.policy.state_dict(), env_, library, memory,\n",
    "                         n_inner, n_outer, alpha_inner, alpha_outer,\n",
    "                         data_model, **ppo_params)\n",
    "    agent.policy.load_state_dict(params)\n",
    "    return agent\n",
    "\n",
    "def adapt_benchmark(agent, est, memory, library, data_model=True,\n",
    "          n_inner=1, n_outer=1, alpha_inner=0.01, alpha_outer=0.1,\n",
    "          **ppo_params):\n",
    "    if data_model:\n",
    "        est_ = learn_env_model(memory, est)\n",
    "        env_ = TanksDataEnv(agent.env.tanks, est_, tstep)\n",
    "    else:\n",
    "        env_ = agent.env\n",
    "    agent.learn(ppo_params['update_interval'], ppo_params['update_interval'])\n",
    "    env_backup = agent.env\n",
    "    agent.env = env_\n",
    "    # If data_model, then interact for the same number of times with the data env\n",
    "    # as the meta-update step does in the outer x inner loops. Otherwise, reuse\n",
    "    # the buffered memory for the same number of times as the meta update step.\n",
    "    if data_model:\n",
    "        agent.learn(timesteps=ppo_params['update_interval'] * (len(library) * n_inner * n_outer),\n",
    "                    update_interval=ppo_params['update_interval'])\n",
    "    else:\n",
    "        pass\n",
    "        # agent.update(policy=agent.policy, memory=memory,\n",
    "        #              epochs=len(library) * n_inner * n_outer, optimizer=agent.optimizer)\n",
    "    agent.env = env_backup\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial(env_, est, starting_policy, library=[], data_model=True, post_steps=10000,\n",
    "          n_inner=1, n_outer=1, alpha_inner=0.01, alpha_outer=0.1,\n",
    "          benchmark=True):\n",
    "    # Make copies of env, and agent trained on nominal system,\n",
    "    # and starting library of policies (if any)\n",
    "    agent_ = PPO(env_, **ppo_params)\n",
    "    agent_.policy.load_state_dict(copy_tensor(starting_policy))\n",
    "    if benchmark:\n",
    "        agent_benchmark = PPO(env_, **ppo_params)\n",
    "        agent_benchmark.policy.load_state_dict(copy_tensor(starting_policy))\n",
    "    library_ = [copy_tensor(p) for p in library]\n",
    "    # Fault occurs, buffer experience with environment\n",
    "    memory_ = Memory()\n",
    "    agent_.experience(memory_, ppo_params['update_interval'], env_, agent_.policy)\n",
    "    # Use meta-learning to adapt to fault\n",
    "    adapt(agent_, est, memory_, library_, data_model,\n",
    "          n_inner, n_outer, alpha_inner, alpha_outer, **ppo_params)\n",
    "    if benchmark:\n",
    "        adapt_benchmark(agent_benchmark, est, memory_, library_, data_model,\n",
    "                        n_inner, n_outer, alpha_inner, alpha_outer, **ppo_params)\n",
    "    # Continue learning\n",
    "    rewards = []\n",
    "    agents = [agent_, agent_benchmark] if benchmark else [agent_]\n",
    "    for a in tqdm(agents, desc='Post-fault training', leave=False):\n",
    "        rewards.append(a.learn(post_steps))\n",
    "    return rewards, agent_.policy.state_dict(), memory_\n",
    "\n",
    "# TODO: Why do trials return different number of episodes, why is mean and std of different length\n",
    "def ntrials(n=NCPU, verbose=10, *trial_args, **trial_kwargs):\n",
    "    res = Parallel(n_jobs=min(n, NCPU), verbose=verbose)(\n",
    "        delayed(trial)(*trial_args, **trial_kwargs) for _ in range(n)\n",
    "    )\n",
    "    benchmark = len(res[0][0])==2\n",
    "    minlen = min([len(r[0][0]) for r in res])  # in case number of episodes / trial is different\n",
    "    rewards = np.asarray([r[0][0][:minlen] for r in res]) # list of episodic rewards from each trial\n",
    "    mean = np.mean(rewards, axis=0)\n",
    "    std = np.std(rewards, axis=0)\n",
    "    if benchmark:\n",
    "        minlen = min([len(r[0][1]) for r in res])  # in case number of episodes / trial is different\n",
    "        rewards_benchmark = np.asarray([r[0][1][:minlen] for r in res])\n",
    "        mean_benchmark = np.mean(rewards_benchmark, axis=0)\n",
    "        std_benchmark = np.std(rewards_benchmark, axis=0)\n",
    "    else:\n",
    "        mean_benchmark = None\n",
    "        std_benchmark = None\n",
    "    return (mean, mean_benchmark), (std, std_benchmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Grid-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "grid = ParameterGrid(dict(\n",
    "    alpha_inner = [1e-3, 1e-2],\n",
    "    alpha_outer = [1e-2, 1e-1],\n",
    "    n_inner = [4],\n",
    "    n_outer = [2],\n",
    "    data_model = [False, True],\n",
    "    post_steps = [30000],\n",
    "    library = [[], library]\n",
    "))\n",
    "os.makedirs(('./bin/hyperparameters'), exist_ok=True)\n",
    "\n",
    "tanks_ = TanksFactory(n = n_tanks, e = n_engines, **nominal_config)\n",
    "env_ = TanksPhysicalEnv(tanks_, tstep)\n",
    "factors = random_degrade(tanks_, atmost_engines=1)\n",
    "print('Tank Factors:', factors[0])\n",
    "print('Engine Factors:', factors[1])\n",
    "\n",
    "hyp_r, hyp_std, hyp_rb, hyp_stdb = [], [], [], []\n",
    "ngrid = 0\n",
    "for trial_params in tqdm(grid, desc='Hyperparameters', leave=False):\n",
    "    (r, r_b), (std, std_b) = ntrials(4, 11, env_, est, agent.policy.state_dict(), **trial_params)\n",
    "    hyp_r.append(r)\n",
    "    hyp_rb.append(r_b)\n",
    "    hyp_std.append(std)\n",
    "    hyp_stdb.append(std_b)\n",
    "    ngrid += 1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ncol = 2\n",
    "nrow = ngrid // ncol + (ngrid % ncol != 0)\n",
    "plt.figure(figsize=(12, 3 * nrow))\n",
    "for i, (grid_params, (r, r_b, std, std_b)) in enumerate(zip(grid, zip(hyp_r, hyp_rb, hyp_std, hyp_stdb))):\n",
    "    plt.subplot(nrow, ncol, i + 1)\n",
    "    plot_adaption(r, r_b, std, std_b)\n",
    "    print([(k, len(v) if k=='library' else v) for k, v in grid_params.items() if k not in ['post_steps']])\n",
    "    plt.title(i)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populated vs. empty policy library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfactor = np.asarray([np.inf, np.inf, np.inf, np.inf, np.inf, 15.])\n",
    "# efactor = np.asarray([np.inf, np.inf])\n",
    "\n",
    "# factors = (tfactor, efactor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_params = dict(\n",
    "    alpha_inner = 1e-3,\n",
    "    alpha_outer = 1e-2,\n",
    "    n_inner = 4,\n",
    "    n_outer = 2,\n",
    "    data_model = True,\n",
    "    post_steps = 300\n",
    ")\n",
    "pth = './bin/populated_vs_empty_library/'\n",
    "os.makedirs(pth, exist_ok=True)\n",
    "\n",
    "tanks_ = TanksFactory(n = n_tanks, e = n_engines, **nominal_config)\n",
    "env_ = TanksPhysicalEnv(tanks_, tstep)\n",
    "factors = random_degrade(tanks_, atmost_engines=1)\n",
    "# degrade(tanks_, min(factors[0][factors[0]!=np.inf]), *factors, **nominal_config)\n",
    "print('Tank Factors:', factors[0])\n",
    "print('Engine Factors:', factors[1])\n",
    "\n",
    "(r, r_b), (std, std_b) = ntrials(4, 20, env_, est, agent.policy.state_dict(), library, **trial_params)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plot_adaption(r, r_b, std, std_b);\n",
    "\n",
    "faulty_tanks = str(list((factors[0] != np.inf).nonzero()[0]))\n",
    "faulty_engines = str(list((factors[1] != np.inf).nonzero()[0]))\n",
    "fname = pth + 'libsize_3_f' + sanitize_filename(faulty_tanks + faulty_engines + str(trial_params))\n",
    "plt.savefig(fname + '.png')\n",
    "with open(fname+'.pickle', 'wb') as f:\n",
    "    pickle.dump(dict(trial_params=trial_params, results=(r, r_b, std, std_b), factors=factors), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty library\n",
    "(r, r_b), (std, std_b) = ntrials(4, 20, env_, est, agent.policy.state_dict(), [], **trial_params)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plot_adaption(r, r_b, std, std_b);\n",
    "\n",
    "fname = pth + 'libsize_0_f' + sanitize_filename(faulty_tanks + faulty_engines + str(trial_params))\n",
    "plt.savefig(fname + '.png')\n",
    "with open(fname+'.pickle', 'wb') as f:\n",
    "    pickle.dump(dict(trial_params=trial_params, results=(r, r_b, std, std_b), factors=factors), f);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential abrupt faults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-fault trial\n",
    "multi_trial_params = dict(\n",
    "    starting_policy = agent.policy.state_dict(),\n",
    "    n_faults = 10,\n",
    "    lib_size = 3,\n",
    "    library=[]\n",
    ")\n",
    "trial_params = dict(\n",
    "    alpha_inner = 1e-3,\n",
    "    alpha_outer = 1e-3,\n",
    "    n_inner = 4,\n",
    "    n_outer = 1,\n",
    "    data_model = False,\n",
    "    post_steps = 30000\n",
    ")\n",
    "\n",
    "def multi_trial(starting_policy, n_faults=5, lib_size=3, library=[], **trial_params):\n",
    "    tanks_ = TanksFactory(n = n_tanks, e = n_engines, **nominal_config)\n",
    "    env_ = TanksPhysicalEnv(tanks_, tstep)\n",
    "    agent_ = PPO(env_, **ppo_params)\n",
    "    agent_.policy.load_state_dict(copy_tensor(starting_policy))\n",
    "    rewards = []\n",
    "    rewards_benchmark = []\n",
    "    fault_times = [0]\n",
    "    for f in trange(n_faults, desc='Faults', leave=False):\n",
    "        factors = random_degrade(tanks_)\n",
    "        rew, params, memory = trial(env_, est, agent_.policy.state_dict(), library, **trial_params)\n",
    "        library.append(params)\n",
    "        library, div = prune_library(library, lib_size, memory, **ppo_params)\n",
    "        agent_.policy.load_state_dict(params)\n",
    "        rewards.extend(rew[0])\n",
    "        rewards_benchmark.extend(rew[1])\n",
    "        fault_times.append(fault_times[-1] + len(rew[0]))\n",
    "    return rewards, rewards_benchmark, fault_times[:-1]\n",
    "\n",
    "def nmulti_trials(n=NCPU, verbose=20, **params):\n",
    "    res = Parallel(n_jobs=min(n, NCPU), verbose=verbose)(\n",
    "        delayed(multi_trial)(**params) for _ in range(n)\n",
    "    )\n",
    "    benchmark = res[0][1] is not None\n",
    "    minlen = min([len(r[0]) for r in res])  # in case number of episodes / trial is different\n",
    "    rewards = np.asarray([r[0][:minlen] for r in res]) # list of episodic rewards from each trial\n",
    "    mean = np.mean(rewards, axis=0)\n",
    "    std = np.std(rewards, axis=0)\n",
    "    # Benchmarks\n",
    "    if benchmark:\n",
    "        minlen = min([len(r[1]) for r in res])  # in case number of episodes / trial is different\n",
    "        rewards_benchmark = np.asarray([r[1][:minlen] for r in res])\n",
    "        mean_benchmark = np.mean(rewards_benchmark, axis=0)\n",
    "        std_benchmark = np.std(rewards_benchmark, axis=0)\n",
    "    else:\n",
    "        mean_benchmark = None\n",
    "        std_benchmark = None\n",
    "    # Faults\n",
    "    minlen = min([len(r[2]) for r in res])  # in case number of episodes / trial is different\n",
    "    fault_times = np.asarray([r[2][:minlen] for r in res]) # list of episodic rewards from each trial\n",
    "    fault_times = np.mean(fault_times, axis=0)\n",
    "    return (mean, mean_benchmark), (std, std_benchmark), fault_times\n",
    "\n",
    "(r, r_b), (std, std_b), f = nmulti_trials(n=4, verbose=20, **multi_trial_params, **trial_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pth = './bin/multi_fault/'\n",
    "os.makedirs(pth, exist_ok=True)\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plot_adaption(r, r_b, std, std_b, f)\n",
    "\n",
    "fname = pth + sanitize_filename(str({k:v for k, v in multi_trial_params.items() \\\n",
    "                                     if k not in ('library', 'starting_policy')}) + str(trial_params))\n",
    "plt.savefig(fname + '.png')\n",
    "with open(fname+'.pickle', 'wb') as fi:\n",
    "    pickle.dump(dict(trial_params=trial_params, multi_trial_params=multi_trial_params,\n",
    "                     results=(r, r_b, std, std_b, f)), fi);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
