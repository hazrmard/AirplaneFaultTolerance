{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing higher order gradient calculations using `pytorch` and the `higher` library. Specifically, the gradient of model parameters with respect to their earlier version during gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.autograd import grad\n",
    "import higher\n",
    "from higher import innerloop_ctx\n",
    "from higher.optim import DifferentiableSGD\n",
    "import numpy as np\n",
    "\n",
    "import notebook_setup\n",
    "import ppo, utils, meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take this function where `a_[0],b_[0]` are current parameters (t=0), `x` is input and `y_` is  predicted model output. `a, b` and `y` are true parameters and value respectively.\n",
    "\n",
    "    y_ = (a_[0]**2 + b_[0]**2) * x\n",
    "\n",
    "The loss is \n",
    "\n",
    "    L = 0.5 * (y_ - y) ** 2\n",
    "\n",
    "Here, I consider just `a`. The gradients are:\n",
    "\n",
    "    d L  / d y_  = y_ - y                   # loss w.r.t prediction\n",
    "    d y_ / d a_[0] = 2 a_[0] x              # prediction w.r.t parameter\n",
    "    d L  / d a_[0] = (y_ - y)(2 a_[0] x)    # loss w.r.t parameter via chain rule\n",
    "\n",
    "I am using SGD with learning rate=1 for simplicity. So, the next version of parameter `a`, `a_[1]` will be:\n",
    "\n",
    "    a_[1] = a_[0] -  d L  / d a_[0]        # update opposite direction of gradient\n",
    "    a_[1] = a_[0] - (y_ - y)(2 a_[0] x)    # substituting\n",
    "    a_[1] = a_[0] * (1 - (y_ - y)(2 x) )   # factoring out a_[0]\n",
    "    a_[1] = a_[0] * (1- 2 x y_ + 2 x y)\n",
    "    a_[1] = a_[0] * (1- 2 x (a_[0]**2 + b_[0]**2) * x) + 2 x y)\n",
    "    a_[1] = a_[0] * (1 - 2 x**2 a_[0]**2 - 2 x**2 b_[0]**2 + 2 x y)\n",
    "    a_[1] = a_[0] - 2 x**2 a_[0]**3 - 2 x**2 a_[0] b_[0]**2 + 2 a_[0] x y\n",
    "\n",
    "Which gives the gradient between different versions of `a_` as:\n",
    "\n",
    "    d a_[1] / d a_[0] = 1 - 6 x**2 a_[0]**2 - 2 x**2 b_[0]**2 + 2 x y\n",
    "\n",
    "Assuming true `a=3, b=4`, and current parameters `a_[0]=2, b_[0]=3`, and input `x=0.1`, then\n",
    "\n",
    "    y =  (3**2 + 4**2) * 0.1 = 2.5\n",
    "    y_ = (2**2 + 3**2) * 0.1 = 1.3\n",
    "    L = 0.5 (1.3 - 2.5)**2 = 0.72\n",
    "\n",
    "    d L  / d y_ = -1.2\n",
    "    d y_ / d a_[0] = 2 * 2 * 0.1 = 0.4\n",
    "    d L  / d a_[0] = (1.3 - 2.5)(2 * 2 * 0.1) = -1.2 * 0.4 = -0.48\n",
    "\n",
    "    a_[1] = 2 - (-0.48) = 2.48\n",
    "    d a_[1] / d a_[0] = 1 - 6 * 0.01 * 4 - 2 * 0.01 * 9 + 2 * 0.1 * 2.5 = 1.08\n",
    "\n",
    "Similarly, `b_[1] = 3.72` using the same method. For the meta-update step, the test loss `TL` is calculated using these updated parameters. The loss gradient is then backprogagated through time=1 to time=0 to get `d TL / d_a[0]`. However since `TL` is a result of both `a_[1], b_[1]`, so backpropagation is done for both `a, b`:\n",
    "\n",
    "    d TL / d a_[0] = (d TL / d a_[1] * d a_[1] / d_a[0]) + (d TL / d b_[1] * d b_[1] / d_a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, a=2., b=3.):\n",
    "        super().__init__()\n",
    "        # Use nn.Modules\n",
    "        self.a = nn.Linear(1,1,False)\n",
    "        self.a.weight.data.fill_(a)\n",
    "        self.b = nn.Linear(1,1,False)\n",
    "        self.b.weight.data.fill_(b)\n",
    "        # Or custom parameters (toggle comments in forward()):\n",
    "        # self.a = nn.Parameter(torch.ones(1, requires_grad=True) * a)\n",
    "        # self.b = nn.Parameter(torch.ones(1, requires_grad=True) * b)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.a(self.a(x)) + self.b(self.b(x))    # with modules\n",
    "        # return (self.a ** 2 + self.b ** 2) * x        # with custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Opt(DifferentiableSGD):\n",
    "    \n",
    "    def _update(self, grouped_grads, **kwargs) -> None:\n",
    "        print('d Loss / d params(t=0): a_[0]:{:.3f} \\t b_[0]{:.3f}'.format(*[g.item() for g in grouped_grads[0]]))\n",
    "        return super()._update(grouped_grads, **kwargs)\n",
    "        \n",
    "\n",
    "higher.register_optim(optim.SGD, Opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for higher order gradients\n",
    "x = torch.ones(1) * 0.1\n",
    "a, b = 3, 4    # true parameter values\n",
    "m = Model()\n",
    "o = optim.SGD(m.parameters(), lr=1.)\n",
    "# Single training step t=0 ==> t=1\n",
    "with higher.innerloop_ctx(m, o, copy_initial_weights=False) as (hm, ho):\n",
    "    y_ = hm(x)\n",
    "    y = (a**2 + b**2) * x\n",
    "    loss = 0.5 * (y_ - y) ** 2\n",
    "\n",
    "    print('Inner optimization')\n",
    "    print('Loss:\\t\\t{:.3f}'.format(loss.item()))\n",
    "    print('d Loss / d y_: {:.3f}'.format(torch.autograd.grad(loss, y_, retain_graph=True)[0].item()))\n",
    "    print('Current params: \\ta_[0]:{:.3f} \\t b_[0]:{:.3f}'.format(\n",
    "          *[p.item() for p in hm.parameters(time=0)]))\n",
    "    \n",
    "    # This populates grads in `m` when copy_initial_weights=False\n",
    "    ho.step(loss)\n",
    "    \n",
    "    print('Updated params: \\ta_[1]:{:.3f} \\t b_[1]:{:.3f}'.format(\n",
    "          *[p.item() for p in hm.parameters(time=1)]))\n",
    "    \n",
    "    print('d a_[1] / d param(t=0): a_[0]:{:.3f} \\t b_[0]:{:.3f}'.format(\n",
    "          *[g.item() for g in \\\n",
    "            torch.autograd.grad(list(hm.parameters(time=1))[0],  # this is first param at t=1 (a_[1])\n",
    "                                hm.parameters(time=0),           # w.r.t all params at t=0\n",
    "                                retain_graph=True)]))\n",
    "    \n",
    "    print()\n",
    "    y_ = hm(x)\n",
    "    y = (a**2 + b**2) * x\n",
    "    testloss = 0.5 * (y_ - y) ** 2\n",
    "    testloss.backward(retain_graph=True)\n",
    "\n",
    "    print('Outer optimization using test loss')\n",
    "    print('TL:\\t\\t{:.3f}'.format(testloss.item()))\n",
    "    print('d TL / d a_[0] = [\\td TL / d a_[1] * d a_[1] / d a_[0] +\\n\\t\\t\\td TL / d b_[1] * d b_[1] / d a_[0] ]')\n",
    "    print('%.3f' % list(m.parameters())[0].grad.item(), '=',\n",
    "          '%.3f' % grad(testloss, list(hm.parameters(time=1))[0], retain_graph=True)[0].item(), '*',\n",
    "          '%.3f' % grad(list(hm.parameters(time=1))[0], list(hm.parameters(time=0))[0], retain_graph=True)[0].item(),\n",
    "          ' + ',\n",
    "          '%.3f' % grad(testloss, list(hm.parameters(time=1))[1], retain_graph=True)[0].item(), '*',\n",
    "          '%.3f' % grad(list(hm.parameters(time=1))[1], list(hm.parameters(time=0))[0])[0].item())\n",
    "    \n",
    "    o.step()  # Outer update (meta-update) using d TL / d params\n",
    "    print('Final parameters:')\n",
    "    print(*m.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Higher order gradient tracking with multiple models.\n",
    "\n",
    "M, m = Model(), Model()\n",
    "m.load_state_dict(M.state_dict())\n",
    "O, o = optim.SGD(M.parameters(), lr=1.), optim.SGD(m.parameters(), lr=1.)\n",
    "\n",
    "with higher.innerloop_ctx(m, o, copy_initial_weights=False) as (hm, ho):\n",
    "    y_ = hm(x)\n",
    "    y = (a**2 + b**2) * x\n",
    "    loss = 0.5 * (y_ - y) ** 2\n",
    "    ho.step(loss)\n",
    "    \n",
    "    y_ = hm(x)\n",
    "    y = (a**2 + b**2) * x\n",
    "    testloss = 0.5 * (y_ - y) ** 2\n",
    "    testloss.backward(retain_graph=True)\n",
    "    \n",
    "    for P, p0, p1 in zip(M.parameters(), hm.parameters(time=0), hm.parameters(time=1)):\n",
    "        if P.grad is None:\n",
    "            P.grad = torch.zeros_like(P)\n",
    "        P.grad.add_(grad(testloss, p0, retain_graph=True)[0])\n",
    "    \n",
    "    O.step()\n",
    "    print('Final parameters:')\n",
    "    print(*M.parameters())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python37664bit2b6a5ffea75d4de087eb7d5fb87e4f6e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
