{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing a toy example for demonstrating MAML in RL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.autograd import grad\n",
    "from higher import innerloop_ctx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "import notebook_setup\n",
    "import ppo, utils\n",
    "from ppo import DEVICE\n",
    "from systems import CartPoleEnv, plot_cartpole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard MAML training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent: ppo.PPO, tasks, n=10, losses=None, seed=None, lr_meta=None, lr_inner=None):\n",
    "    \"\"\"Pre-training model using higher-order gradients on per-task samples from `training_tasks`\"\"\"\n",
    "    if seed is not None: torch.manual_seed(seed)\n",
    "    model, opt = agent.policy, agent.optimizer\n",
    "    og_lrs = []\n",
    "    if lr_meta is not None:\n",
    "        for pgroup in opt.param_groups:\n",
    "            og_lrs.append(pgroup['lr'])\n",
    "            pgroup['lr'] = lr_meta\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=n)\n",
    "    memory = ppo.Memory()\n",
    "    for e in trange(n, leave=False, desc='MAML Initialization'):\n",
    "        opti = optim.Adam(model.parameters(), **opt.defaults)\n",
    "        for i, env in enumerate(tasks):\n",
    "            with innerloop_ctx(model, opti, copy_initial_weights=False,\n",
    "                              override=None if lr_inner is None else dict(lr=[lr_inner])) \\\n",
    "            as (fmodel, diffopt):\n",
    "\n",
    "                agent.experience(memory, timesteps=k, env=env, policy=fmodel)\n",
    "                agent.update(fmodel, memory, epochs=1, optimizer=diffopt, higher_optim=True)\n",
    "                memory.clear()\n",
    "                \n",
    "                agent.experience(memory, timesteps=k, env=env, policy=fmodel)\n",
    "                l = agent.update(fmodel, memory, optimizer=None)\n",
    "                memory.clear()\n",
    "                if losses is not None:\n",
    "                    losses[i].append(l.item())\n",
    "\n",
    "        opt.step()\n",
    "        scheduler.step()\n",
    "        opt.zero_grad()\n",
    "    if lr_meta is not None:\n",
    "        for pgroup, lr in zip(opt.param_groups, og_lrs):\n",
    "            pgroup['lr'] = lr\n",
    "\n",
    "def benchmark_train(agent, tasks, n=10, losses=None, seed=None):\n",
    "    \"\"\"Pre-training model on aggregated samples from `training_tasks`\"\"\"\n",
    "    if seed is not None: torch.manual_seed(seed)\n",
    "    model, opt = agent.policy, agent.optimizer\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=n)\n",
    "    memory = ppo.Memory()\n",
    "    for _ in trange(n, leave=False, desc='Bench. Initialization'):\n",
    "        for i, env in enumerate(tasks):\n",
    "\n",
    "                agent.experience(memory, timesteps=k, env=env, policy=model)\n",
    "                l = agent.update(model, memory, epochs=1, optimizer=opt, higher_optim=False)\n",
    "                memory.clear()\n",
    "                if losses is not None:\n",
    "                    losses[i].append(l.item())\n",
    "\n",
    "        opt.step()\n",
    "        scheduler.step()\n",
    "        opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(agent, n, env, losses=None, rewards=None, callback=None, track_higher_grads=False, seed=None):\n",
    "    \"\"\"Fine-tuning model on tasks contained in `evaluation_tasks`\"\"\"\n",
    "    if seed is not None: torch.manual_seed(seed)\n",
    "    model = agent.policy\n",
    "    memory = ppo.Memory()\n",
    "    opti = optim.Adam(model.parameters(), **agent.optimizer.defaults)\n",
    "    with innerloop_ctx(model, opti, track_higher_grads=track_higher_grads) as (fmodel, diffopt):\n",
    "        for e in trange(n, leave=False, desc='Testing'):\n",
    "\n",
    "            agent.experience(memory, timesteps=k, env=env, policy=fmodel)\n",
    "            l = agent.update(fmodel, memory, epochs=n_adapt, optimizer=diffopt, higher_optim=True)\n",
    "\n",
    "            if losses is not None or rewards is not None:\n",
    "                episodic_rewards = utils.cache_to_episodic_rewards([memory.rewards], [memory.is_terminals])\n",
    "                if losses is not None: losses.append(l.item())\n",
    "                if rewards is not None: rewards.append(np.nanmean(episodic_rewards))\n",
    "            \n",
    "            if callback is not None:\n",
    "                callback(locals())\n",
    "\n",
    "            memory.clear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_tasks = [0,1,2,3,4]\n",
    "evaluation_task = 6\n",
    "k = 500        # number of examples per task\n",
    "alpha = 0.02   # global learning rate\n",
    "alpha_meta = 0.02\n",
    "alpha_inner = 0.1 # initialization lr for our approach\n",
    "n_adapt = 5\n",
    "n_train = 10\n",
    "n_test = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task(seed=None, randomize=True):\n",
    "    env = CartPoleEnv(seed)\n",
    "    return env.randomize() if randomize else env\n",
    "\n",
    "def make_model(seed=None):\n",
    "    return ppo.PPO(\n",
    "        env=None,\n",
    "        policy=ppo.ActorCriticDiscrete,\n",
    "        state_dim=4,\n",
    "        action_dim=2,\n",
    "        n_latent_var=32,\n",
    "        lr=alpha,\n",
    "        seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = make_model()\n",
    "e = get_task(randomize=False)\n",
    "a.env = e\n",
    "r = a.learn(50000, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(len(r)), r)\n",
    "plt.ylim(top=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard MAML testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = make_model(0)\n",
    "bench = make_model(0)\n",
    "bench.policy.load_state_dict(agent.policy.state_dict())\n",
    "\n",
    "r_test = []\n",
    "r_bench_test = []\n",
    "\n",
    "train(agent, list(map(get_task, training_tasks)), n=n_train, seed=0, lr_meta=alpha_meta, lr_inner=alpha_inner)\n",
    "benchmark_train(bench, list(map(get_task, training_tasks)), n=n_train, seed=0)\n",
    "\n",
    "env = get_task(evaluation_task)\n",
    "test(agent, n=n_test, env=env, rewards=r_test, seed=0)\n",
    "env = get_task(evaluation_task)\n",
    "test(bench, n=n_test, env=env, rewards=r_bench_test, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss perfomance\n",
    "plt.plot(r_test, label='MAML')\n",
    "plt.plot(r_bench_test, label='Bench')\n",
    "plt.grid(True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting actual outputs\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1,2,1)\n",
    "plot_cartpole(get_task(evaluation_tasks[-1]), agent, legend=False)\n",
    "plt.subplot(1,2,2)\n",
    "test(agent, evaluation_tasks[-1], losses=None, callback=lambda args: plot_cartpole(args.get('env'), args.get('agent')))\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python37664bit2b6a5ffea75d4de087eb7d5fb87e4f6e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
