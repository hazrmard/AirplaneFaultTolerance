{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from higher import innerloop_ctx\n",
    "\n",
    "import notebook_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient persistence/accumulation after `load_state_dict`\n",
    "* `backward()` accumulates gradients regardless of new state dict loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Sequential(nn.Linear(1,1,False))\n",
    "(m(torch.ones(1,1))**2).backward()\n",
    "g_before = []\n",
    "for p in m.parameters():\n",
    "    g_before.append(p.grad.clone())\n",
    "print('M grads:', g_before)\n",
    "\n",
    "m2 = nn.Sequential(nn.Linear(1,1,False))\n",
    "m.load_state_dict(m2.state_dict())\n",
    "g_after = []\n",
    "for p in m.parameters():\n",
    "    g_after.append(p.grad)\n",
    "print('After loading M2 params', g_after)\n",
    "\n",
    "(m(torch.ones(1,1))**2).backward()\n",
    "(m2(torch.ones(1,1))**2).backward()\n",
    "g2 = []\n",
    "for p in m2.parameters():\n",
    "    g2.append(p.grad)\n",
    "print('M2 grads:', g2)\n",
    "\n",
    "g_final = []\n",
    "for p in m.parameters():\n",
    "    g_final.append(p.grad)\n",
    "print('M grads after backlward w/ new state dict:', g_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `higher` context w/ `copy_initial_weights` & loading multiple `state_dict`s\n",
    "\n",
    "`m2` and `m3` are converted to functional models using `higher`, and load their state dicts from `m`.\n",
    "\n",
    "* Original weights referenced with `copy_initial_weights=False` are **not** used when new `state_dict` is loaded.\n",
    "* **However** gradients are still accumulated when `copy_initial_weights=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Sequential(nn.Linear(1,1,False))\n",
    "m2 = nn.Sequential(nn.Linear(1,1,False))\n",
    "m3 = nn.Sequential(nn.Linear(1,1,False))\n",
    "with torch.no_grad():\n",
    "    for p in m.parameters(): p.fill_(1.)\n",
    "    for p in m2.parameters(): p.fill_(2.)\n",
    "    for p in m3.parameters(): p.fill_(3.)\n",
    "o = optim.SGD(m.parameters(), lr=0.1)\n",
    "\n",
    "diff, grads = [], []\n",
    "for name, m_ in zip(('m2', 'm3'), (m2, m3)):\n",
    "    with innerloop_ctx(m, o, copy_initial_weights=False) as (fm, fo):\n",
    "        fm.load_state_dict(m_.state_dict())\n",
    "        print('m', m.state_dict())\n",
    "        print(name, m_.state_dict())\n",
    "        print('fm', fm.state_dict())\n",
    "\n",
    "        loss = (fm(torch.ones(1,1))**2)\n",
    "        print('dL/dm', torch.autograd.grad(loss, m.parameters(), retain_graph=True))\n",
    "        fo.step(loss)\n",
    "        print('After update to fm')\n",
    "        print('m', m.state_dict())\n",
    "        print(name, m_.state_dict())\n",
    "        print('fm', fm.state_dict())\n",
    "        print('Test loss:')\n",
    "        loss = fm(torch.ones(1,1))**2\n",
    "        loss.backward(retain_graph=True)\n",
    "        print('dL/dfm', torch.autograd.grad(loss, fm.parameters(), retain_graph=True))\n",
    "        print('dfm/dm', torch.autograd.grad(list(fm.parameters())[0], m.parameters(), retain_graph=True))\n",
    "        print('Gradients on m', torch.autograd.grad(loss, m.parameters(), retain_graph=True))\n",
    "        \n",
    "        pdiff = []\n",
    "        grad = []\n",
    "        for p0, p1 in zip(fm.parameters(time=0), fm.parameters(time=1)):\n",
    "            pdiff.append(p1 - p0)\n",
    "            grad.append(torch.autograd.grad(p1.sum(), p0)[0])\n",
    "        diff.append(pdiff)\n",
    "        grads.append(grad)\n",
    "        print('=' * 10 + '\\n')\n",
    "\n",
    "        \n",
    "print('Gradients on m: sum of test gradients on m2 m3')\n",
    "print(*map(lambda p: p.grad, m.parameters()))\n",
    "print('Parameter differences from m, for each state_dict loaded and updated:')\n",
    "print(*diff)\n",
    "print('Gradients w.r.t m, for each of updated m2, m3:')\n",
    "print(*diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second order grads w/o `higher` using `create_graph` and `retain_graph`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Sequential(nn.Linear(1,1,False))\n",
    "m2 = nn.Sequential(nn.Linear(1,1,False))\n",
    "o = optim.SGD(m.parameters(), lr=0.1)\n",
    "o2 = optim.SGD(m2.parameters(), lr=0.1)\n",
    "with torch.no_grad():\n",
    "    for p in m.parameters(): p.fill_(1.)\n",
    "    for p in m2.parameters(): p.fill_(1.)\n",
    "\n",
    "###\n",
    "        \n",
    "loss = (m(torch.ones(1))**2).sum()\n",
    "loss.backward(create_graph=True, retain_graph=True)\n",
    "o.step()\n",
    "\n",
    "o.zero_grad()\n",
    "loss = (m(torch.ones(1))**2).sum()\n",
    "loss.backward(create_graph=True, retain_graph=True)\n",
    "for p in m.parameters(): print(p.data, p.grad.data)\n",
    "\n",
    "###\n",
    "    \n",
    "loss = (m2(torch.ones(1))**2).sum()\n",
    "loss.backward(create_graph=False, retain_graph=False)\n",
    "o2.step()\n",
    "\n",
    "o2.zero_grad()\n",
    "loss = (m2(torch.ones(1))**2).sum()\n",
    "loss.backward()\n",
    "for p in m2.parameters(): print(p.data, p.grad.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing higher grads out o `innerloop_ctx`\n",
    "* Fast weights and differentiation across parameters is possible outside `innerloop_ctx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Sequential(nn.Linear(1,1,False))\n",
    "o = optim.SGD(m.parameters(), lr=0.1)\n",
    "with torch.no_grad():\n",
    "    for p in m.parameters(): p.fill_(1.)\n",
    "\n",
    "m2 = None\n",
    "\n",
    "with innerloop_ctx(m, o, track_higher_grads=True) as (fm, fo):\n",
    "    loss = (fm(torch.ones(1))**2).sum()\n",
    "    fo.step(loss)\n",
    "    m2 = fm\n",
    "    loss = (fm(torch.ones(1))**2).sum()\n",
    "    loss.backward(retain_graph=True)\n",
    "    print('Inside context')\n",
    "    print('dL/dp(t=1):', torch.autograd.grad(loss, m2.parameters(time=1), retain_graph=True))\n",
    "    print('dp(t=1)/dp(t=0):', torch.autograd.grad(sum(list(m2.parameters())), m2.parameters(time=0), retain_graph=True))\n",
    "    print('dL/dp(t=0):', torch.autograd.grad(loss, m2.parameters(time=0), retain_graph=True))\n",
    "\n",
    "print('Outside context')\n",
    "print('dp(t=1)/dp(t=0):', torch.autograd.grad(sum(list(m2.parameters())), m2.parameters(time=0)))\n",
    "print('p(t=0)')\n",
    "for p in m2.parameters(time=0):\n",
    "    print(p.data)\n",
    "print('p(t=0)')\n",
    "for p in m2.parameters(time=1):\n",
    "    print(p.data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
