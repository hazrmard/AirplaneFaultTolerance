{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from itertools import combinations\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from types import SimpleNamespace\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import manifold\n",
    "from sklearn import model_selection\n",
    "from scipy.integrate import trapz\n",
    "import optuna\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "import notebook_setup\n",
    "import ppo, utils, meta\n",
    "from ppo import DEVICE\n",
    "from systems import TanksPhysicalEnv, TanksFactory, plot_tanks\n",
    "\n",
    "datadir = Path(os.path.abspath('../bin/systol21'))\n",
    "os.makedirs(datadir, exist_ok=True)\n",
    "os.makedirs(datadir / 'plots', exist_ok=True)\n",
    "\n",
    "now = lambda: datetime.now().strftime('%y%m%d%H%M-');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Env parameters\n",
    "alpha = 0.0367\n",
    "model_seed = 0\n",
    "tstep = 1000e-3\n",
    "update_interval = 500\n",
    "n_latent_var = 96\n",
    "epochs = 10\n",
    "timesteps = 40000\n",
    "random_faults = False\n",
    "\n",
    "# Single mode faults, 0 index is nominal\n",
    "faults = [\n",
    "        dict(),\n",
    "    # valves 25\n",
    "        dict(valves_min=np.asarray([0,0.25,0,0,0.25,0])),\n",
    "        dict(valves_min=np.asarray([0,0,0.25,0,0,0.25])),\n",
    "        dict(valves_min=np.asarray([0.25,0.25,0,0,0,0])),\n",
    "        dict(valves_min=np.asarray([0,0.25,0.25,0,0,0])),\n",
    "    # valves 50\n",
    "        dict(valves_min=np.asarray([0,0.5,0,0.5,0,0])),\n",
    "        dict(valves_min=np.asarray([0,0,0.5,0,0.5,0])),\n",
    "        dict(valves_min=np.asarray([0.5,0,0.5,0,0,0])),\n",
    "        dict(valves_min=np.asarray([0,0.5,0.5,0,0,0])),\n",
    "#     # valve\n",
    "#         dict(valves_min=np.asarray([0,0.5,0,0,0,0])),\n",
    "#         dict(valves_min=np.asarray([0,0,0.5,0,0,0])),\n",
    "#         dict(valves_min=np.asarray([0,0,0,0.5,0,0])),\n",
    "#         dict(valves_min=np.asarray([0,0,0,0,0.5,0])),\n",
    "    # leaks\n",
    "        dict(leaks=np.asarray([0.005,0,0,0,0,0])),\n",
    "        dict(leaks=np.asarray([0,0.005,0,0,0,0])),\n",
    "        dict(leaks=np.asarray([0,0,0,0,0.005,0])),\n",
    "        dict(leaks=np.asarray([0,0,0,0,0,0.005])),\n",
    "    # pumps\n",
    "        dict(pumps=np.asarray([1e-2,1e-3,1e-2,1e-2,1e-2,1e-2])),\n",
    "        dict(pumps=np.asarray([1e-2,1e-2,1e-3,1e-2,1e-2,1e-2])),\n",
    "        dict(pumps=np.asarray([1e-2,1e-2,1e-3,1e-3,1e-2,1e-2])),\n",
    "        dict(pumps=np.asarray([1e-2,1e-2,1e-2,1e-3,1e-3,1e-2])),\n",
    "    ]\n",
    "\n",
    "mode_indices = dict(\n",
    "    nominal = slice(0, 1),\n",
    "    valves25 = slice(1, 5),\n",
    "    valves50 = slice(5,9),\n",
    "    # valve = slice(9,13),\n",
    "    leaks = slice(9, 13),\n",
    "    pumps = slice(13, 17)\n",
    ")\n",
    "\n",
    "scatter_fmt = dict(\n",
    "        nominal=dict(c='black', marker='o', s=60),\n",
    "        valves25=dict(c='r', marker='h', s=40),\n",
    "        valves50=dict(c='lightcoral', marker='D', s=40),\n",
    "        valve=dict(c='firebrick', marker='s', s=40),\n",
    "        leaks=dict(c='g', marker='v', s=40),\n",
    "        pumps=dict(c='b', marker='p', s=40)\n",
    "      )\n",
    "\n",
    "ls = \\\n",
    "    [dict(ls='-')] + \\\n",
    "    [dict(ls=':')] * 22\n",
    "\n",
    "\n",
    "if random_faults:\n",
    "    # Random composite faults\n",
    "    random_ = np.random.RandomState(model_seed)\n",
    "    faults = [dict()]\n",
    "    for _ in range(len(faults) - 1):\n",
    "        faults.append(dict(valves_min=random_.rand(6) * 0.5,\n",
    "                            valves_max=random_.rand(6) * 0.5 + 0.5,\n",
    "                            pumps=random_.rand(6) * 1e-2 + 5e-3,\n",
    "                            engines=random_.rand(2) * 2e-2 + 1e-2))\n",
    "\n",
    "env_params = SimpleNamespace(alpha=alpha, model_seed=model_seed, tstep=tstep,\n",
    "                             update_interval=update_interval, timesteps=timesteps,\n",
    "                             random_faults=random_faults, faults=faults, colors=colors, ls=ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task(seed=None, randomize=False, params={}, tstep=tstep):\n",
    "    env = TanksPhysicalEnv(TanksFactory(**params), seed=seed, tstep=tstep)\n",
    "    return env if not randomize else env.randomize()\n",
    "\n",
    "def make_model(seed=None, n_latent_var=n_latent_var, lr=alpha, update_interval=update_interval, epochs=epochs):\n",
    "    return ppo.PPO(\n",
    "        env=None,\n",
    "        policy=ppo.ActorCriticDiscrete,\n",
    "        state_dim=6,\n",
    "        action_dim=6,\n",
    "        n_latent_var=n_latent_var,\n",
    "        lr=lr,\n",
    "        seed=seed,\n",
    "        update_interval=update_interval,\n",
    "        epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read params\n",
    "env_params = utils.read_pickle(datadir / 'env_params')\n",
    "alpha = env_params.alpha\n",
    "model_seed = env_params.model_seed\n",
    "tstep = env_params.tstep\n",
    "update_interval = env_params.update_interval\n",
    "timesteps = env_params.timesteps\n",
    "random_faults = env_params.random_faults\n",
    "colors = env_params.colors\n",
    "ls = env_params.ls\n",
    "faults = env_params.faults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def objective(trial: optuna.Trial):\n",
    "    lr = trial.suggest_loguniform('lr', 1e-5, 1e-1)\n",
    "    n_latent_vars = trial.suggest_int('n_latent_vars', 1, 200, log=True)\n",
    "    update_interval = trial.suggest_int('update_interval', 500, 5000, step=500)\n",
    "    epochs = trial.suggest_int('epochs', 1, 10)\n",
    "    \n",
    "    env_ = get_task(model_seed, tstep=tstep)\n",
    "    agent_ = make_model(model_seed, n_latent_var, lr, update_interval, epochs)\n",
    "    agent_.env = env_\n",
    "    r = agent_.learn(timesteps)\n",
    "    return np.mean(r[-10:])\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=25)\n",
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Reward components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env = get_task(seed=model_seed, params=faults[0])\n",
    "done = False\n",
    "s = env.reset()\n",
    "done = False\n",
    "components = []\n",
    "reward = []\n",
    "while not done:\n",
    "    a = np.zeros(6)\n",
    "    ns, r, done, _ = env.step(a)\n",
    "    comp = env.reward_components(env.t - env.tstep, s, a, ns, done)\n",
    "    components.append(comp)\n",
    "    reward.append(r)\n",
    "    s = ns\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "components = np.asarray(components)\n",
    "for name, comp in zip(('centre', 'activity', 'spread', 'level', 'deficit'),\n",
    "                      np.transpose(components)):\n",
    "    plt.plot(comp, label=name)\n",
    "plt.ylabel('Components')\n",
    "plt.legend()\n",
    "plt.twinx()\n",
    "plt.plot(reward, ls=':', c='black', label='reward')\n",
    "plt.ylabel('Cumulative Reward')\n",
    "plt.legend()\n",
    "plt.xlabel('Timesteps')\n",
    "plt.title('Rewards')\n",
    "plt.savefig(datadir / 'plots' / 'rewards')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nominal vs. faulty rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "rewards = []\n",
    "for f, fault in enumerate(tqdm(faults[:1] + faults[5:], total=len(faults), leave=False)):\n",
    "    env = get_task(seed=model_seed, params=fault)\n",
    "    done = False\n",
    "    env.reset()\n",
    "    reward = []\n",
    "    while not done:\n",
    "        _, r, done, _ = env.step(np.zeros(6))\n",
    "        reward.append(r)\n",
    "    rewards.append(reward)\n",
    "    plt.plot(reward, label='nominal' if f==0 else str(f), ls='-' if f==0 else ':')\n",
    "    print('%2d\\t%4.1f\\t%s' % (f, sum(reward), fault))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL with nominal and faulty env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 6))\n",
    "for f, fault in enumerate(tqdm(faults[:2], total=len(faults), leave=False)):\n",
    "    env = get_task(model_seed, params=fault)\n",
    "    agent = make_model(model_seed)\n",
    "    agent.env = env\n",
    "    r = agent.learn(timesteps * 1.5)\n",
    "    plt.plot(utils.rollmean(r, 5), label='nominal' if f==0 else str(f), ls=':')\n",
    "plt.legend()\n",
    "plt.title('lr=%.3f, tstep=%.3f, interval=%d, steps=%d' % (alpha, tstep, update_interval, timesteps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer RL using proximal policies\n",
    "\n",
    "* Sample memory\n",
    "* Find closest policy in library\n",
    "* Substitute and learn\n",
    "* Add to library and prune\n",
    "\n",
    "Benchmark against:\n",
    "\n",
    "* Vanilla RL\n",
    "* Bayesean policy re-use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt(agent, env, library, timesteps):\n",
    "    m = ppo.Memory()\n",
    "    agent.experience(m, timesteps, env, agent.policy)\n",
    "    idx, vals = meta.rank_library(library, agent, m)\n",
    "    return library[idx[0]], idx[0]\n",
    "\n",
    "def component_agg(store: dict):\n",
    "    def agg(lcl: dict):\n",
    "        info = lcl.get('info')\n",
    "        done = lcl.get('done')\n",
    "        reward = lcl.get('reward')\n",
    "        \n",
    "        if done:\n",
    "            for key, value in info.items():\n",
    "                store[key][-1].append(0.)\n",
    "        else:\n",
    "            for key, value in info.items():\n",
    "                store[key][-1][-1] += value\n",
    "    return agg  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train nominal environment\n",
    "agent = make_model(model_seed) # agent is the global agent class, agent_ is local\n",
    "env = get_task(model_seed)     # env is the global nominal env class, env_ is local\n",
    "agent.env = env\n",
    "nominal_r = agent.learn(timesteps)\n",
    "nominal_params = agent.policy.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policies[0] = nominal_params\n",
    "policies_r[0] = nominal_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train policies for all faults\n",
    "# each entry in faults corresponds to an entry in policies\n",
    "policies = [nominal_params]\n",
    "policies_r = [nominal_r]\n",
    "for fault in tqdm(env_params.faults[1:], leave=False, desc='Training policies for all faults'):\n",
    "    agent_ = make_model(model_seed)\n",
    "    env_ = get_task(model_seed, params=fault)\n",
    "    agent_.env = env_\n",
    "    policies_r.append(agent_.learn(timesteps))\n",
    "    policies.append(agent_.policy.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot policy rewards\n",
    "for i, r in enumerate(policies_r[:1] + policies_r[1:]):\n",
    "    plt.plot(utils.rollmean(r, 5), label='nominal' if i==0 else str(i), ls='-' if i==0 else ':')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# save policies\n",
    "latest_policies_version = now() + 'policies'\n",
    "utils.write_pickle([policies, policies_r, env_params], datadir / latest_policies_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load policies\n",
    "policies, policies_r, env_params = utils.read_pickle(datadir / latest_policies_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the domain of faults to use for the experiments to follow,\n",
    "# if manual=False, domain_* lists are used to to cross-validation splits\n",
    "domain = 'novelpumps'\n",
    "prune = 0\n",
    "manual_split = False\n",
    "\n",
    "if domain in mode_indices:\n",
    "    domain_idx = mode_indices[domain]\n",
    "elif domain.startswith('valves'):\n",
    "    domain_idx = slice(1, 9)\n",
    "elif domain.startswith('novelvalves'):\n",
    "    manual_split = True\n",
    "    train_idx = [[13,14,15,16], [9,10,11,12]]\n",
    "    test_idx = [[5], [6]]\n",
    "elif domain.startswith('novelpumps'):\n",
    "    manual_split = True\n",
    "    train_idx = [[5,6,7,8], [9,10,11,12]]\n",
    "    test_idx = [[13], [14]]\n",
    "elif domain.startswith('novelleaks'):\n",
    "    manual_split = True\n",
    "    train_idx = [[5,6,7,8], [13,14,15,16]]\n",
    "    test_idx = [[9], [10]]\n",
    "elif domain.startswith('leakspumps'):\n",
    "    domain_idx = slice(11,15)\n",
    "elif domain.startswith('valveleaks'):\n",
    "    domain_idx = slice(3, 7)\n",
    "else:\n",
    "    domain_idx = slice(None)\n",
    "\n",
    "if not manual_split:\n",
    "    if isinstance(domain_idx, slice):\n",
    "        domain_faults = faults[domain_idx]\n",
    "        domain_policies = policies[domain_idx]\n",
    "    else:\n",
    "        domain_faults = [faults[i] for i in domain_idx]\n",
    "        domain_policies = [policies[i] for i in domain_idx]\n",
    "    xvalidator = model_selection.LeavePOut(1)\n",
    "    bench_iterator = enumerate(tqdm(xvalidator.split(domain_policies),\n",
    "                              leave=False,\n",
    "                              total=xvalidator.get_n_splits(domain_policies),\n",
    "                              desc='Cross-validation split'))\n",
    "else:\n",
    "    domain_policies = policies\n",
    "    domain_faults = faults\n",
    "    bench_iterator = enumerate(tqdm(zip(train_idx, test_idx),\n",
    "                              leave=False,\n",
    "                              total=len(train_idx),\n",
    "                              desc='Cross-validation split'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment - Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing experiment\n",
    "exp_name = '2104141120-benchmark-cv-%s-prune%d' % (domain, prune)\n",
    "exp_data = utils.read_pickle(datadir / exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up new experiment\n",
    "exp_name = now() + ('benchmark-cv-%s-prune%d' % (domain, prune))\n",
    "exp_data = dict(\n",
    "    vanilla_rs = [],\n",
    "    vanilla_comp = dict(centre=[], activity=[], spread=[], level=[], deficit=[]),\n",
    "    adapt_rs = [],\n",
    "    adapt_comp = dict(centre=[], activity=[], spread=[], level=[], deficit=[]),\n",
    "    other_rs = [],\n",
    "    param_idxs = [],\n",
    "    fault_idxs = [],\n",
    "    time_adapt = [],\n",
    "    time_prune = [],\n",
    "    nominal_r = nominal_r,\n",
    "    domain_idx = domain_idx,\n",
    "    domain = domain,\n",
    "    prune = prune,\n",
    "    kept_idx = [],\n",
    "    env_params = env_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(exp_name)\n",
    "fault_rs = dict()\n",
    "# use subset of faults/policies for train/cross validation\n",
    "for i, (train_faults, test_faults) in bench_iterator:\n",
    "    library = [domain_policies[fidx] for fidx in train_faults]\n",
    "    if prune > 0:\n",
    "        memory = ppo.Memory()\n",
    "        agent.experience(memory, update_interval, agent.env, agent.policy)\n",
    "        start = time.process_time()\n",
    "        library, _, kept_idx = meta.prune_library(library, agent, memory, max_library_size=prune, seed=model_seed)\n",
    "        end = time.process_time()\n",
    "        exp_data['kept_idx'].append(kept_idx)\n",
    "        exp_data['time_prune'].append(end - start)\n",
    "\n",
    "    # Benchmarking\n",
    "    fidx = test_faults[0]\n",
    "    fault = domain_faults[fidx] # only 1 test fault (leave 1 out)\n",
    "    exp_data['fault_idxs'].append(fidx)\n",
    "\n",
    "    # Vanilla RL\n",
    "    agent_ = make_model(model_seed)\n",
    "    agent_.policy.load_state_dict(nominal_params)\n",
    "    env_ = get_task(model_seed, params=fault)\n",
    "    agent_.env = env_\n",
    "    for _, v in exp_data['vanilla_comp'].items(): v.append([0.])\n",
    "    exp_data['vanilla_rs'].append(agent_.learn(timesteps, step_callback=component_agg(exp_data['vanilla_comp'])))\n",
    "\n",
    "    # Proximal policy transfer RL\n",
    "    agent_ = make_model(model_seed)\n",
    "    env_ = get_task(model_seed, params=fault)\n",
    "    agent_.env = env_\n",
    "    m = ppo.Memory()\n",
    "    start = time.process_time()\n",
    "    params, idx = adapt(agent_, env_, [nominal_params] + library, update_interval)\n",
    "    end = time.process_time()\n",
    "    exp_data['time_adapt'].append(end - start)\n",
    "    idx -= 1  # nominal params are prepended to library, converting back to library index\n",
    "              # if -1, means that nominal params were most suitable\n",
    "    exp_data['param_idxs'].append(idx)\n",
    "    agent_.policy.load_state_dict(params)\n",
    "    for _, v in exp_data['adapt_comp'].items(): v.append([0.])\n",
    "    exp_data['adapt_rs'].append(agent_.learn(timesteps, step_callback=component_agg(exp_data['adapt_comp'])))\n",
    "\n",
    "    # Remainder of policies, to see whether the sampled policy is indeed better\n",
    "    r = []\n",
    "    for i, params in enumerate(tqdm(library, leave=False, desc='Library benchmark')):\n",
    "        if i==idx:\n",
    "            continue  # idx was selected policy, already calculated\n",
    "        elif (i, fidx) in fault_rs:\n",
    "            continue\n",
    "        else:\n",
    "            agent_ = make_model(model_seed)\n",
    "            env_ = get_task(model_seed, params=fault)\n",
    "            agent_.env = env_\n",
    "            agent_.policy.load_state_dict(params)\n",
    "            fault_rs[(i, fidx)] = agent_.learn(timesteps)\n",
    "        r.append(agent_.learn(timesteps))\n",
    "    exp_data['other_rs'].append(r)\n",
    "    utils.write_pickle(exp_data, datadir / exp_name) # write with each cv split, so we can check results before cell finishes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabulated numbers\n",
    "exp_name = '2105172156-benchmark-cv-novelpumps-prune0'\n",
    "exp_data = utils.read_pickle(datadir / exp_name)\n",
    "\n",
    "print('ADAPT')\n",
    "a = utils.homogenous_array(exp_data['adapt_rs'])\n",
    "am = np.mean(a, axis=0)\n",
    "aauc = trapz(am)\n",
    "astd = np.std(a, axis=0)\n",
    "print('Total:', aauc)\n",
    "for comp, vals in exp_data['adapt_comp'].items():\n",
    "    c = utils.homogenous_array(vals)\n",
    "    cm = np.mean(c, axis=0)\n",
    "    cauc = trapz(cm)\n",
    "    print(comp, cauc)\n",
    "\n",
    "print('\\nVANILLA')\n",
    "v = utils.homogenous_array(exp_data['vanilla_rs'])\n",
    "vm = np.mean(v, axis=0)\n",
    "vauc = trapz(vm)\n",
    "print('Total:', vauc)\n",
    "for comp, vals in exp_data['vanilla_comp'].items():\n",
    "    c = utils.homogenous_array(vals)\n",
    "    cm = np.mean(c, axis=0)\n",
    "    cauc = trapz(cm)\n",
    "    print(comp, cauc)\n",
    "\n",
    "print('\\nAVERAGE')\n",
    "x = []\n",
    "for other_x in exp_data['other_rs']: x.extend(other_x)\n",
    "x = utils.homogenous_array(x)\n",
    "xm = np.mean(x, axis=0)\n",
    "xauc = trapz(xm)\n",
    "print('Total:', xauc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting params\n",
    "rolling_mean = int(np.sqrt(len(nominal_r)) / 2)\n",
    "nominal_span = rolling_mean * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total Reward performance per split\n",
    "rows = len(exp_data['vanilla_rs'])\n",
    "fig, axes = plt.subplots(rows, 1, figsize=(6, 3 * rows), squeeze=False)\n",
    "axes = np.ravel(axes)\n",
    "for ax, vr, ar, xr, pidx, fidx in zip(axes,\n",
    "                                      exp_data['vanilla_rs'],\n",
    "                                      exp_data['adapt_rs'],\n",
    "                                      exp_data['other_rs'],\n",
    "                                      exp_data['param_idxs'],\n",
    "                                      exp_data['fault_idxs']):\n",
    "    ax.plot(np.arange(-nominal_span+1, 1), nominal_r[-nominal_span:], c='b')\n",
    "    ax.axvline(0, c='r', ls='-.')\n",
    "    ax.plot(utils.rollmean(ar, rolling_mean), c='g')\n",
    "    ax.plot(utils.rollmean(vr, rolling_mean), c='b')\n",
    "    for r in xr:\n",
    "        ax.plot(utils.rollmean(r, rolling_mean), c='g', ls=':')\n",
    "    ax.set_ylabel('Reward / episode')\n",
    "    ax.set_xlabel('Episode')\n",
    "plt.tight_layout()\n",
    "plt.savefig(datadir / 'plots' / exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average reward performance over all splits\n",
    "v = utils.homogenous_array(exp_data['vanilla_rs'])\n",
    "x = []\n",
    "for x_ in exp_data['other_rs']:\n",
    "    x.extend(x_)\n",
    "x = utils.homogenous_array(x)\n",
    "a = utils.homogenous_array(exp_data['adapt_rs'])\n",
    "vstd = utils.rollmean(np.std(v, axis=0), rolling_mean)\n",
    "xstd = utils.rollmean(np.std(x, axis=0), rolling_mean)\n",
    "astd = utils.rollmean(np.std(a, axis=0), rolling_mean)\n",
    "vm = utils.rollmean(np.mean(v, axis=0), rolling_mean)\n",
    "xm = utils.rollmean(np.mean(x, axis=0), rolling_mean)\n",
    "am = utils.rollmean(np.mean(a, axis=0), rolling_mean)\n",
    "plt.plot(np.arange(-nominal_span+1, 1), nominal_r[-nominal_span:], c='b', label='Vanilla RL')\n",
    "plt.gca().axvline(0, c='r', ls='-.', label='Fault')\n",
    "plt.plot(vm, c='b')\n",
    "plt.plot(xm, c='lime', ls='-.', label='Random average')\n",
    "plt.plot(am, c='g', label='Ours')\n",
    "plt.fill_between(np.arange(len(vm)), vm-vstd, vm+vstd, alpha=0.3, color='b')\n",
    "plt.fill_between(np.arange(len(am)), am-astd, am+astd, alpha=0.3, color='g')\n",
    "plt.fill_between(np.arange(len(xm)), xm-xstd, xm+xstd, alpha=0.3, color='lime')\n",
    "plt.ylabel('Reward / episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.tight_layout()\n",
    "plt.savefig(datadir / 'plots' / (exp_name + '-agg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average component performance over all splits\n",
    "vanilla_comp = {key: utils.homogenous_array(val) for key, val in exp_data['vanilla_comp'].items()}\n",
    "adapt_comp = {key: utils.homogenous_array(val) for key, val in exp_data['adapt_comp'].items()}\n",
    "\n",
    "v = utils.homogenous_array(exp_data['vanilla_rs'])\n",
    "a = utils.homogenous_array(exp_data['adapt_rs'])\n",
    "vstd = utils.rollmean(np.std(v, axis=0), rolling_mean)\n",
    "astd = utils.rollmean(np.std(a, axis=0), rolling_mean)\n",
    "vm = utils.rollmean(np.mean(v, axis=0), rolling_mean)\n",
    "am = utils.rollmean(np.mean(a, axis=0), rolling_mean)\n",
    "\n",
    "plt.plot(np.arange(-nominal_span+1, 1), nominal_r[-nominal_span:], c='b', label='Vanilla RL')\n",
    "plt.gca().axvline(0, c='r', ls='-.', label='Fault')\n",
    "plt.plot(vm, c='b')\n",
    "plt.plot(am, c='g', label='Ours')\n",
    "plt.fill_between(np.arange(len(vm)), vm-vstd, vm+vstd, alpha=0.3, color='b')\n",
    "plt.fill_between(np.arange(len(am)), am-astd, am+astd, alpha=0.3, color='g')\n",
    "plt.ylabel('Reward / episode')\n",
    "plt.xlabel('Episode')\n",
    "\n",
    "plt.twinx()\n",
    "handles = []\n",
    "for comp, ls in zip((vanilla_comp, adapt_comp), (':', '-')):\n",
    "    for key, val in comp.items():\n",
    "        handles.append(plt.plot(utils.rollmean(np.mean(val, axis=0), rolling_mean), label=key, ls=ls)[0])\n",
    "    plt.gca().set_prop_cycle(None)\n",
    "plt.legend(handles=handles[:len(handles)//2])\n",
    "plt.tight_layout()\n",
    "plt.savefig(datadir / 'plots' / (exp_name + '-comp-agg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment - Policy distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting policies in relation to each other using Multi-dimensional scaling\n",
    "memory = ppo.Memory()\n",
    "agent.experience(memory, update_interval, agent.env, agent.policy)\n",
    "\n",
    "metrics = {'jensenshannon': None, 'euclidean': None, 'l1': None, 'cosine': None}\n",
    "for metric_name in metrics:\n",
    "    distmat = np.zeros((len(policies), len(policies)))\n",
    "    for p1, p2 in combinations(range(len(policies)), 2):\n",
    "        distmat[p1, p2], distmat[p2, p1] = meta.distance(memory, agent.policy,\n",
    "                                                         policies[p1], policies[p2],\n",
    "                                                         metric=metric_name)\n",
    "    distmat /= np.max(distmat)\n",
    "    proj = manifold.MDS(dissimilarity='precomputed', random_state=model_seed)\n",
    "    coords = proj.fit_transform(distmat)\n",
    "    metrics[metric_name]= dict(distmat=distmat, coords=coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "for i, (metric_name, metric) in enumerate(metrics.items()):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    coords = metric['coords']\n",
    "    for mode, idx in mode_indices.items():\n",
    "        plt.scatter(coords[idx, 0], coords[idx, 1], label=mode, **scatter_fmt[mode])\n",
    "    plt.gca().set_aspect('equal', 'datalim')\n",
    "    plt.gca().set_xlim(-0.6, 0.6)\n",
    "    plt.gca().set_ylim(-0.6, 0.6)\n",
    "    plt.grid(True)\n",
    "    for i in range(len(policies)):\n",
    "        plt.annotate(str(i), coords[i], coords[i]+3, textcoords='offset points')\n",
    "    plt.title('Projection - ' + metric_name)\n",
    "plt.legend()\n",
    "plt.savefig(datadir / 'plots' / 'mds-metrics', pad_inches=0.1, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "axes = []\n",
    "enum_faults = np.arange(len(faults))\n",
    "for i, (metric_name, metric) in enumerate(metrics.items()):\n",
    "    ax = plt.subplot(2, 2, i+1)\n",
    "    axes.append(ax)\n",
    "    distmat = metric['distmat']\n",
    "    im = plt.matshow(distmat, 0)\n",
    "#     plt.colorbar()\n",
    "    for mode, idx in mode_indices.items():\n",
    "        fidx = enum_faults[idx]\n",
    "        start, end = fidx[0], fidx[-1]\n",
    "        plt.axvline(start - 0.5, c=scatter_fmt[mode]['c'])\n",
    "        plt.axvline(end + 0.5, c=scatter_fmt[mode]['c'])\n",
    "        plt.axhline(start - 0.5, c=scatter_fmt[mode]['c'])\n",
    "        plt.axhline(end + 0.5, c=scatter_fmt[mode]['c'])\n",
    "    plt.title(metric_name)\n",
    "# plt.legend()\n",
    "fig.colorbar(im, ax=axes)\n",
    "fig.tight_layout()\n",
    "plt.savefig(datadir / 'plots' / 'distmats-metrics', pad_inches=0, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Experiment contd. - Pruning and policy space\n",
    "\n",
    "* Progressively pruning and which policies are kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "exp_name = now() + 'pruned-space'\n",
    "exp_data = dict(\n",
    "    prunes = [len(policies[1:]), len(policies[1:]) // 2 , len(policies[1:]) // 3, len(policies[1:]) // 4],\n",
    "    distmat = distmat,\n",
    "    coords = coords,\n",
    "    domain_idx = domain_idx,\n",
    "    keeps = [],\n",
    "    env_params = env_params\n",
    ")\n",
    "\n",
    "for prune in tqdm(exp_data['prunes'], leave=False):\n",
    "    memory = ppo.Memory()\n",
    "    agent.experience(memory, update_interval, agent.env, agent.policy)\n",
    "    _, _, keep = meta.prune_library(policies[1:], agent, memory, max_library_size=prune, seed=model_seed)\n",
    "    exp_data['keeps'].append([k+1 for k in keep])  # convert policies[1:] indices to policies indices\n",
    "\n",
    "keeps = exp_data['keeps']\n",
    "prunes = exp_data['prunes']\n",
    "utils.write_pickle(exp_data, datadir / exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rows = (len(exp_data['prunes']) + 1) // 2\n",
    "fig, axes = plt.subplots(rows, 2, figsize=(8, 4 * rows), squeeze=False)\n",
    "coords = exp_data['coords']\n",
    "axes = np.ravel(axes)\n",
    "\n",
    "indices = list(np.arange(len(faults)))\n",
    "midx = [indices[idx] for idx in mode_indices.values()]\n",
    "handles = []\n",
    "for i, (ax, keep) in enumerate(zip(axes, exp_data['keeps'])):\n",
    "    h = ax.scatter(coords[:1,0], coords[:1, 1], s=60, c='black', label='Nominal')\n",
    "    if i==0: handles.append(h)\n",
    "    for mode, idx in zip(mode_indices.keys(), midx):\n",
    "        if mode=='nominal': continue\n",
    "        mode_keep = list(set(idx).intersection(set(keep)))\n",
    "        kept_coords = coords[np.ix_(mode_keep)]\n",
    "        if len(kept_coords):\n",
    "            h = ax.scatter(kept_coords[:,0], kept_coords[:, 1], label=mode, **scatter_fmt[mode])\n",
    "            if i==0: handles.append(h)\n",
    "\n",
    "    ax.set_aspect('equal', 'datalim')\n",
    "    ax.set_xlim(axes[0].get_xlim())\n",
    "    ax.set_ylim(axes[0].get_ylim())\n",
    "    ax.set(xticks=[], yticks=[])\n",
    "    ax.set_title('No pruning' if i==0 else ('%d policies' % len(keep)))\n",
    "\n",
    "plt.legend(handles=handles, fontsize='medium')\n",
    "plt.tight_layout()\n",
    "plt.savefig(datadir / 'plots' / 'mds-prunes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment contd. - Policy Sampling\n",
    "\n",
    "* Finding best policy for each fault"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = now() + 'policy-sampling'\n",
    "exp_data = dict(\n",
    "    fault_idx = [],\n",
    "    selected_idx = [],\n",
    "    env_params = env_params,\n",
    "    faults = faults,\n",
    "    prunes = prunes,\n",
    "    keeps = keeps,\n",
    "    coords = coords\n",
    ")\n",
    "\n",
    "for keep in tqdm(keeps, leave=False):\n",
    "    domain_faults = [faults[k] for k in keep]    # test for all faults (excluding nominal idx==0)\n",
    "    domain_policies = [policies[k] for k in keep]\n",
    "    sample_iterator = model_selection.LeavePOut(1).split(domain_faults)\n",
    "    fidx, sidx = [], []\n",
    "    for (train_idx, test_idx) in tqdm(sample_iterator, leave=False, total=len(domain_faults)):\n",
    "        library = [domain_policies[idx] for idx in train_idx]\n",
    "        agent_ = make_model(model_seed)\n",
    "        env_ = get_task(model_seed, params=domain_faults[test_idx[0]])\n",
    "        _, sample_idx = adapt(agent_, env_, [nominal_params] + library, update_interval)\n",
    "        sample_idx = keep[train_idx[sample_idx - 1]]  # convert to local library idx, and then to idx in domain_policies\n",
    "        fidx.append(keep[test_idx[0]])\n",
    "        sidx.append(sample_idx)\n",
    "    exp_data['fault_idx'].append(fidx)  # convert from domain_faults to faults indexing\n",
    "    exp_data['selected_idx'].append(sidx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = (len(exp_data['prunes']) + 1) // 2\n",
    "fig, axes = plt.subplots(rows, 2, figsize=(8, 4 * rows), squeeze=False)\n",
    "coords = exp_data['coords']\n",
    "axes = np.ravel(axes)\n",
    "\n",
    "indices = list(np.arange(len(faults)))\n",
    "midx = [indices[idx] for idx in mode_indices.values()]\n",
    "handles = []\n",
    "for i, (ax, keep, fidx, sidx) in enumerate(zip(axes, exp_data['keeps'], exp_data['fault_idx'], exp_data['selected_idx'])):\n",
    "    # Plot projections\n",
    "    h = ax.scatter(coords[:1,0], coords[:1, 1], s=60, c='black', label='Nominal')\n",
    "    if i==0: handles.append(h)\n",
    "    for mode, idx in zip(mode_indices.keys(), midx):\n",
    "        if mode=='nominal': continue\n",
    "        mode_keep = list(set(idx).intersection(set(keep)))\n",
    "        kept_coords = coords[np.ix_(mode_keep)]\n",
    "        if len(kept_coords):\n",
    "            h = ax.scatter(kept_coords[:,0], kept_coords[:, 1], label=mode, **scatter_fmt[mode])\n",
    "            if i==0: handles.append(h)\n",
    "    # Plot arrows between projections\n",
    "    for fi, si in zip(fidx, sidx):\n",
    "        dx, dy = coords[si] - coords[fi]\n",
    "        ax.arrow(coords[fi, 0], coords[fi, 1],  dx, dy, length_includes_head=True, width=1e-4, head_width=10e-4, ls=':')\n",
    "    \n",
    "    for no in keep:\n",
    "        ax.annotate(str(no), coords[no], coords[no]+3, textcoords='offset points')\n",
    "\n",
    "    ax.set_aspect('equal', 'datalim')\n",
    "    ax.set_xlim(axes[0].get_xlim())\n",
    "    ax.set_ylim(axes[0].get_ylim())\n",
    "    ax.set(xticks=[], yticks=[])\n",
    "    ax.set_title('No pruning' if i==0 else ('%d policies' % len(keep)))\n",
    "plt.tight_layout()\n",
    "plt.legend(handles=handles, fontsize='medium')\n",
    "plt.savefig(datadir / 'plots' / 'mds-sampling')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
