{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import notebook_setup\n",
    "\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "import warnings\n",
    "import os\n",
    "# warnings.filterwarnings(\"error\", category=UserWarning)\n",
    "os.makedirs(os.path.expanduser('~/Data/tensorboard/'), exist_ok=True)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gym\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV, ParameterGrid\n",
    "from sklearn.metrics import make_scorer, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm, trange\n",
    "from pytorchbridge import TorchEstimator\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from ppo import PPO, Memory, ActorCriticMultiBinary, returns\n",
    "from systems import TanksPhysicalEnv, TanksFactory, plot_tanks\n",
    "from utils import (cache_function, cache_to_episodic_rewards,\n",
    "                   cache_to_episodes, copy_tensor, copy_mlp_regressor,\n",
    "                   sanitize_filename, get_gradients)\n",
    "from meta import (learn_env_model, meta_update, distance, prune_library,\n",
    "                  plot_adaption, rank_policies, maml_initialize)\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED = 0\n",
    "NCPU = os.cpu_count() // 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'Fuel-tanks'\n",
    "n_tanks = 6\n",
    "n_engines = 2\n",
    "tstep = 1e0\n",
    "env = TanksPhysicalEnv(TanksFactory(), seed=SEED, tstep=tstep)\n",
    "PhysicalEnv = TanksPhysicalEnv\n",
    "DataEnv = None\n",
    "nominal_config = dict(\n",
    "    heights = np.ones(n_tanks),\n",
    "    cross_section = np.ones(n_tanks),\n",
    "    valves_min = np.zeros(n_tanks),\n",
    "    valves_max = np.ones(n_tanks),\n",
    "    resistances = np.ones(n_tanks) * 1e2,\n",
    "    pumps = np.ones(n_tanks) * 0.1,\n",
    "    engines = np.ones(n_engines) * 0.05\n",
    ")\n",
    "env_fn = lambda seed=SEED: PhysicalEnv(TanksFactory(), seed=seed, tstep=tstep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tanks_ = TanksFactory(n = n_tanks, e = n_engines, **nominal_config)\n",
    "env_ = TanksPhysicalEnv(tanks_, tstep)\n",
    "env_.reset()\n",
    "d = False\n",
    "i = 0\n",
    "R = 0\n",
    "while not d and i < 20:\n",
    "    i += 1\n",
    "    s, r, d, _ = env_.step(env_.action_space.sample())\n",
    "    R += r\n",
    "    print(i, s, '{:.2f}'.format(sum(s)), r)\n",
    "print(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tanks(env_, columns=n_tanks, single_size=(3,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### System model\n",
    "\n",
    "* TODO: Check if training data tuples are actually causal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degradation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def degrade(env, time: float, tfactor: np.ndarray,\n",
    "            efactor: np.ndarray, **nominal):\n",
    "    tanks = env.tanks\n",
    "    if not isinstance(tfactor, (list, tuple, np.ndarray)):\n",
    "        # If a single degradation factor given, assume it is\n",
    "        # identical for all tanks.\n",
    "        tfactor = np.ones(n_tanks) * tfactor\n",
    "    if not isinstance(efactor, (list, tuple, np.ndarray)):\n",
    "        # If a single degradation factor given, assume it is\n",
    "        # identical for all engines.\n",
    "        efactor = np.ones(n_engines) * efactor\n",
    "    for i in range(n_tanks):\n",
    "        tanks.pumps[i] = nominal['pumps'][i] * (1 - time / tfactor[i])\n",
    "        tanks.resistances[i] = nominal['resistances'][i] + \\\n",
    "                               nominal['resistances'][i] * time / tfactor[i]\n",
    "    for i in range(n_engines):\n",
    "        tanks.engines[i] = nominal['engines'][i] + \\\n",
    "                           nominal['engines'][i] * time / efactor[i]\n",
    "\n",
    "\n",
    "def random_degrade(env, random=np.random,\n",
    "                   tfactors=(10, 20), efactors=(10, 20),\n",
    "                   atmost_tanks=1, atmost_engines=1):\n",
    "    n_tanks = len(tanks.heights)\n",
    "    tanks = env.tanks\n",
    "    tfactor = np.ones(n_tanks) * np.inf\n",
    "    if atmost_tanks > 0:\n",
    "        tanks_affected = random.randint(1, atmost_tanks + 1)\n",
    "        idx_affected = random.choice(n_tanks, size=tanks_affected, replace=False)\n",
    "        tfactor[idx_affected] = random.randint(*tfactors, size=tanks_affected)\n",
    "        \n",
    "    efactor = np.ones(n_engines) * np.inf\n",
    "    if atmost_engines > 0:\n",
    "        engines_affected = random.randint(1, atmost_engines + 1)\n",
    "        idx_affected = random.choice(n_engines, size=engines_affected, replace=False)\n",
    "        efactor[idx_affected] = random.randint(*efactors, size=engines_affected)\n",
    "    \n",
    "    degrade(env, min([t if t != np.inf else 0 for t in tfactor]), tfactor, efactor, **nominal_config)\n",
    "    return tfactor, efactor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced MAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = env_fn()\n",
    "timesteps = 30000            # max timesteps in one episode\n",
    "\n",
    "ppo_params = dict(\n",
    "    state_dim = env.observation_space.shape[0],\n",
    "    action_dim = 6,\n",
    "    policy=ActorCriticMultiBinary,\n",
    "    epochs = 3,                  # update policy for K epochs\n",
    "    lr = 0.002,                   # learning rate\n",
    "    n_latent_var = 64,           # number of variables in hidden layer\n",
    "    betas = (0.9, 0.999),\n",
    "    gamma = 0.99,                # discount factor\n",
    "    eps_clip = 0.2,              # clip parameter for PPO\n",
    "    update_interval = 1000,      # update policy every n timesteps\n",
    "    seed = SEED\n",
    ")\n",
    "library_size = 4\n",
    "random_library = False\n",
    "random_fault = False\n",
    "data_model = False\n",
    "random = np.random.RandomState(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nominal agent trained without fault\n",
    "agent = PPO(env, **ppo_params)\n",
    "r = agent.learn(timesteps, track_higher_gradients=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(len(r)), r)\n",
    "plt.title('Rewards on system under nominal conditions');\n",
    "# plt.ylim(bottom=0, top=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate library of policies\n",
    "library = []\n",
    "library_rewards = []\n",
    "library_envs = []\n",
    "library_grads = []\n",
    "\n",
    "if random_library:\n",
    "    library_envs = [random_degrade(env_fn(), random) for _ in range(library_size)]\n",
    "else:\n",
    "    env_params = [\n",
    "        dict(resistances=[100, 100, 100, 70,  80,   90], pumps=[0.1, 0.1, 0.1, 0.,  0.1, 0.1], engines=[0.05, 0.1]),\n",
    "        dict(resistances=[100, 100, 100, 150, 200, 100], pumps=[0.1, 0.1, 0.1, 0.1, 0.1, 0.1], engines=[0.1, 0.05]),\n",
    "        dict(resistances=[90,  100, 100, 70,   80,  90], pumps=[0.1, 0.1, 0.,  0.1, 0.1, 0.1], engines=[0.05, 0.1]),\n",
    "        dict(resistances=[100,  75, 100, 100,  75, 100], pumps=[0.1, 0. , 0.1, 0.1, 0.1, 0.1], engines=[0.05, 0.1]),\n",
    "    ]\n",
    "    for env_param in env_params:\n",
    "        env_ = env_fn()\n",
    "        env_.set_parameters(**env_param)\n",
    "        library_envs.append(env_)\n",
    "\n",
    "# factors = [random_degrade(**nominal_config) for _ in range(3)]\n",
    "\n",
    "for env_ in tqdm(library_envs, leave=False):\n",
    "    # introduce some fault and learn data-driven model\n",
    "    # degrade(env_, factor, **nominal_config)\n",
    "    if data_model:\n",
    "        est_ = copy_mlp_regressor(est)  # copy estimator hyperparameters etc.\n",
    "        x, y = generate_training_data(env_, episodes=50)  # random actions!\n",
    "        est_.fit(x, y)\n",
    "        # Train agent on data-driven model\n",
    "        env_ = DataEnv(env_, est_)\n",
    "    agent_ = PPO(env_, **ppo_params)\n",
    "    agent_.policy.load_state_dict(copy_tensor(agent.policy.state_dict()))\n",
    "    rewards = agent_.learn(timesteps, track_higher_gradients=True)\n",
    "    library.append(copy_tensor(agent_.policy.state_dict()))\n",
    "    library_rewards.append(rewards)\n",
    "    library_grads.append(get_gradients(agent_.meta_policy.parameters(), agent_.meta_policy.parameters(time=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot library rewards\n",
    "plt.figure(figsize=(6, 4))\n",
    "for i, rewards in enumerate(library_rewards):\n",
    "    plt.plot(rewards, label='Policy#{}'.format(i))\n",
    "plt.title('Episodic rewards on process with faults')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Reward')\n",
    "plt.legend()\n",
    "plt.grid(True, 'both')\n",
    "plt.tight_layout()\n",
    "\n",
    "pth = f'./bin/{env_name}/'\n",
    "os.makedirs(pth, exist_ok=True)\n",
    "plt.savefig(pth+env_name+'_library_rewards.png')\n",
    "with open(pth + env_name + '_library_rewards.pickle', 'wb') as f:\n",
    "    pickle.dump(dict(library_rewards=library_rewards), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faulty environment\n",
    "env_ = env_fn()\n",
    "if not random_fault:\n",
    "    env_.set_parameters(\n",
    "        resistances=[90, 80, 70, 100, 100, 80],\n",
    "        engines=[0.075, 0.125],\n",
    "        pumps=[0.1, 0.1, 0.,  0.1, 0.1, 0.1]\n",
    "    )\n",
    "else:\n",
    "    random_degrade(env_, random, **nominal_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Experiment functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def adapt(agent, est, memory, library, interactive_env=True,\n",
    "          n_inner=1, n_outer=1, alpha_inner=0.01, alpha_outer=0.1,\n",
    "          mode='fomaml', library_grads=None, rank=-1, **ppo_params):\n",
    "    env_ = agent.env\n",
    "    params = meta_update(agent.policy.state_dict(), env_, library, memory,\n",
    "                         n_inner, n_outer, alpha_inner, alpha_outer,\n",
    "                         interactive_env, mode, library_grads, rank, **ppo_params)\n",
    "    agent.policy.load_state_dict(params)\n",
    "    return agent\n",
    "\n",
    "def adapt_benchmark(agent, est, memory, library, interactive_env=True,\n",
    "          n_inner=1, n_outer=1, alpha_inner=0.01, alpha_outer=0.1,\n",
    "           mode=None, library_grads=None, rank=None, **ppo_params):\n",
    "    env_ = agent.env\n",
    "    agent.learn(ppo_params['update_interval'], ppo_params['update_interval'])\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def trial(env_, est, starting_policy, library=[], interactive_env=False, post_steps=10000,\n",
    "          n_inner=1, n_outer=1, alpha_inner=0.01, alpha_outer=0.1,\n",
    "          mode='fomaml', library_grads=None, rank=-1, benchmark=True, seed=SEED):\n",
    "    # local copy of ppo_params with seed overwritten with the trial seed\n",
    "    ppo_params['seed'] = seed\n",
    "    env_.seed(seed)\n",
    "    # Make copies of env, and agent trained on nominal system,\n",
    "    # and starting library of policies (if any)\n",
    "    library_ = [copy_tensor(p) for p in library]\n",
    "    agent_ = PPO(env_, **ppo_params)\n",
    "    agent_.policy.load_state_dict(copy_tensor(starting_policy))\n",
    "    # Fault occurs, buffer experience with environment\n",
    "    memory_ = Memory()\n",
    "    agent_.experience(memory_, 4*ppo_params['update_interval'], env_, agent_.policy)\n",
    "    # Use meta-learning to adapt to fault\n",
    "    agent_.env.reset()\n",
    "    adapt(agent_, est, memory_, library_, interactive_env,\n",
    "          n_inner, n_outer, alpha_inner, alpha_outer,\n",
    "          mode, library_grads, rank, **ppo_params)\n",
    "    if benchmark:\n",
    "        library_maml, gradients_maml = \\\n",
    "            maml_initialize(starting_policy, env_fn, library_size,\n",
    "                            n_inner, alpha_inner, **ppo_params)\n",
    "        agent_benchmark_maml = PPO(env_, **ppo_params)\n",
    "        agent_benchmark_maml.env.seed(seed)\n",
    "        agent_benchmark_maml.env.reset()\n",
    "        agent_benchmark_maml.policy.load_state_dict(copy_tensor(starting_policy))\n",
    "        adapt(agent_benchmark_maml, est, memory_, library_maml, True,\n",
    "              0, n_outer, alpha_inner, alpha_outer,\n",
    "              mode, gradients_maml, rank=-1, **ppo_params)\n",
    "        \n",
    "        agent_benchmark_vanilla = PPO(env_, **ppo_params)\n",
    "        agent_benchmark_vanilla.env.seed(seed)\n",
    "        agent_benchmark_vanilla.env.reset()\n",
    "        agent_benchmark_vanilla.policy.load_state_dict(copy_tensor(starting_policy))\n",
    "        adapt_benchmark(agent_benchmark_vanilla, est, memory_, library_, data_model,\n",
    "                        n_inner, n_outer, alpha_inner, alpha_outer,\n",
    "                        **ppo_params)\n",
    "        \n",
    "    # Continue learning\n",
    "    rewards = []\n",
    "    agents = [agent_, agent_benchmark_maml, agent_benchmark_vanilla] if benchmark else [agent_]\n",
    "    for a in tqdm(agents, desc='Post-fault training', leave=False):\n",
    "        rewards.append(a.learn(post_steps))\n",
    "    return rewards, agent_.policy.state_dict(), memory_\n",
    "\n",
    "\n",
    "\n",
    "def ntrials(n=NCPU, verbose=10, *trial_args, **trial_kwargs):\n",
    "\n",
    "    res = Parallel(n_jobs=min(n, NCPU), verbose=verbose)(\n",
    "        delayed(trial)(*trial_args, seed=SEED+i, **trial_kwargs) for i in range(n)\n",
    "    )\n",
    "    # res = [\n",
    "    #   [[[r..],[r..],[r..]], state_dict, memory]\n",
    "    # ]\n",
    "    n_rewards = len(res[0][0]) # our approach, maml, vanilla ppo\n",
    "    means, stds = [], []\n",
    "    for reward_idx in range(n_rewards):\n",
    "        maxlen = max([len(r[0][reward_idx]) for r in res])\n",
    "        rewards = np.empty((len(res), maxlen))\n",
    "        rewards.fill(np.nan)\n",
    "        for i, result in enumerate(res):\n",
    "            rewards[i, :len(result[0][reward_idx])] = result[0][reward_idx]\n",
    "        means.append(np.nanmean(rewards, axis=0))\n",
    "        stds.append(np.nanstd(rewards, axis=0))\n",
    "    return means, stds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Grid-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "grid = ParameterGrid(dict(\n",
    "    alpha_inner = [1e-3, 1e-2, 1e-1],\n",
    "    alpha_outer = [1e-3, 1e-2, 1e-1],\n",
    "    n_inner = [0, 3],\n",
    "    n_outer = [1, 3],\n",
    "    data_model = [False],\n",
    "    post_steps = [30000],\n",
    "    library = [library],\n",
    "    library_grads = [library_grads],\n",
    "    rank = [-1, 1, 2],\n",
    "    mode = ['maml', 'fomaml', 'reptile']\n",
    "))\n",
    "\n",
    "pth = f'./bin/{env_name}/hyperparameters/'\n",
    "os.makedirs(pth, exist_ok=True)\n",
    "\n",
    "env_ = env_fn()\n",
    "env_.set_parameters(\n",
    "    masscart=1.5,\n",
    "    masspole=0.1,\n",
    "    length=0.5,\n",
    "    force_mag=-10,\n",
    ")\n",
    "\n",
    "hyp_r, hyp_std, hyp_rb, hyp_stdb = [], [], [], []\n",
    "ngrid = 0\n",
    "for trial_params in tqdm(grid, desc='Hyperparameters', leave=False):\n",
    "    \n",
    "    (r, r_b), (std, std_b) = ntrials(3, 10, env_, None, agent.policy.state_dict(), **trial_params)\n",
    "    \n",
    "    hyp_r.append(r)\n",
    "    hyp_rb.append(r_b)\n",
    "    hyp_std.append(std)\n",
    "    hyp_stdb.append(std_b)\n",
    "    ngrid += 1\n",
    "    \n",
    "    fname = pth + env_name + '_' + \\\n",
    "            sanitize_filename(str({k:v for k,v in trial_params.items() if not k.startswith('library')}))\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plot_adaption((r, r_b), (std, std_b), ('e-MAML', 'Vanilla RL'));\n",
    "    plt.savefig(fname + '.png')\n",
    "    with open(fname+'.pickle', 'wb') as f:\n",
    "        pickle.dump(dict(trial_params=trial_params, results=(r, r_b, std, std_b)), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "for trial_params, r, r_b, std, std_b in tqdm(zip(grid, hyp_r, hyp_std, hyp_rb, hyp_stdb), leave=False, total=len(grid)):\n",
    "    \n",
    "    fname = pth + env_name + '_' + \\\n",
    "            sanitize_filename(str({k:v for k,v in trial_params.items() if not k.startswith('library')}))\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plot_adaption((r, r_b), (std, std_b), ('e-MAML', 'Vanilla RL'));\n",
    "    plt.savefig(fname + '.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    i+=1\n",
    "    if i==2: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ncol = 2\n",
    "nrow = ngrid // ncol + (ngrid % ncol != 0)\n",
    "plt.figure(figsize=(12, 3 * nrow))\n",
    "for i, (grid_params, (r, r_b, std, std_b)) in enumerate(zip(grid, zip(hyp_r, hyp_rb, hyp_std, hyp_stdb))):\n",
    "    plt.subplot(nrow, ncol, i + 1)\n",
    "    plot_adaption((r, r_b), (std, std_b), ('e-MAML', 'Vanilla RL'));\n",
    "    print([(k, len(v) if k=='library' else v) for k, v in grid_params.items() if k not in ['post_steps']])\n",
    "    plt.title(i)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Performance-weighed sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Most favorable policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = PPO(env_, **ppo_params)\n",
    "a.policy.load_state_dict(agent.policy.state_dict())\n",
    "m = Memory()\n",
    "a.experience(m, ppo_params['update_interval'], env_, a.policy)\n",
    "\n",
    "ret = torch.tensor(returns(m.rewards, m.is_terminals, a.gamma)).float().to(DEVICE)\n",
    "ret = (ret - ret.mean()) / (ret.std() + 1e-5)\n",
    "states = torch.tensor(m.states).float().to(DEVICE).detach()\n",
    "actions = torch.tensor(m.actions).float().to(DEVICE).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "args, vals = rank_policies(m, library, **ppo_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.asarray(vals)[args]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pth = f'./bin/{env_name}/rnd_vs_lib/'\n",
    "os.makedirs(pth, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trial_params = dict(\n",
    "    alpha_inner = 1e-3,\n",
    "    alpha_outer = 2e-3,\n",
    "    n_inner = 3,\n",
    "    n_outer = 3,\n",
    "    interactive_env = False,\n",
    "    post_steps = 30000,\n",
    "    library = library,\n",
    "    library_grads = library_grads,\n",
    "    mode = 'maml',\n",
    "    rank = 2,\n",
    "    benchmark=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env_.reset()\n",
    "(r, r_m, r_v), (std, std_m, std_v) = ntrials(1, 20, env_, None, agent.policy.state_dict(), **trial_params)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plot_adaption((r, r_m, r_v), (std, std_m, std_v), ('e-MAML', 'MAML', 'Vanilla RL'))\n",
    "\n",
    "fname = pth + env_name + '_' + sanitize_filename(str({k:v for k,v in trial_params.items() if not k.startswith('library')}))\n",
    "\n",
    "# plt.savefig(fname + '.png')\n",
    "# with open(fname+'.pickle', 'wb') as f:\n",
    "#     pickle.dump(dict(trial_params=trial_params, results=((r, r_m, r_v), (std, std_m, std_v))), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trial_params['rank'] = -1\n",
    "trial_params['benchmark'] = False\n",
    "\n",
    "env_.reset()\n",
    "(r2,), (std2,) = ntrials(1, 20, env_, None, agent.policy.state_dict(), **trial_params)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plot_adaption((r2, r, r_v), (std2, std, std_v), ('e-MAML (4)', 'e-MAML(2)', 'Vanilla RL'));\n",
    "\n",
    "fname = pth + env_name + '_' + sanitize_filename(str({k:v for k,v in trial_params.items() if  not k.startswith('library')}))\n",
    "\n",
    "# plt.savefig(fname + '.png')\n",
    "# with open(fname+'.pickle', 'wb') as f:\n",
    "#     pickle.dump(dict(trial_params=trial_params, results=(r, r_b, std, std_b)), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plot_adaption((r2, r, r_m, r_v), (std2, std, std_m, std_v), ('e-MAML (rank=4)', 'e-MAML (rank=2)', 'MAML', 'Vanilla RL'))\n",
    "plt.title('Performance-weighed sampling comparison')\n",
    "plt.tight_layout()\n",
    "fname = pth + env_name + '_' + 'rank_comparison'\n",
    "plt.savefig(fname + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy complement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning complement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = []\n",
    "comp_rewards = []\n",
    "comp_envs = []\n",
    "\n",
    "e_params = [\n",
    "        dict(resistances=[100, 100, 100, 70,  80,   90], pumps=[0.1, 0.1, 0.1, 0.,  0.1, 0.1], engines=[0.05, 0.1]),\n",
    "        dict(resistances=[100, 100, 100, 70,  80,   90], pumps=[0. , 0.1, 0.1, 0.,  0.1, 0.1], engines=[0.05, 0.1]),\n",
    "        dict(resistances=[100, 100, 100, 150, 200, 100], pumps=[0.1, 0.1, 0.1, 0.1, 0.1, 0.1], engines=[0.1, 0.05]),\n",
    "        dict(resistances=[100, 100, 100, 150, 200, 100], pumps=[0.1, 0.1, 0.1, 0.1, 0. , 0. ], engines=[0.1, 0.05]),\n",
    "        dict(resistances=[90,  100, 100, 70,   80,  90], pumps=[0.1, 0.1, 0.,  0.1, 0.1, 0.1], engines=[0.05, 0.1]),\n",
    "        dict(resistances=[90,  100, 100, 70,   80,  90], pumps=[0. , 0.1, 0.,  0.1, 0.1, 0. ], engines=[0.05, 0.1]),\n",
    "        dict(resistances=[100,  75, 100, 100,  75, 100], pumps=[0.1, 0. , 0.1, 0.1, 0.1, 0.1], engines=[0.05, 0.1]),\n",
    "]\n",
    "for e_param in e_params:\n",
    "    e = env_fn()\n",
    "    e.set_parameters(**e_param)\n",
    "    comp_envs.append(e)\n",
    "\n",
    "# factors = [random_degrade(**nominal_config) for _ in range(3)]\n",
    "\n",
    "for e in tqdm(comp_envs, leave=False):\n",
    "    agent_ = PPO(e, **ppo_params)\n",
    "    agent_.policy.load_state_dict(copy_tensor(agent.policy.state_dict()))\n",
    "    rewards = agent_.learn(50000, track_higher_gradients=False)\n",
    "    comp.append(copy_tensor(agent_.policy.state_dict()))\n",
    "    comp_rewards.append(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = PPO(env, **ppo_params)\n",
    "a.policy.load_state_dict(agent.policy.state_dict())\n",
    "m = Memory()\n",
    "a.experience(m, ppo_params['update_interval'], env, a.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_, divmat, idx = prune_library(comp, library_size, m, **ppo_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divmat.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pth = f'./bin/{env_name}/complement/'\n",
    "os.makedirs(pth, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_params = dict(\n",
    "    alpha_inner = 1e-3,\n",
    "    alpha_outer = 2e-3,\n",
    "    n_inner = 3,\n",
    "    n_outer = 3,\n",
    "    interactive_env = False,\n",
    "    post_steps = 30000,\n",
    "    library = comp_,\n",
    "    library_grads = None,\n",
    "    mode = 'fomaml',\n",
    "    rank = -1,\n",
    "    benchmark=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_.reset()\n",
    "(r, r_m, r_v), (std, std_m, std_v) = ntrials(1, 20, env_, None, agent.policy.state_dict(), **trial_params)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plot_adaption((r, r_m, r_v), (std, std_m, std_v), ('e-MAML(most divergent)', 'MAML', 'Vanilla RL'))\n",
    "\n",
    "fname = pth + env_name + '_' + sanitize_filename(str({k:v for k,v in trial_params.items() if not k.startswith('library')}))\n",
    "\n",
    "# plt.savefig(fname + '.png')\n",
    "# with open(fname+'.pickle', 'wb') as f:\n",
    "#     pickle.dump(dict(trial_params=trial_params, results=((r, r_m, r_v), (std, std_m, std_v))), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_params['library'] = [comp[i] for i in range(len(comp)) if i not in idx[0][1]]\n",
    "trial_params['rank'] = -1\n",
    "trial_params['benchmark'] = False\n",
    "\n",
    "env_.reset()\n",
    "(r2,), (std2,) = ntrials(1, 20, env_, None, agent.policy.state_dict(), **trial_params)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plot_adaption((r, r2, r_v), (std, std2, std_v), ('e-MAML (most divergent)', 'e-MAML(least divergent)', 'Vanilla RL'));\n",
    "\n",
    "fname = pth + env_name + '_' + sanitize_filename(str({k:v for k,v in trial_params.items() if  not k.startswith('library')}))\n",
    "\n",
    "# plt.savefig(fname + '.png')\n",
    "# with open(fname+'.pickle', 'wb') as f:\n",
    "#     pickle.dump(dict(trial_params=trial_params, results=(r, r_b, std, std_b)), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plot_adaption((r, r2, r_m, r_v), (std, std2, std_m, std_v), ('e-MAML (4 most divergent)', 'e-MAML (3 least divergent)', 'MAML', 'Vanilla RL'))\n",
    "plt.title('Complement divergence comparison')\n",
    "plt.tight_layout()\n",
    "fname = pth + env_name + '_' + 'divergence_comparison'\n",
    "plt.savefig(fname + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plotting env actions\n",
    "i=0\n",
    "for e, p in tqdm(zip(comp_envs, comp), total=len(comp), leave=False):\n",
    "    e.seed(SEED)\n",
    "    a = PPO(e, **ppo_params)\n",
    "    a.policy.load_state_dict(p)\n",
    "    plot_tanks(e, a, columns=n_tanks, single_size=(3,1.2), legend=(i == len(comp) - 1))\n",
    "    plt.show()\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## $\\Delta \\theta$ Approximations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pth = f'./bin/{env_name}/deltatheta/'\n",
    "os.makedirs(pth, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trial_params = dict(\n",
    "    alpha_inner = 1e-3,\n",
    "    alpha_outer = 2e-3,\n",
    "    n_inner = 3,\n",
    "    n_outer = 3,\n",
    "    interactive_env = False,\n",
    "    post_steps = 30000,\n",
    "    library = library,\n",
    "    library_grads = library_grads,\n",
    "    mode = 'maml',\n",
    "    rank = 2,\n",
    "    benchmark=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env_.reset()\n",
    "(r, r_m, r_v), (std, std_m, std_v) = ntrials(1, 20, env_, None, agent.policy.state_dict(), **trial_params)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plot_adaption((r, r_v), (std, std_v), ('MAML', 'Vanilla RL'))\n",
    "\n",
    "fname = pth + env_name + '_' + sanitize_filename(str({k:v for k,v in trial_params.items() if not k.startswith('library')}))\n",
    "\n",
    "# plt.savefig(fname + '.png')\n",
    "# with open(fname+'.pickle', 'wb') as f:\n",
    "#     pickle.dump(dict(trial_params=trial_params, results=((r, r_m, r_v), (std, std_m, std_v))), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trial_params['mode'] = 'fomaml'\n",
    "trial_params['benchmark'] = False\n",
    "\n",
    "env_.reset()\n",
    "(r2,), (std2,) = ntrials(1, 20, env_, None, agent.policy.state_dict(), **trial_params)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plot_adaption((r, r2, r_v), (std, std2, std_v), ('MAML', 'FOMAML', 'Vanilla RL'));\n",
    "\n",
    "fname = pth + env_name + '_' + sanitize_filename(str({k:v for k,v in trial_params.items() if  not k.startswith('library')}))\n",
    "\n",
    "# plt.savefig(fname + '.png')\n",
    "# with open(fname+'.pickle', 'wb') as f:\n",
    "#     pickle.dump(dict(trial_params=trial_params, results=(r, r_b, std, std_b)), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trial_params['mode'] = 'reptile'\n",
    "trial_params['benchmark'] = False\n",
    "\n",
    "env_.reset()\n",
    "(r3,), (std3,) = ntrials(1, 20, env_, None, agent.policy.state_dict(), **trial_params)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plot_adaption((r, r2, r_v), (std, std2, std_v), ('e-MAML (most divergent)', 'e-MAML(least divergent)', 'Vanilla RL'));\n",
    "\n",
    "fname = pth + env_name + '_' + sanitize_filename(str({k:v for k,v in trial_params.items() if  not k.startswith('library')}))\n",
    "\n",
    "# plt.savefig(fname + '.png')\n",
    "# with open(fname+'.pickle', 'wb') as f:\n",
    "#     pickle.dump(dict(trial_params=trial_params, results=(r, r_b, std, std_b)), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plot_adaption((r, r2, r3), (std, std2, std3), ('MAML', 'FOMAML', 'REPTILE'))\n",
    "plt.title('Update step approximation comparison')\n",
    "plt.tight_layout()\n",
    "fname = pth + env_name + '_' + 'deltatheta_comparison_' + ('ranked' if trial_params['rank'] > 0 else 'unranked')\n",
    "plt.savefig(fname + '.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
