{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import notebook_setup\n",
    "\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "import warnings\n",
    "import os\n",
    "# warnings.filterwarnings(\"error\", category=UserWarning)\n",
    "os.makedirs(os.path.expanduser('~/Data/tensorboard/'), exist_ok=True)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gym\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV, ParameterGrid\n",
    "from sklearn.metrics import make_scorer, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm, trange\n",
    "from pytorchbridge import TorchEstimator\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from ppo import PPO, Memory, ActorCriticDiscrete, returns\n",
    "from systems import CartPoleEnv, CartPoleDataEnv, plot_cartpole\n",
    "from utils import (cache_function, cache_to_episodic_rewards,\n",
    "                   cache_to_episodes, copy_tensor, copy_mlp_regressor,\n",
    "                   sanitize_filename, get_gradients)\n",
    "from meta import (learn_env_model, meta_update, distance, prune_library,\n",
    "                  plot_adaption, rank_policies, maml_initialize)\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED = 0\n",
    "NCPU = os.cpu_count() // 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'Cartpole'\n",
    "env = CartPoleEnv(seed=SEED)\n",
    "PhysicalEnv = CartPoleEnv\n",
    "DataEnv = CartPoleDataEnv\n",
    "nominal_config = dict(\n",
    "    masscart=1.0,\n",
    "    masspole=0.1,\n",
    "    length=1.0,\n",
    "    force_mag=10.0\n",
    ")\n",
    "env_fn = lambda seed=SEED: PhysicalEnv(seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### System model\n",
    "\n",
    "* TODO: Check if training data tuples are actually causal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Generate training data\n",
    "def generate_training_data(env, episodes=50):\n",
    "    Xtrain, Ytrain = [], []\n",
    "    x = env.reset()\n",
    "    e = 0\n",
    "    while e < episodes:\n",
    "        u = env.action_space.sample()\n",
    "        Xtrain.append(np.concatenate((x, (u,))))\n",
    "        x, _, done, _ = env.step(u)\n",
    "        Ytrain.append(x)\n",
    "        if done:\n",
    "            e += 1\n",
    "            x = env.reset()\n",
    "    return np.asarray(Xtrain, dtype=np.float32), np.asarray(Ytrain, dtype=np.float32)\n",
    "\n",
    "Xtrain, Ytrain = generate_training_data(env, episodes=50)\n",
    "print(Xtrain.shape, Ytrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid = GridSearchCV(MLPRegressor(), scoring=make_scorer(r2_score, multioutput='uniform_average'),\n",
    "                   param_grid={\n",
    "                       'hidden_layer_sizes': ((32, 32), (64, 64)),\n",
    "                       'activation': ('relu', 'logistic'),\n",
    "                       'learning_rate_init': (1e-2, 1e-3),\n",
    "                       'warm_start': (True,)\n",
    "                   },\n",
    "                   n_jobs=12, verbose=1)\n",
    "grid.fit(Xtrain, Ytrain)\n",
    "pd.DataFrame(grid.cv_results_).sort_values(by='rank_test_score', ascending=True, axis=0).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train on episodes-1, validate on 1 episode worth of instances:\n",
    "est = grid.best_estimator_\n",
    "# Plot performance\n",
    "\n",
    "plot_cartpole(CartPoleDataEnv(env, est))\n",
    "plt.suptitle('');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degradation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def degrade(env, factor, **nominal):\n",
    "    if isinstance(factor, (int, float)):\n",
    "        factor = np.ones(len(nominal)) * factor\n",
    "    else:\n",
    "        factor = np.asarray(factor)\n",
    "    params = {k: v*(1+f) for (k, v), f in zip(nominal.items(), factor)}\n",
    "    env.set_parameters(**params)\n",
    "\n",
    "\n",
    "def random_degrade(env=None, random=np.random.RandomState(SEED), **nominal):\n",
    "    env = CartPoleEnv() if env is None else env\n",
    "    feature_size = len(CartPoleEnv.params)\n",
    "    feature_min = np.asarray([0.1, 0.01, 0.1, -20.])\n",
    "    feature_max = np.asarray([5.0, 0.50, 2.5, +20.])\n",
    "    feature_min_abs = np.asarray([0.1, 0.01, 0.1, 2.])\n",
    "\n",
    "    if isinstance(random, np.random.RandomState):\n",
    "        features = random.randn(feature_size)\n",
    "    elif isinstance(random, np.ndarray):\n",
    "        features = random\n",
    "    else:\n",
    "        random = np.random.RandomState(random)\n",
    "        features = random.rand(feature_size)\n",
    "    \n",
    "    features = np.clip(features, feature_min, feature_max)\n",
    "    features = np.where(np.abs(features) < feature_min_abs,\n",
    "                        np.sign(features) * feature_min_abs,\n",
    "                        features)\n",
    "    params = {k:v for k, v in zip(env.params, features)}\n",
    "    env.set_parameters(**params)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced MAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = env_fn()\n",
    "timesteps = 30000            # max timesteps in one episode\n",
    "\n",
    "ppo_params = dict(\n",
    "    state_dim = env.observation_space.shape[0],\n",
    "    action_dim = 2,\n",
    "    policy=ActorCriticDiscrete,\n",
    "    epochs = 3,                  # update policy for K epochs\n",
    "    lr = 0.002,                   # learning rate\n",
    "    n_latent_var = 32,           # number of variables in hidden layer\n",
    "    betas = (0.9, 0.999),\n",
    "    gamma = 0.99,                # discount factor\n",
    "    eps_clip = 0.2,              # clip parameter for PPO\n",
    "    update_interval = 500,      # update policy every n timesteps\n",
    "    seed = SEED\n",
    ")\n",
    "library_size = 4\n",
    "random_library = False\n",
    "random_fault = False\n",
    "data_model = False\n",
    "random = np.random.RandomState(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nominal agent trained without fault\n",
    "agent = PPO(env, **ppo_params)\n",
    "r = agent.learn(timesteps, track_higher_gradients=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(len(r)), r)\n",
    "plt.title('Rewards on system under nominal conditions');\n",
    "plt.ylim(bottom=0, top=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate library of policies\n",
    "library = []\n",
    "library_rewards = []\n",
    "library_envs = []\n",
    "library_grads = []\n",
    "\n",
    "if random_library:\n",
    "    library_envs = [random_degrade(env_fn(), random) for _ in range(library_size)]\n",
    "else:\n",
    "    env_params = [\n",
    "        dict(masscart=1., masspole=0.1, length=0.5, force_mag=10.),\n",
    "        dict(masscart=2., masspole=0.2, length=0.5, force_mag=15.),\n",
    "        dict(masscart=1., masspole=0.1, length=0.5, force_mag=-10.),\n",
    "        dict(masscart=2., masspole=0.2, length=0.5, force_mag=-15.)\n",
    "    ]\n",
    "    for env_param in env_params:\n",
    "        env_ = env_fn()\n",
    "        env_.set_parameters(**env_param)\n",
    "        library_envs.append(env_)\n",
    "\n",
    "# factors = [random_degrade(**nominal_config) for _ in range(3)]\n",
    "\n",
    "for env_ in tqdm(library_envs, leave=False):\n",
    "    # introduce some fault and learn data-driven model\n",
    "    # degrade(env_, factor, **nominal_config)\n",
    "    if data_model:\n",
    "        est_ = copy_mlp_regressor(est)  # copy estimator hyperparameters etc.\n",
    "        x, y = generate_training_data(env_, episodes=50)  # random actions!\n",
    "        est_.fit(x, y)\n",
    "        # Train agent on data-driven model\n",
    "        env_ = DataEnv(env_, est_)\n",
    "    agent_ = PPO(env_, **ppo_params)\n",
    "    agent_.policy.load_state_dict(copy_tensor(agent.policy.state_dict()))\n",
    "    rewards = agent_.learn(30000, track_higher_gradients=True)\n",
    "    library.append(copy_tensor(agent_.policy.state_dict()))\n",
    "    library_rewards.append(rewards)\n",
    "    library_grads.append(get_gradients(agent_.meta_policy.parameters(), agent_.meta_policy.parameters(time=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot library rewards\n",
    "plt.figure(figsize=(6, 4))\n",
    "for i, rewards in enumerate(library_rewards):\n",
    "    plt.plot(rewards, label='Policy#{}'.format(i))\n",
    "plt.title('Episodic rewards on process with faults')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Reward')\n",
    "plt.ylim(bottom=0, top=1500)\n",
    "plt.legend()\n",
    "plt.grid(True, 'both')\n",
    "plt.tight_layout()\n",
    "\n",
    "pth = f'./bin/{env_name}/'\n",
    "os.makedirs(pth, exist_ok=True)\n",
    "plt.savefig(pth+env_name+'_library_rewards.png')\n",
    "with open(pth + env_name + '_library_rewards.pickle', 'wb') as f:\n",
    "    pickle.dump(dict(library_rewards=library_rewards), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faulty environment\n",
    "env_ = env_fn()\n",
    "if not random_fault:\n",
    "    env_.set_parameters(\n",
    "        masscart=1.5,\n",
    "        masspole=0.125,\n",
    "        length=0.75,\n",
    "        force_mag=-12,\n",
    "    )\n",
    "else:\n",
    "    random_degrade(env_, random, **nominal_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Experiment functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def adapt(agent, est, memory, library, interactive_env=True,\n",
    "          n_inner=1, n_outer=1, alpha_inner=0.01, alpha_outer=0.1,\n",
    "          mode='fomaml', library_grads=None, rank=-1, **ppo_params):\n",
    "    env_ = agent.env\n",
    "    params = meta_update(agent.policy.state_dict(), env_, library, memory,\n",
    "                         n_inner, n_outer, alpha_inner, alpha_outer,\n",
    "                         interactive_env, mode, library_grads, rank, **ppo_params)\n",
    "    agent.policy.load_state_dict(params)\n",
    "    return agent\n",
    "\n",
    "def adapt_benchmark(agent, est, memory, library, interactive_env=True,\n",
    "          n_inner=1, n_outer=1, alpha_inner=0.01, alpha_outer=0.1,\n",
    "           mode=None, library_grads=None, rank=None, **ppo_params):\n",
    "    env_ = agent.env\n",
    "    agent.learn(ppo_params['update_interval'], ppo_params['update_interval'])\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def trial(env_, est, starting_policy, library=[], interactive_env=False, post_steps=10000,\n",
    "          n_inner=1, n_outer=1, alpha_inner=0.01, alpha_outer=0.1,\n",
    "          mode='fomaml', library_grads=None, rank=-1, benchmark=True, seed=SEED):\n",
    "    # local copy of ppo_params with seed overwritten with the trial seed\n",
    "    ppo_params['seed'] = seed\n",
    "    env_.seed(seed)\n",
    "    # Make copies of env, and agent trained on nominal system,\n",
    "    # and starting library of policies (if any)\n",
    "    library_ = [copy_tensor(p) for p in library]\n",
    "    agent_ = PPO(env_, **ppo_params)\n",
    "    agent_.policy.load_state_dict(copy_tensor(starting_policy))\n",
    "    # Fault occurs, buffer experience with environment\n",
    "    memory_ = Memory()\n",
    "    agent_.experience(memory_, 4*ppo_params['update_interval'], env_, agent_.policy)\n",
    "    # Use meta-learning to adapt to fault\n",
    "    agent_.env.reset()\n",
    "    adapt(agent_, est, memory_, library_, interactive_env,\n",
    "          n_inner, n_outer, alpha_inner, alpha_outer,\n",
    "          mode, library_grads, rank, **ppo_params)\n",
    "    if benchmark:\n",
    "        library_maml, gradients_maml = \\\n",
    "            maml_initialize(starting_policy, env_fn, library_size,\n",
    "                            n_inner, alpha_inner, **ppo_params)\n",
    "        agent_benchmark_maml = PPO(env_, **ppo_params)\n",
    "        agent_benchmark_maml.env.seed(seed)\n",
    "        agent_benchmark_maml.env.reset()\n",
    "        agent_benchmark_maml.policy.load_state_dict(copy_tensor(starting_policy))\n",
    "        adapt(agent_benchmark_maml, est, memory_, library_maml, True,\n",
    "              0, n_outer, alpha_inner, alpha_outer,\n",
    "              mode, gradients_maml, rank=-1, **ppo_params)\n",
    "        \n",
    "        agent_benchmark_vanilla = PPO(env_, **ppo_params)\n",
    "        agent_benchmark_vanilla.env.seed(seed)\n",
    "        agent_benchmark_vanilla.env.reset()\n",
    "        agent_benchmark_vanilla.policy.load_state_dict(copy_tensor(starting_policy))\n",
    "        adapt_benchmark(agent_benchmark_vanilla, est, memory_, library_, data_model,\n",
    "                        n_inner, n_outer, alpha_inner, alpha_outer,\n",
    "                        **ppo_params)\n",
    "        \n",
    "    # Continue learning\n",
    "    rewards = []\n",
    "    agents = [agent_, agent_benchmark_maml, agent_benchmark_vanilla] if benchmark else [agent_]\n",
    "    for a in tqdm(agents, desc='Post-fault training', leave=False):\n",
    "        rewards.append(a.learn(post_steps))\n",
    "    return rewards, agent_.policy.state_dict(), memory_\n",
    "\n",
    "\n",
    "\n",
    "def ntrials(n=NCPU, verbose=10, *trial_args, **trial_kwargs):\n",
    "\n",
    "    res = Parallel(n_jobs=min(n, NCPU), verbose=verbose)(\n",
    "        delayed(trial)(*trial_args, seed=SEED+i, **trial_kwargs) for i in range(n)\n",
    "    )\n",
    "    # res = [\n",
    "    #   [[[r..],[r..],[r..]], state_dict, memory]\n",
    "    # ]\n",
    "    n_rewards = len(res[0][0]) # our approach, maml, vanilla ppo\n",
    "    means, stds = [], []\n",
    "    for reward_idx in range(n_rewards):\n",
    "        maxlen = max([len(r[0][reward_idx]) for r in res])\n",
    "        rewards = np.empty((len(res), maxlen))\n",
    "        rewards.fill(np.nan)\n",
    "        for i, result in enumerate(res):\n",
    "            rewards[i, :len(result[0][reward_idx])] = result[0][reward_idx]\n",
    "        means.append(np.nanmean(rewards, axis=0))\n",
    "        stds.append(np.nanstd(rewards, axis=0))\n",
    "    return means, stds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Grid-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "grid = ParameterGrid(dict(\n",
    "    alpha_inner = [1e-3, 1e-2, 1e-1],\n",
    "    alpha_outer = [1e-3, 1e-2, 1e-1],\n",
    "    n_inner = [0, 3],\n",
    "    n_outer = [1, 3],\n",
    "    data_model = [False],\n",
    "    post_steps = [30000],\n",
    "    library = [library],\n",
    "    library_grads = [library_grads],\n",
    "    rank = [-1, 1, 2],\n",
    "    mode = ['maml', 'fomaml', 'reptile']\n",
    "))\n",
    "\n",
    "pth = f'./bin/{env_name}/hyperparameters/'\n",
    "os.makedirs(pth, exist_ok=True)\n",
    "\n",
    "env_ = env_fn()\n",
    "env_.set_parameters(\n",
    "    masscart=1.5,\n",
    "    masspole=0.1,\n",
    "    length=0.5,\n",
    "    force_mag=-10,\n",
    ")\n",
    "\n",
    "hyp_r, hyp_std, hyp_rb, hyp_stdb = [], [], [], []\n",
    "ngrid = 0\n",
    "for trial_params in tqdm(grid, desc='Hyperparameters', leave=False):\n",
    "    \n",
    "    (r, r_b), (std, std_b) = ntrials(3, 10, env_, None, agent.policy.state_dict(), **trial_params)\n",
    "    \n",
    "    hyp_r.append(r)\n",
    "    hyp_rb.append(r_b)\n",
    "    hyp_std.append(std)\n",
    "    hyp_stdb.append(std_b)\n",
    "    ngrid += 1\n",
    "    \n",
    "    fname = pth + env_name + '_' + \\\n",
    "            sanitize_filename(str({k:v for k,v in trial_params.items() if not k.startswith('library')}))\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plot_adaption((r, r_b), (std, std_b), ('e-MAML', 'Vanilla RL'));\n",
    "    plt.savefig(fname + '.png')\n",
    "    with open(fname+'.pickle', 'wb') as f:\n",
    "        pickle.dump(dict(trial_params=trial_params, results=(r, r_b, std, std_b)), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "for trial_params, r, r_b, std, std_b in tqdm(zip(grid, hyp_r, hyp_std, hyp_rb, hyp_stdb), leave=False, total=len(grid)):\n",
    "    \n",
    "    fname = pth + env_name + '_' + \\\n",
    "            sanitize_filename(str({k:v for k,v in trial_params.items() if not k.startswith('library')}))\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plot_adaption((r, r_b), (std, std_b), ('e-MAML', 'Vanilla RL'));\n",
    "    plt.savefig(fname + '.png')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    i+=1\n",
    "    if i==2: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ncol = 2\n",
    "nrow = ngrid // ncol + (ngrid % ncol != 0)\n",
    "plt.figure(figsize=(12, 3 * nrow))\n",
    "for i, (grid_params, (r, r_b, std, std_b)) in enumerate(zip(grid, zip(hyp_r, hyp_rb, hyp_std, hyp_stdb))):\n",
    "    plt.subplot(nrow, ncol, i + 1)\n",
    "    plot_adaption((r, r_b), (std, std_b), ('e-MAML', 'Vanilla RL'));\n",
    "    print([(k, len(v) if k=='library' else v) for k, v in grid_params.items() if k not in ['post_steps']])\n",
    "    plt.title(i)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance-weighed sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Most favorable policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = PPO(env_, **ppo_params)\n",
    "a.policy.load_state_dict(agent.policy.state_dict())\n",
    "m = Memory()\n",
    "a.experience(m, ppo_params['update_interval'], env_, a.policy)\n",
    "\n",
    "ret = torch.tensor(returns(m.rewards, m.is_terminals, a.gamma)).float().to(DEVICE)\n",
    "ret = (ret - ret.mean()) / (ret.std() + 1e-5)\n",
    "states = torch.tensor(m.states).float().to(DEVICE).detach()\n",
    "actions = torch.tensor(m.actions).float().to(DEVICE).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "args, vals = rank_policies(m, library, **ppo_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "np.asarray(vals)[args]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pth = f'./bin/{env_name}/rnd_vs_lib/'\n",
    "os.makedirs(pth, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trial_params = dict(\n",
    "    alpha_inner = 1e-3,\n",
    "    alpha_outer = 2e-3,\n",
    "    n_inner = 0,\n",
    "    n_outer = 5,\n",
    "    interactive_env = False,\n",
    "    post_steps = 30000,\n",
    "    library = library,\n",
    "    library_grads = library_grads,\n",
    "    mode = 'maml',\n",
    "    rank = -1,\n",
    "    benchmark=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env_.reset()\n",
    "(r, r_m, r_v), (std, std_m, std_v) = ntrials(1, 20, env_, None, agent.policy.state_dict(), **trial_params)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plot_adaption((r, r_m, r_v), (std, std_m, std_v), ('e-MAML', 'MAML', 'Vanilla RL'))\n",
    "\n",
    "fname = pth + env_name + '_' + sanitize_filename(str({k:v for k,v in trial_params.items() if not k.startswith('library')}))\n",
    "\n",
    "# plt.savefig(fname + '.png')\n",
    "# with open(fname+'.pickle', 'wb') as f:\n",
    "#     pickle.dump(dict(trial_params=trial_params, results=((r, r_m, r_v), (std, std_m, std_v))), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trial_params['rank'] = 2\n",
    "trial_params['benchmark'] = False\n",
    "\n",
    "env_.reset()\n",
    "(r2,), (std2,) = ntrials(1, 20, env_, None, agent.policy.state_dict(), **trial_params)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plot_adaption((r, r2, r_v), (std, std2, std_v), ('e-MAML (4)', 'e-MAML(2)', 'Vanilla RL'));\n",
    "\n",
    "fname = pth + env_name + '_' + sanitize_filename(str({k:v for k,v in trial_params.items() if  not k.startswith('library')}))\n",
    "\n",
    "# plt.savefig(fname + '.png')\n",
    "# with open(fname+'.pickle', 'wb') as f:\n",
    "#     pickle.dump(dict(trial_params=trial_params, results=(r, r_b, std, std_b)), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plot_adaption((r, r2, r_m, r_v), (std, std2, std_m, std_v), ('e-MAML (rank=4)', 'e-MAML (rank=2)', 'MAML', 'Vanilla RL'))\n",
    "plt.title(f'{env_name}-Performance-weighed sampling comparison')\n",
    "plt.tight_layout()\n",
    "fname = pth + env_name + '_' + 'rank_comparison'\n",
    "plt.savefig(fname + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy complement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning complement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = []\n",
    "comp_rewards = []\n",
    "comp_envs = []\n",
    "\n",
    "e_params = [\n",
    "    dict(masscart=1., masspole=0.1, length=0.5, force_mag=10.),\n",
    "    dict(masscart=1.5, masspole=0.1, length=0.5, force_mag=10.),\n",
    "    dict(masscart=2., masspole=0.2, length=0.5, force_mag=15.),\n",
    "    dict(masscart=2., masspole=0.15, length=0.5, force_mag=15.),\n",
    "    dict(masscart=1., masspole=0.1, length=0.5, force_mag=-10.),\n",
    "    dict(masscart=1., masspole=0.1, length=0.5, force_mag=-12.),\n",
    "    dict(masscart=2., masspole=0.2, length=0.5, force_mag=-15.),\n",
    "]\n",
    "for e_param in e_params:\n",
    "    e = env_fn()\n",
    "    e.set_parameters(**e_param)\n",
    "    comp_envs.append(e)\n",
    "\n",
    "# factors = [random_degrade(**nominal_config) for _ in range(3)]\n",
    "\n",
    "for e in tqdm(comp_envs, leave=False):\n",
    "    agent_ = PPO(e, **ppo_params)\n",
    "    agent_.policy.load_state_dict(copy_tensor(agent.policy.state_dict()))\n",
    "    rewards = agent_.learn(50000, track_higher_gradients=False)\n",
    "    comp.append(copy_tensor(agent_.policy.state_dict()))\n",
    "    comp_rewards.append(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = PPO(env, **ppo_params)\n",
    "a.policy.load_state_dict(agent.policy.state_dict())\n",
    "m = Memory()\n",
    "a.experience(m, ppo_params['update_interval'], env, a.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_, divmat, idx = prune_library(comp, library_size, m, **ppo_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divmat.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pth = f'./bin/{env_name}/complement/'\n",
    "os.makedirs(pth, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_params = dict(\n",
    "    alpha_inner = 1e-3,\n",
    "    alpha_outer = 2e-3,\n",
    "    n_inner = 0,\n",
    "    n_outer = 5,\n",
    "    interactive_env = False,\n",
    "    post_steps = 30000,\n",
    "    library = comp_,\n",
    "    library_grads = None,\n",
    "    mode = 'fomaml',\n",
    "    rank = 2,\n",
    "    benchmark=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_.reset()\n",
    "(r, r_m, r_v), (std, std_m, std_v) = ntrials(1, 20, env_, None, agent.policy.state_dict(), **trial_params)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plot_adaption((r, r_m, r_v), (std, std_m, std_v), ('e-MAML(most divergent)', 'MAML', 'Vanilla RL'))\n",
    "\n",
    "fname = pth + env_name + '_' + sanitize_filename(str({k:v for k,v in trial_params.items() if not k.startswith('library')}))\n",
    "\n",
    "# plt.savefig(fname + '.png')\n",
    "# with open(fname+'.pickle', 'wb') as f:\n",
    "#     pickle.dump(dict(trial_params=trial_params, results=((r, r_m, r_v), (std, std_m, std_v))), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_params['library'] = [comp[i] for i in range(len(comp)) if i not in idx[0][1]]\n",
    "trial_params['benchmark'] = False\n",
    "\n",
    "env_.reset()\n",
    "(r2,), (std2,) = ntrials(1, 20, env_, None, agent.policy.state_dict(), **trial_params)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plot_adaption((r, r2, r_v), (std, std2, std_v), ('e-MAML (most divergent)', 'e-MAML(least divergent)', 'Vanilla RL'));\n",
    "\n",
    "fname = pth + env_name + '_' + sanitize_filename(str({k:v for k,v in trial_params.items() if  not k.startswith('library')}))\n",
    "\n",
    "# plt.savefig(fname + '.png')\n",
    "# with open(fname+'.pickle', 'wb') as f:\n",
    "#     pickle.dump(dict(trial_params=trial_params, results=(r, r_b, std, std_b)), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plot_adaption((r, r2, r_m, r_v), (std, std2, std_m, std_v), ('e-MAML (4 most divergent)', 'e-MAML (3 least divergent)', 'MAML', 'Vanilla RL'))\n",
    "plt.title('Complement divergence comparison')\n",
    "plt.tight_layout()\n",
    "fname = pth + env_name + '_' + 'divergence_comparison'\n",
    "plt.savefig(fname + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting env actions\n",
    "plt.figure(figsize=(15,6))\n",
    "rows, cols = 2, (len(comp) + 1) // 2 + (1 if (len(comp) + 1) % 2 else 0)\n",
    "for i, (e, p) in tqdm(enumerate(zip(comp_envs, comp)), total=len(comp), leave=False):\n",
    "    e.seed(SEED)\n",
    "    a = PPO(e, **ppo_params)\n",
    "    a.policy.load_state_dict(p)\n",
    "    plt.subplot(rows, cols, i+1)\n",
    "    handles, labels = plot_cartpole(e, a, maxlen=500, legend=False)\n",
    "    axx, axa = plt.gcf().axes[2*i], plt.gcf().axes[2*i+1]\n",
    "    plt.title(str(i+1))\n",
    "    if i != len(comp)-1 and (i+1) % cols != 0:\n",
    "        axa.set_yticklabels([])\n",
    "        axa.set_ylabel('')\n",
    "    if i !=0 and i % cols !=0:\n",
    "        axx.set_yticklabels([])\n",
    "        axx.set_ylabel('')\n",
    "plt.subplot(rows, cols, len(comp)+1)\n",
    "plt.axis('off')\n",
    "plt.legend(handles, labels)\n",
    "plt.tight_layout(pad=0.2)\n",
    "\n",
    "fname = pth + env_name + '_divergence_behaviors'\n",
    "plt.savefig(fname + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## $\\Delta \\theta$ Approximations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pth = f'./bin/{env_name}/deltatheta/'\n",
    "os.makedirs(pth, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trial_params = dict(\n",
    "    alpha_inner = 1e-3,\n",
    "    alpha_outer = 2e-3,\n",
    "    n_inner = 0,\n",
    "    n_outer = 5,\n",
    "    interactive_env = False,\n",
    "    post_steps = 30000,\n",
    "    library = library,\n",
    "    library_grads = library_grads,\n",
    "    mode = 'maml',\n",
    "    rank = -1,\n",
    "    benchmark=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env_.reset()\n",
    "(r, r_m, r_v), (std, std_m, std_v) = ntrials(1, 20, env_, None, agent.policy.state_dict(), **trial_params)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plot_adaption((r, r_v), (std, std_v), ('MAML', 'Vanilla RL'))\n",
    "\n",
    "fname = pth + env_name + '_' + sanitize_filename(str({k:v for k,v in trial_params.items() if not k.startswith('library')}))\n",
    "\n",
    "# plt.savefig(fname + '.png')\n",
    "# with open(fname+'.pickle', 'wb') as f:\n",
    "#     pickle.dump(dict(trial_params=trial_params, results=((r, r_m, r_v), (std, std_m, std_v))), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trial_params['mode'] = 'fomaml'\n",
    "trial_params['benchmark'] = False\n",
    "\n",
    "env_.reset()\n",
    "(r2,), (std2,) = ntrials(1, 20, env_, None, agent.policy.state_dict(), **trial_params)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plot_adaption((r, r2, r_v), (std, std2, std_v), ('MAML', 'FOMAML', 'Vanilla RL'));\n",
    "\n",
    "fname = pth + env_name + '_' + sanitize_filename(str({k:v for k,v in trial_params.items() if  not k.startswith('library')}))\n",
    "\n",
    "# plt.savefig(fname + '.png')\n",
    "# with open(fname+'.pickle', 'wb') as f:\n",
    "#     pickle.dump(dict(trial_params=trial_params, results=(r, r_b, std, std_b)), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "trial_params['mode'] = 'reptile'\n",
    "trial_params['benchmark'] = False\n",
    "\n",
    "env_.reset()\n",
    "(r3,), (std3,) = ntrials(1, 20, env_, None, agent.policy.state_dict(), **trial_params)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plot_adaption((r, r2, r_v), (std, std2, std_v), ('e-MAML (most divergent)', 'e-MAML(least divergent)', 'Vanilla RL'));\n",
    "\n",
    "fname = pth + env_name + '_' + sanitize_filename(str({k:v for k,v in trial_params.items() if  not k.startswith('library')}))\n",
    "\n",
    "# plt.savefig(fname + '.png')\n",
    "# with open(fname+'.pickle', 'wb') as f:\n",
    "#     pickle.dump(dict(trial_params=trial_params, results=(r, r_b, std, std_b)), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plot_adaption((r, r2, r3), (std, std2, std3), ('MAML', 'FOMAML', 'REPTILE'))\n",
    "plt.title('Update step approximation comparison')\n",
    "plt.tight_layout()\n",
    "fname = pth + env_name + '_' + 'deltatheta_comparison_' + ('ranked' if trial_params['rank'] > 0 else 'unranked')\n",
    "plt.savefig(fname + '.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
