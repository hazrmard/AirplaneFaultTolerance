{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import notebook_setup\n",
    "\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "import warnings\n",
    "import os\n",
    "# warnings.filterwarnings(\"error\", category=UserWarning)\n",
    "os.makedirs(os.path.expanduser('~/Data/tensorboard/'), exist_ok=True)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gym\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV, ParameterGrid\n",
    "from sklearn.metrics import make_scorer, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm, trange\n",
    "from pytorchbridge import TorchEstimator\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from utils import (cache_function, cache_to_episodic_rewards,\n",
    "                   cache_to_episodes, copy_tensor, copy_mlp_regressor,\n",
    "                   sanitize_filename)\n",
    "from ppo import PPO, Memory, ActorCriticDiscrete\n",
    "from meta import learn_env_model, meta_update, kl_div, prune_library, plot_adaption\n",
    "from systems import CartPoleEnv, CartPoleDataEnv, plot_cartpole\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED = 0\n",
    "NCPU = os.cpu_count() // 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'CartPole-v1'\n",
    "env = CartPoleEnv()\n",
    "PhysicalEnv = CartPoleEnv\n",
    "DataEnv = CartPoleDataEnv\n",
    "nominal_config = dict(\n",
    "    masscart=1.0,\n",
    "    masspole=0.1,\n",
    "    length=1.0,\n",
    "    force_mag=10.0,\n",
    "    tau=0.2\n",
    ")\n",
    "env_fn = lambda: PhysicalEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### System model\n",
    "\n",
    "* TODO: Check if training data tuples are actually causal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Generate training data\n",
    "def generate_training_data(env, episodes=50):\n",
    "    Xtrain, Ytrain = [], []\n",
    "    x = env.reset()\n",
    "    e = 0\n",
    "    while e < episodes:\n",
    "        u = env.action_space.sample()\n",
    "        Xtrain.append(np.concatenate((x, (u,))))\n",
    "        x, _, done, _ = env.step(u)\n",
    "        Ytrain.append(x)\n",
    "        if done:\n",
    "            e += 1\n",
    "            x = env.reset()\n",
    "    return np.asarray(Xtrain, dtype=np.float32), np.asarray(Ytrain, dtype=np.float32)\n",
    "\n",
    "Xtrain, Ytrain = generate_training_data(env, episodes=50)\n",
    "print(Xtrain.shape, Ytrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid = GridSearchCV(MLPRegressor(), scoring=make_scorer(r2_score, multioutput='uniform_average'),\n",
    "                   param_grid={\n",
    "                       'hidden_layer_sizes': ((32, 32), (64, 64)),\n",
    "                       'activation': ('relu', 'logistic'),\n",
    "                       'learning_rate_init': (1e-2, 1e-3),\n",
    "                       'warm_start': (True,)\n",
    "                   },\n",
    "                   n_jobs=12, verbose=1)\n",
    "grid.fit(Xtrain, Ytrain)\n",
    "pd.DataFrame(grid.cv_results_).sort_values(by='rank_test_score', ascending=True, axis=0).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train on episodes-1, validate on 1 episode worth of instances:\n",
    "est = grid.best_estimator_\n",
    "# Plot performance\n",
    "\n",
    "plot_cartpole(CartPoleDataEnv(env, est))\n",
    "plt.suptitle('');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "module = nn.Sequential(\n",
    "    nn.Linear(12, 64),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(64, 6),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "o = optim.Adam(module.parameters(), lr=50e-5)\n",
    "l = nn.MSELoss()\n",
    "e = TorchEstimator(module, o, l, epochs=100, batch_size=32, early_stopping=True, verbose=True, max_tol_iter=10, tol=1e-5)\n",
    "e.fit(Xtrain, Ytrain);\n",
    "est=e # replace scklearn's estimator with pyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degradation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def degrade(env, factor, **nominal):\n",
    "    if isinstance(factor, (int, float)):\n",
    "        factor = np.ones(len(nominal)) * factor\n",
    "    else:\n",
    "        factor = np.asarray(factor)\n",
    "    params = {k: v*(1+f) for (k, v), f in zip(nominal.items(), factor)}\n",
    "    env.set_parameters(**params)\n",
    "\n",
    "\n",
    "def random_degrade(env=None, factors=(-0.75, 0.75), random=np.random, **nominal):\n",
    "    factors = random.rand(len(nominal)) * (factors[1] - factors[0]) + factors[0]\n",
    "    factors[abs(factors) < 0.1] = 0.1 * np.sign(factors[abs(factors) < 0.1])\n",
    "    if env is not None:\n",
    "        degrade(env, factors, **nominal)\n",
    "    return factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complementary MAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = env_fn()\n",
    "timesteps = 30000            # max timesteps in one episode\n",
    "\n",
    "ppo_params = dict(\n",
    "    state_dim = env.observation_space.shape[0],\n",
    "    action_dim = 2,\n",
    "    policy=ActorCriticDiscrete,\n",
    "    epochs = 5,                  # update policy for K epochs\n",
    "    lr = 0.01,                  # learning rate\n",
    "    n_latent_var = 32,           # number of variables in hidden layer\n",
    "    betas = (0.9, 0.999),\n",
    "    gamma = 0.99,                # discount factor\n",
    "    eps_clip = 0.2,              # clip parameter for PPO\n",
    "    update_interval = 1000      # update policy every n timesteps\n",
    ")\n",
    "library_size = 3\n",
    "data_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nominal agent trained without fault\n",
    "agent = PPO(env, **ppo_params)\n",
    "r = agent.learn(timesteps)\n",
    "library = [copy_tensor(agent.policy.state_dict())]   # initialize library with policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(np.arange(len(r)), r)\n",
    "plt.title('Rewards on system under nominal conditions');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate library of policies\n",
    "library_rewards = []\n",
    "del library[1:]\n",
    "\n",
    "factors = [random_degrade(**nominal_config) for _ in range(3)]\n",
    "\n",
    "for factor in tqdm(factors, total=len(factors), leave=False):\n",
    "    env_ = env_fn()\n",
    "    # introduce some fault and learn data-driven model\n",
    "    degrade(env_, factor, **nominal_config)\n",
    "    if data_model:\n",
    "        est_ = copy_mlp_regressor(est)  # copy estimator hyperparameters etc.\n",
    "        x, y = generate_training_data(env_, episodes=50)  # random actions!\n",
    "        est_.fit(x, y)\n",
    "        # Train agent on data-driven model\n",
    "        env_ = DataEnv(env_, est_)\n",
    "    agent_ = PPO(env_, **ppo_params)\n",
    "    rewards = agent_.learn(30000)\n",
    "    library.append(copy_tensor(agent_.policy.state_dict()))\n",
    "    library_rewards.append(rewards)\n",
    "\n",
    "# PLot library rewards\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i, rewards in enumerate(library_rewards):\n",
    "    plt.plot(rewards, label='Policy#{}'.format(i))\n",
    "plt.title('Episodic rewards on model with faults')\n",
    "plt.legend()\n",
    "plt.grid(True, 'both')\n",
    "\n",
    "pth = './bin/library/'\n",
    "os.makedirs(pth, exist_ok=True)\n",
    "plt.savefig(pth+env_name+'_library_rewards.png')\n",
    "with open(pth + 'lib_rwards.pickle', 'wb') as f:\n",
    "    pickle.dump(dict(library_rewards=library_rewards, factors=factors), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt(agent, est, memory, library, data_model=True,\n",
    "          n_inner=1, n_outer=1, alpha_inner=0.01, alpha_outer=0.1,\n",
    "          **ppo_params):\n",
    "    if data_model:\n",
    "        est_ = learn_env_model(memory, est)\n",
    "        env_ = DataEnv(agent.env, est_)\n",
    "    else:\n",
    "        env_ = agent.env\n",
    "    params = meta_update(agent.policy.state_dict(), env_, library, memory,\n",
    "                         n_inner, n_outer, alpha_inner, alpha_outer,\n",
    "                         data_model, **ppo_params)\n",
    "    agent.policy.load_state_dict(params)\n",
    "    return agent\n",
    "\n",
    "def adapt_benchmark(agent, est, memory, library, data_model=True,\n",
    "          n_inner=1, n_outer=1, alpha_inner=0.01, alpha_outer=0.1,\n",
    "          **ppo_params):\n",
    "    if data_model:\n",
    "        est_ = learn_env_model(memory, est)\n",
    "        env_ = DataEnv(agent.env, est_)\n",
    "    else:\n",
    "        env_ = agent.env\n",
    "    agent.learn(ppo_params['update_interval'], ppo_params['update_interval'])\n",
    "    env_backup = agent.env\n",
    "    agent.env = env_\n",
    "    # If data_model, then interact for the same number of times with the data env\n",
    "    # as the meta-update step does in the outer x inner loops. Otherwise, reuse\n",
    "    # the buffered memory for the same number of times as the meta update step.\n",
    "    if data_model:\n",
    "        agent.learn(timesteps=ppo_params['update_interval'] * (len(library) * n_inner * n_outer),\n",
    "                    update_interval=ppo_params['update_interval'])\n",
    "    else:\n",
    "        pass\n",
    "        # agent.update(policy=agent.policy, memory=memory,\n",
    "        #              epochs=len(library) * n_inner * n_outer, optimizer=agent.optimizer)\n",
    "    agent.env = env_backup\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trial(env_, est, starting_policy, library=[], data_model=True, post_steps=10000,\n",
    "          n_inner=1, n_outer=1, alpha_inner=0.01, alpha_outer=0.1,\n",
    "          benchmark=True):\n",
    "    # Make copies of env, and agent trained on nominal system,\n",
    "    # and starting library of policies (if any)\n",
    "    agent_ = PPO(env_, **ppo_params)\n",
    "    agent_.policy.load_state_dict(copy_tensor(starting_policy))\n",
    "    if benchmark:\n",
    "        agent_benchmark = PPO(env_, **ppo_params)\n",
    "        agent_benchmark.policy.load_state_dict(copy_tensor(starting_policy))\n",
    "    library_ = [copy_tensor(p) for p in library]\n",
    "    # Fault occurs, buffer experience with environment\n",
    "    memory_ = Memory()\n",
    "    agent_.experience(memory_, ppo_params['update_interval'], env_, agent_.policy)\n",
    "    # Use meta-learning to adapt to fault\n",
    "    adapt(agent_, est, memory_, library_, data_model,\n",
    "          n_inner, n_outer, alpha_inner, alpha_outer, **ppo_params)\n",
    "    if benchmark:\n",
    "        adapt_benchmark(agent_benchmark, est, memory_, library_, data_model,\n",
    "                        n_inner, n_outer, alpha_inner, alpha_outer, **ppo_params)\n",
    "    # Continue learning\n",
    "    rewards = []\n",
    "    agents = [agent_, agent_benchmark] if benchmark else [agent_]\n",
    "    for a in tqdm(agents, desc='Post-fault training', leave=False):\n",
    "        rewards.append(a.learn(post_steps))\n",
    "    return rewards, agent_.policy.state_dict(), memory_\n",
    "\n",
    "# TODO: Why do trials return different number of episodes, why is mean and std of different length\n",
    "def ntrials(n=NCPU, verbose=10, *trial_args, **trial_kwargs):\n",
    "    res = Parallel(n_jobs=min(n, NCPU), verbose=verbose)(\n",
    "        delayed(trial)(*trial_args, **trial_kwargs) for _ in range(n)\n",
    "    )\n",
    "    benchmark = len(res[0][0])==2\n",
    "    minlen = min([len(r[0][0]) for r in res])  # in case number of episodes / trial is different\n",
    "    rewards = np.asarray([r[0][0][:minlen] for r in res]) # list of episodic rewards from each trial\n",
    "    mean = np.mean(rewards, axis=0)\n",
    "    std = np.std(rewards, axis=0)\n",
    "    if benchmark:\n",
    "        minlen = min([len(r[0][1]) for r in res])  # in case number of episodes / trial is different\n",
    "        rewards_benchmark = np.asarray([r[0][1][:minlen] for r in res])\n",
    "        mean_benchmark = np.mean(rewards_benchmark, axis=0)\n",
    "        std_benchmark = np.std(rewards_benchmark, axis=0)\n",
    "    else:\n",
    "        mean_benchmark = None\n",
    "        std_benchmark = None\n",
    "    return (mean, mean_benchmark), (std, std_benchmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Grid-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "grid = ParameterGrid(dict(\n",
    "    alpha_inner = [1e-3, 1e-2],\n",
    "    alpha_outer = [1e-2, 1e-1],\n",
    "    n_inner = [4],\n",
    "    n_outer = [2],\n",
    "    data_model = [False, True],\n",
    "    post_steps = [30000],\n",
    "    library = [[], library]\n",
    "))\n",
    "os.makedirs(('./bin/hyperparameters'), exist_ok=True)\n",
    "\n",
    "tanks_ = TanksFactory(n = n_tanks, e = n_engines, **nominal_config)\n",
    "env_ = TanksPhysicalEnv(tanks_, tstep)\n",
    "factors = random_degrade(tanks_, atmost_engines=1)\n",
    "print('Tank Factors:', factors[0])\n",
    "print('Engine Factors:', factors[1])\n",
    "\n",
    "hyp_r, hyp_std, hyp_rb, hyp_stdb = [], [], [], []\n",
    "ngrid = 0\n",
    "for trial_params in tqdm(grid, desc='Hyperparameters', leave=False):\n",
    "    (r, r_b), (std, std_b) = ntrials(4, 11, env_, est, agent.policy.state_dict(), **trial_params)\n",
    "    hyp_r.append(r)\n",
    "    hyp_rb.append(r_b)\n",
    "    hyp_std.append(std)\n",
    "    hyp_stdb.append(std_b)\n",
    "    ngrid += 1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ncol = 2\n",
    "nrow = ngrid // ncol + (ngrid % ncol != 0)\n",
    "plt.figure(figsize=(12, 3 * nrow))\n",
    "for i, (grid_params, (r, r_b, std, std_b)) in enumerate(zip(grid, zip(hyp_r, hyp_rb, hyp_std, hyp_stdb))):\n",
    "    plt.subplot(nrow, ncol, i + 1)\n",
    "    plot_adaption(r, r_b, std, std_b)\n",
    "    print([(k, len(v) if k=='library' else v) for k, v in grid_params.items() if k not in ['post_steps']])\n",
    "    plt.title(i)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populated vs. empty policy library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_params = dict(\n",
    "    alpha_inner = 1e-3,\n",
    "    alpha_outer = 1e-2,\n",
    "    n_inner = 4,\n",
    "    n_outer = 2,\n",
    "    data_model = True,\n",
    "    post_steps = 30000\n",
    ")\n",
    "pth = './bin/populated_vs_empty_library/'\n",
    "os.makedirs(pth, exist_ok=True)\n",
    "\n",
    "env_ = env_fn()\n",
    "factors = random_degrade(env_, **nominal_config)\n",
    "# degrade(env_, factors, **nominal_config)\n",
    "print('Factors:', factors)\n",
    "\n",
    "(r, r_b), (std, std_b) = ntrials(4, 20, env_, est, agent.policy.state_dict(), library, **trial_params)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plot_adaption(r, r_b, std, std_b);\n",
    "\n",
    "fname = pth + env_name + 'libsize_3_f' + sanitize_filename(str(factors) + str(trial_params))\n",
    "plt.savefig(fname + '.png')\n",
    "with open(fname+'.pickle', 'wb') as f:\n",
    "    pickle.dump(dict(trial_params=trial_params, results=(r, r_b, std, std_b), factors=factors), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty library\n",
    "(r, r_b), (std, std_b) = ntrials(4, 20, env_, est, agent.policy.state_dict(), [], **trial_params)\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "plot_adaption(r, r_b, std, std_b);\n",
    "\n",
    "fname = pth + env_name + 'libsize_3_f' + sanitize_filename(str(factors) + str(trial_params))\n",
    "plt.savefig(fname + '.png')\n",
    "with open(fname+'.pickle', 'wb') as f:\n",
    "    pickle.dump(dict(trial_params=trial_params, results=(r, r_b, std, std_b), factors=factors), f);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Sequential abrupt faults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Multi-fault trial\n",
    "multi_trial_params = dict(\n",
    "    starting_policy = agent.policy.state_dict(),\n",
    "    n_faults = 10,\n",
    "    lib_size = 3,\n",
    "    library=[]\n",
    ")\n",
    "trial_params = dict(\n",
    "    alpha_inner = 1e-3,\n",
    "    alpha_outer = 1e-3,\n",
    "    n_inner = 4,\n",
    "    n_outer = 1,\n",
    "    data_model = False,\n",
    "    post_steps = 30000\n",
    ")\n",
    "\n",
    "def multi_trial(starting_policy, n_faults=5, lib_size=3, library=[], **trial_params):\n",
    "    tanks_ = TanksFactory(n = n_tanks, e = n_engines, **nominal_config)\n",
    "    env_ = TanksPhysicalEnv(tanks_, tstep)\n",
    "    agent_ = PPO(env_, **ppo_params)\n",
    "    agent_.policy.load_state_dict(copy_tensor(starting_policy))\n",
    "    rewards = []\n",
    "    rewards_benchmark = []\n",
    "    fault_times = [0]\n",
    "    for f in trange(n_faults, desc='Faults', leave=False):\n",
    "        factors = random_degrade(tanks_)\n",
    "        rew, params, memory = trial(env_, est, agent_.policy.state_dict(), library, **trial_params)\n",
    "        library.append(params)\n",
    "        library, div = prune_library(library, lib_size, memory, **ppo_params)\n",
    "        agent_.policy.load_state_dict(params)\n",
    "        rewards.extend(rew[0])\n",
    "        rewards_benchmark.extend(rew[1])\n",
    "        fault_times.append(fault_times[-1] + len(rew[0]))\n",
    "    return rewards, rewards_benchmark, fault_times[:-1]\n",
    "\n",
    "def nmulti_trials(n=NCPU, verbose=20, **params):\n",
    "    res = Parallel(n_jobs=min(n, NCPU), verbose=verbose)(\n",
    "        delayed(multi_trial)(**params) for _ in range(n)\n",
    "    )\n",
    "    benchmark = res[0][1] is not None\n",
    "    minlen = min([len(r[0]) for r in res])  # in case number of episodes / trial is different\n",
    "    rewards = np.asarray([r[0][:minlen] for r in res]) # list of episodic rewards from each trial\n",
    "    mean = np.mean(rewards, axis=0)\n",
    "    std = np.std(rewards, axis=0)\n",
    "    # Benchmarks\n",
    "    if benchmark:\n",
    "        minlen = min([len(r[1]) for r in res])  # in case number of episodes / trial is different\n",
    "        rewards_benchmark = np.asarray([r[1][:minlen] for r in res])\n",
    "        mean_benchmark = np.mean(rewards_benchmark, axis=0)\n",
    "        std_benchmark = np.std(rewards_benchmark, axis=0)\n",
    "    else:\n",
    "        mean_benchmark = None\n",
    "        std_benchmark = None\n",
    "    # Faults\n",
    "    minlen = min([len(r[2]) for r in res])  # in case number of episodes / trial is different\n",
    "    fault_times = np.asarray([r[2][:minlen] for r in res]) # list of episodic rewards from each trial\n",
    "    fault_times = np.mean(fault_times, axis=0)\n",
    "    return (mean, mean_benchmark), (std, std_benchmark), fault_times\n",
    "\n",
    "(r, r_b), (std, std_b), f = nmulti_trials(n=4, verbose=20, **multi_trial_params, **trial_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pth = './bin/multi_fault/'\n",
    "os.makedirs(pth, exist_ok=True)\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plot_adaption(r, r_b, std, std_b, f)\n",
    "\n",
    "fname = pth + sanitize_filename(str({k:v for k, v in multi_trial_params.items() \\\n",
    "                                     if k not in ('library', 'starting_policy')}) + str(trial_params))\n",
    "plt.savefig(fname + '.png')\n",
    "with open(fname+'.pickle', 'wb') as fi:\n",
    "    pickle.dump(dict(trial_params=trial_params, multi_trial_params=multi_trial_params,\n",
    "                     results=(r, r_b, std, std_b, f)), fi);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
