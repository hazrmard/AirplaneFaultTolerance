{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "pyStateSpace_path = os.path.abspath('../pyStateSpace')\n",
    "try:\n",
    "    import pystatespace\n",
    "except ImportError:\n",
    "    if pyStateSpace_path not in sys.path:\n",
    "        sys.path.append(pyStateSpace_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from copy import deepcopy\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # disable tensorflow warning messages\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import r2_score, make_scorer\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines import PPO2\n",
    "from tqdm.auto import tqdm, trange\n",
    "from pystatespace import Trapezoid\n",
    "\n",
    "from tanks import TanksFactory, TanksPhysicalEnv, TanksDataEnv, TanksDataRecurrentEnv\n",
    "from plotting import plot_tanks\n",
    "from utils import cache_function, cache_to_training_set, rewards_from_actions\n",
    "\n",
    "SMALL_SIZE = 15\n",
    "MEDIUM_SIZE = 17\n",
    "BIGGER_SIZE = 19\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "plt.rc('lines', linewidth = 2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tanks = 6\n",
    "n_engines = 2\n",
    "tstep = 1e0\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "nominal_config = {\n",
    "    'heights': np.ones(n_tanks),\n",
    "    'cross_section':np.ones(n_tanks),\n",
    "    'valves_min':np.zeros(n_tanks),\n",
    "    'valves_max':np.ones(n_tanks),\n",
    "    'resistances':np.ones(n_tanks) * 1e2,\n",
    "    'pumps':np.ones(n_tanks) * 0.1,\n",
    "    'engines':np.ones(n_engines) * 0.05\n",
    "}\n",
    "\n",
    "tanks = TanksFactory(n = n_tanks, e = n_engines, **nominal_config)\n",
    "system = Trapezoid(dims=6, outputs=6, dx=tanks.dxdt, out=tanks.y, tstep=tstep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env_physical = DummyVecEnv([lambda: TanksPhysicalEnv(tanks, tstep=tstep)])\n",
    "plot_tanks(env_physical.envs[0], plot='closed')\n",
    "plt.suptitle('Fuel tank levels over time');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random actions which are repeated for a random number\n",
    "# of time steps\n",
    "def rand_u(length: int, dim: int):\n",
    "    i = 0\n",
    "    u = np.zeros((length, dim))\n",
    "    while i < arr_len:\n",
    "        subseq = min(length - i, np.random.randint(1, int(length / 2)))\n",
    "        u_ = np.random.choice(2, (1, dim))\n",
    "        u[i:i+subseq] = u_\n",
    "        i += subseq\n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data\n",
    "episode_duration = sum(tanks.heights * tanks.cross_section) \\\n",
    "                   / min(sum(tanks.pumps), sum(tanks.engines))\n",
    "episode_length = int(episode_duration / system.tstep)\n",
    "episodes = 50\n",
    "\n",
    "# An episode of length n will have n-1\n",
    "# state[i], action[i], state[i+1] tuples\n",
    "arr_len = episode_length - 1\n",
    "Xtrain = np.empty((episodes * arr_len, n_tanks * 2))\n",
    "Ytrain = np.empty((episodes * arr_len, n_tanks))\n",
    "for e in range(episodes):\n",
    "    u = rand_u(episode_length, n_tanks)\n",
    "    t = np.linspace(0, episode_duration, num=episode_length, endpoint=False)\n",
    "    x, _ = system.predict(t, tanks.heights, u)\n",
    "    Xtrain[e * arr_len: (e+1) * arr_len, :n_tanks] = x[:-1]\n",
    "    Xtrain[e * arr_len: (e+1) * arr_len, n_tanks:] = u[:-1]\n",
    "    Ytrain[e * arr_len: (e+1) * arr_len] = x[1:]\n",
    "\n",
    "print(Xtrain.shape, Ytrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(MLPRegressor(), scoring=make_scorer(r2_score, multioutput='uniform_average'),\n",
    "                   param_grid={\n",
    "                       'hidden_layer_sizes': ((64, 64), (128, 128), (256, 256), (512, 512)),\n",
    "                       'activation': ('relu', 'logistic'),\n",
    "                       'learning_rate_init': (1e-2, 5e-3, 1e-3),\n",
    "                       'early_stopping': (True,)\n",
    "                   },\n",
    "                   n_jobs=12, verbose=1)\n",
    "grid.fit(Xtrain, Ytrain)\n",
    "pd.DataFrame(grid.cv_results_).sort_values(by='rank_test_score', ascending=True, axis=0).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train on episodes-1, validate on 1 episode worth of instances:\n",
    "est = grid.best_estimator_\n",
    "est.set_params(random_state=seed)\n",
    "# Plot performance\n",
    "env_data = TanksDataEnv(tanks, est, tstep)\n",
    "plot_tanks(env_data, plot='closed')\n",
    "plt.suptitle('Modeled fuel tank levels over time');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Degradation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def degrade(tanks: TanksFactory, time: float, tfactor: np.ndarray,\n",
    "            efactor: np.ndarray, **nominal):\n",
    "    if not isinstance(tfactor, (list, tuple, np.ndarray)):\n",
    "        # If a single degradation factor given, assume it is\n",
    "        # identical for all tanks.\n",
    "        pfactor = np.ones(n_tanks) * tfactor\n",
    "    if not isinstance(efactor, (list, tuple, np.ndarray)):\n",
    "        # If a single degradation factor given, assume it is\n",
    "        # identical for all engines.\n",
    "        efactor = np.ones(n_engines) * efactor\n",
    "    for i in range(n_tanks):\n",
    "        tanks.pumps[i] = nominal['pumps'][i] * (1 - time / tfactor[i])\n",
    "        tanks.resistances[i] = nominal['resistances'][i] + \\\n",
    "                               nominal['resistances'][i] * time / tfactor[i]\n",
    "    for i in range(n_engines):\n",
    "        tanks.engines[i] = nominal['engines'][i] + \\\n",
    "                           nominal['engines'][i] * time / efactor[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward function performance for normal vs degraded system\n",
    "t = TanksFactory(n = n_tanks, e = n_engines, **nominal_config)\n",
    "e = TanksPhysicalEnv(t, tstep)\n",
    "tfactor = [np.inf, np.inf, np.inf, np.inf, np.inf, np.inf]\n",
    "efactor = [20., np.inf]\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "u, r, d = np.zeros(6), [], False\n",
    "e.reset()\n",
    "while not d:\n",
    "    _,r_,d,_ = e.step(u)\n",
    "    r.append(r_)\n",
    "plt.plot(r, 'c-', label='Nominal, closed')\n",
    "\n",
    "u, r, d = np.ones(6), [], False\n",
    "e.reset()\n",
    "while not d:\n",
    "    _,r_,d,_ = e.step(u)\n",
    "    r.append(r_)\n",
    "plt.plot(r, 'c--', label='Nominal, open')\n",
    "\n",
    "degrade(t, 10, tfactor, efactor, **nominal_config)\n",
    "u, r, d = np.zeros(6), [], False\n",
    "e.reset()\n",
    "while not d:\n",
    "    _,r_,d,_ = e.step(u)\n",
    "    r.append(r_)\n",
    "plt.plot(r, 'r:', label='Degraded, closed')\n",
    "\n",
    "u, r, d = np.ones(6), [], False\n",
    "e.reset()\n",
    "while not d:\n",
    "    _,r_,d,_ = e.step(u)\n",
    "    r.append(r_)\n",
    "plt.plot(r, 'r-.', label='Degraded, open')\n",
    "\n",
    "plt.legend(framealpha=0.5, ncol=2)\n",
    "plt.title('Rewards over single episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.xlabel('Time /s')\n",
    "plt.xlim(left=0, right=len(r))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control Loop\n",
    "\n",
    "1. Learn nominal policy from physical model\n",
    "2. During operation/Online:\n",
    "    1. Degrade system\n",
    "    2. Improve control using on-policy RL on actual system\n",
    "3. Offline (more sample efficiency):\n",
    "    1. Learn data-driven model of system from recorded measurements\n",
    "    2. Improve control using on-policy RL on data-driven model\n",
    "4. Goto 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reinforcement learning\n",
    "update_interval = 128\n",
    "online_interval = 512\n",
    "offline_interval = 2048\n",
    "minibatch = 8  # number of batches per update_interval\n",
    "policy_learning_rate = 1e-2\n",
    "# policy_arch=[128, 128, dict(vf=[32, 32], pi=[32, 32])]\n",
    "policy_arch=[64, 64, dict(vf=[16, 16], pi=[16, 16])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A constructor that generates components for a single trial based\n",
    "# on global variables\n",
    "def get_components():\n",
    "    env = DummyVecEnv([lambda: TanksPhysicalEnv(tanks, tstep=tstep)])\n",
    "    model = deepcopy(grid.best_estimator_)\n",
    "    model.set_params(warm_start=True)\n",
    "    agent = PPO2(MlpPolicy, env, verbose=0, n_steps=update_interval,\n",
    "                 nminibatches=minibatch, learning_rate=policy_learning_rate,\n",
    "                 policy_kwargs=dict(net_arch=policy_arch), seed=seed)\n",
    "    return env, model, agent\n",
    "\n",
    "# A single trial which runs for periods*online_interval steps\n",
    "# given a pair of degradation factors\n",
    "def trial(tfactor, efactor, periods, agent, env, model, offline=True, return_caches=False):\n",
    "    if return_caches:\n",
    "        cache = []\n",
    "    rewards_rl, rewards_open, rewards_closed = \\\n",
    "        np.zeros(periods), np.zeros(periods), np.zeros(periods)\n",
    "    \n",
    "    for period in trange(periods, leave=False):\n",
    "        degrade(tanks, period, tfactor, efactor, **nominal_config)\n",
    "        # Online-learning + control\n",
    "        agent.set_env(env)\n",
    "        x_cache, u_cache, d_cache, r_cache = [], [], [], []\n",
    "        agent.learn(total_timesteps=online_interval,\n",
    "                    callback=cache_function(x_cache, u_cache, d_cache, r_cache))\n",
    "        # Accumulate rewards per period\n",
    "        rewards_rl[period] = np.sum(r_cache) / online_interval\n",
    "        rewards_open[period] = \\\n",
    "            rewards_from_actions(env.envs[0], np.ones((online_interval, n_tanks)))\\\n",
    "           / online_interval\n",
    "        rewards_closed[period] = \\\n",
    "            rewards_from_actions(env.envs[0], np.zeros((online_interval, n_tanks)))\\\n",
    "           / online_interval\n",
    "        if return_caches:\n",
    "            cache.append((x_cache, u_cache, d_cache, r_cache))\n",
    "        # Offline learning\n",
    "        if offline:\n",
    "            # Learn model\n",
    "            xu, x_next = cache_to_training_set(x_cache, u_cache, d_cache)\n",
    "            model.fit(xu, x_next)\n",
    "            # Set data-driven environment\n",
    "            agent.set_env(DummyVecEnv([lambda: TanksDataEnv(tanks, model, tstep)]))\n",
    "            # Learn optimal control\n",
    "            agent.learn(total_timesteps=offline_interval)\n",
    "    \n",
    "    if return_caches:\n",
    "            return caches\n",
    "    return rewards_rl, rewards_open, rewards_closed\n",
    "\n",
    "# Run a trial multiple times where degradation factors are chosen randomly from\n",
    "# a range. The number of degrading tanks is also random (<= atmost)\n",
    "def multiple_trials(n_trials, periods=10, tfactors=(10, 20), efactors=(10, 20),\n",
    "                    atmost_tanks=2, atmost_engines=2, offline=True):\n",
    "    np.random.seed(seed)\n",
    "    random = np.random.RandomState(seed)\n",
    "    r_rl, r_open, r_closed = [], [], []\n",
    "\n",
    "    for _ in trange(n_trials, leave=False):\n",
    "        env, model, agent = get_components()\n",
    "        \n",
    "        tfactor = np.ones(n_tanks) * np.inf\n",
    "        if atmost_tanks > 0:\n",
    "            tanks_affected = random.randint(1, atmost_tanks + 1)\n",
    "            idx_affected = random.choice(n_tanks, size=tanks_affected, replace=False)\n",
    "            tfactor[idx_affected] = random.randint(*tfactors, size=tanks_affected)\n",
    "        \n",
    "        efactor = random.randint(*efactors, size=n_engines)\n",
    "        if atmost_engines > 0:\n",
    "            engines_affected = random.choice(1, atmost_engines + 1)\n",
    "            idx_affected = random.choice(n_engines, size=engines_affected, replace=False)\n",
    "            tfactor[idx_affected] = random.randint(*efactors, size=engines_affected)\n",
    "        \n",
    "        rewards_rl, rewards_open, rewards_closed = trial(tfactor, efactor, periods,\n",
    "                                                         agent, env, model, offline)\n",
    "        r_rl.append(rewards_rl)\n",
    "        r_open.append(rewards_open)\n",
    "        r_closed.append(rewards_closed)\n",
    "\n",
    "    mean_rl = np.mean(r_rl, axis=0)\n",
    "    mean_open = np.mean(r_open, axis=0)\n",
    "    mean_closed = np.mean(r_closed, axis=0)\n",
    "    std_rl = np.std(r_rl, axis=0)\n",
    "    std_open = np.std(r_open, axis=0)\n",
    "    std_closed = np.std(r_closed, axis=0)\n",
    "    return mean_rl, mean_open, mean_closed,\\\n",
    "           std_rl, std_open, std_closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Single trial, one fault\n",
    "# Storing RL, valves open, valves closed rewards for when\n",
    "# offline=True and False\n",
    "offline_res_single = {True: (None, None, None), False: (None, None, None)}\n",
    "\n",
    "# Plot single trial rewards for offline=True\n",
    "periods = 10\n",
    "tfactor = [np.inf, np.inf, np.inf, np.inf, np.inf, np.inf]\n",
    "efactor = [20, np.inf]\n",
    "offline = False\n",
    "env, model, agent = get_components()\n",
    "\n",
    "offline_res_single[offline] = trial(tfactor, efactor, periods,\n",
    "                             agent, env, model, offline)\n",
    "\n",
    "# Plot single trial rewards for offline=False\n",
    "offline = True\n",
    "env, model, agent = get_components()\n",
    "\n",
    "offline_res_single[offline] = trial(tfactor, efactor, periods,\n",
    "                             agent, env, model, offline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot control actions for a single episode\n",
    "period = 10\n",
    "degrade(tanks, period, tfactor, efactor, **nominal_config)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plot_tanks(env.envs[0], agent)\n",
    "plt.suptitle('Fault in E1 at time={}s, offline={}'.format(period * online_interval, offline))\n",
    "# plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Single trial, multiple faults\n",
    "# Storing RL, valves open, valves closed rewards for when\n",
    "# offline=True and False\n",
    "offline_res_multiple = {True: (None, None, None), False: (None, None, None)}\n",
    "\n",
    "# Plot single trial rewards for offline=True\n",
    "periods = 10\n",
    "tfactor = [np.inf, np.inf, np.inf, np.inf, 20, 20]\n",
    "efactor = [20., np.inf]\n",
    "offline = False\n",
    "env, model, agent = get_components()\n",
    "\n",
    "offline_res_multiple[offline] = trial(tfactor, efactor, periods,\n",
    "                             agent, env, model, offline)\n",
    "\n",
    "# Plot single trial rewards for offline=False\n",
    "offline = True\n",
    "env, model, agent = get_components()\n",
    "\n",
    "offline_res_multiple[offline] = trial(tfactor, efactor, periods,\n",
    "                             agent, env, model, offline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.subplot(2,1,1)\n",
    "if offline_res_single[True][1] is not None:\n",
    "    rewards_open, rewards_closed = offline_res_single[True][1], offline_res_single[True][2]\n",
    "else:\n",
    "    rewards_open, rewards_closed = offline_res_single[False][1], offline_res_single[False][2]\n",
    "rewards_rl_offline, rewards_rl_online = offline_res_single[True][0], offline_res_single[False][0]\n",
    "plt.plot(rewards_open, '--', label='Open')\n",
    "plt.plot(rewards_closed, ':', label='Closed')\n",
    "if rewards_rl_offline is not None:\n",
    "    plt.plot(rewards_rl_offline, 'g-')\n",
    "if rewards_rl_online is not None:\n",
    "    plt.plot(rewards_rl_online, 'g-.')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True, which='both')\n",
    "plt.ylabel('Rewards per step')\n",
    "# plt.xlabel('Time /s')\n",
    "plt.tick_params(labelbottom=False)\n",
    "plt.xticks(plt.gca().get_xticks(), plt.gca().get_xticks() * online_interval)\n",
    "plt.xlim(left=0, right=periods-1)\n",
    "plt.title('Fault in E1')\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure(figsize=(10, 8))\n",
    "plt.subplot(2,1,2)\n",
    "if offline_res[True][1] is not None:\n",
    "    rewards_open, rewards_closed = offline_res_multiple[True][1], offline_res_multiple[True][2]\n",
    "else:\n",
    "    rewards_open, rewards_closed = offline_res_multiple[False][1], offline_res_multiple[False][2]\n",
    "rewards_rl_offline, rewards_rl_online = offline_res_multiple[True][0], offline_res_multiple[False][0]\n",
    "plt.plot(rewards_open, '--')\n",
    "plt.plot(rewards_closed, ':')\n",
    "if rewards_rl_offline is not None:\n",
    "    plt.plot(rewards_rl_offline, 'g-', label='RL, Offline=True')\n",
    "if rewards_rl_online is not None:\n",
    "    plt.plot(rewards_rl_online, 'g-.', label='RL, Offline=False')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True, which='both')\n",
    "plt.ylabel('Rewards per step')\n",
    "plt.xlabel('Time /s')\n",
    "plt.xticks(plt.gca().get_xticks(), plt.gca().get_xticks() * online_interval)\n",
    "plt.xlim(left=0, right=periods-1)\n",
    "plt.title('Faults in T5, T6, E1')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot control actions for a single episode\n",
    "period = 10\n",
    "degrade(tanks, period, tfactor, efactor, **nominal_config)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plot_tanks(env.envs[0], agent)\n",
    "plt.suptitle('Faults in T5, T6, E1 at time={}s, offline={}'.format(period * online_interval, offline))\n",
    "# plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offline_res_agg = {True: (None,)*6, False: (None,)*6}\n",
    "\n",
    "# Plot multiple trial rewards for offline=True\n",
    "periods = 10\n",
    "tfactors = (10, 30)\n",
    "efactors = (10, 30)\n",
    "atmost_tanks = 1\n",
    "atmost_engines = 1\n",
    "offline = True\n",
    "\n",
    "offline_res_agg[offline] = \\\n",
    "    multiple_trials(20, periods, tfactors, efactors, atmost_tanks, atmost_engines, offline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot multiple trial rewards for offline=False\n",
    "offline = False\n",
    "\n",
    "offline_res_agg[offline] = \\\n",
    "    multiple_trials(20, periods, tfactors, efactors, atmost_tanks, atmost_engines, offline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "rewards_open, rewards_closed = offline_res_agg[offline][1], offline_res_agg[offline][2]\n",
    "std_open, std_closed = offline_res_agg[offline][4], offline_res_agg[offline][5]\n",
    "rewards_rl_offline, rewards_rl_online = offline_res_agg[True][0], offline_res_agg[False][0]\n",
    "std_rl_offline, std_rl_online = offline_res_agg[True][3], offline_res_agg[False][3]\n",
    "\n",
    "plt.plot(rewards_open, '--', label='Open')\n",
    "plt.plot(rewards_closed, ':', label='Closed')\n",
    "if rewards_rl_offline is not None:\n",
    "    plt.plot(rewards_rl_offline, 'g-', label='RL, Offline=True')\n",
    "    plt.fill_between(np.arange(periods), rewards_rl_offline + std_rl_offline,\n",
    "                     rewards_rl_offline - std_rl_offline,\n",
    "                     alpha=0.3, color='g')\n",
    "if rewards_rl_online is not None:\n",
    "    plt.plot(rewards_rl_online, 'g-.', label='RL, Offline=False')\n",
    "    plt.fill_between(np.arange(periods), rewards_rl_online + std_rl_online,\n",
    "                     rewards_rl_online - std_rl_online,\n",
    "                     alpha=0.3, color='g')\n",
    "plt.gca().set_prop_cycle(None)\n",
    "plt.fill_between(np.arange(periods), rewards_open + std_open, rewards_open - std_open, alpha=0.3)\n",
    "plt.fill_between(np.arange(periods), rewards_closed + std_closed, rewards_closed - std_closed, alpha=0.3)\n",
    "plt.legend(ncol=2)\n",
    "plt.grid(True, which='both')\n",
    "plt.ylabel('Rewards per step')\n",
    "plt.xlabel('Time /s')\n",
    "plt.xticks(plt.gca().get_xticks(), plt.gca().get_xticks() * online_interval)\n",
    "plt.xlim(left=0, right=periods-1)\n",
    "plt.title('Online')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
