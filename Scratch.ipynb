{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.distributions import Bernoulli, MultivariateNormal, Categorical\n",
    "import gym\n",
    "\n",
    "import notebook_setup\n",
    "from tqdm.auto import tqdm, trange\n",
    "from systems import CartPoleEnv\n",
    "from systems import CartPoleContinuousEnv\n",
    "from ppo import ActorCriticDiscrete, ActorCriticMultiBinary, ActorCriticBox, PPO, DEVICE, Memory, returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ppo_params = dict(\n",
    "    state_dim=4,\n",
    "    action_dim=2,\n",
    "    n_latent_var=32,\n",
    "    lr=0.02,\n",
    "    epochs=5,\n",
    "    update_interval=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "agent = PPO(CartPoleEnv(), ActorCriticDiscrete, **ppo_params)\n",
    "rewards = agent.learn(30000)\n",
    "plt.scatter(np.arange(len(rewards)), rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ActorCriticDiscrete(state_dim=4, action_dim=2, n_latent_var=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "\n",
    "ppo_params = dict(\n",
    "    state_dim=env.observation_space.shape[0],\n",
    "    action_dim=env.action_space.shape[0],\n",
    "    n_latent_var=64,\n",
    "    lr=0.0003,\n",
    "    epochs=75,\n",
    "    update_interval=3000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "agent = PPO(env, ActorCriticBox, **ppo_params)\n",
    "rewards = agent.learn(3000)\n",
    "plt.scatter(np.arange(len(rewards)), rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env = CartPoleContinuousEnv()\n",
    "\n",
    "ppo_params = dict(\n",
    "    state_dim=env.observation_space.shape[0],\n",
    "    action_dim=env.action_space.shape[0],\n",
    "    n_latent_var=32,\n",
    "    lr=0.02,\n",
    "    epochs=25,\n",
    "    update_interval=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "agent = PPO(env, ActorCriticBox, **ppo_params)\n",
    "rewards = agent.learn(10000)\n",
    "plt.scatter(np.arange(len(rewards)), rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Discretized Continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class ActorCriticBoxDiscrete(ActorCriticBox):\n",
    "    \n",
    "    def predict(self, state):\n",
    "        action, logprob = super().predict(state)\n",
    "        return int(np.round(np.clip(action.item(), 0, 1))), logprob\n",
    "\n",
    "    def evaluate(self, state, action):\n",
    "        action_logprobs, state_value, dist_entropy = \\\n",
    "            super().evaluate(state, action)\n",
    "        return action_logprobs, state_value, dist_entropy\n",
    "\n",
    "env = CartPoleEnv()\n",
    "\n",
    "ppo_params = dict(\n",
    "    state_dim=4,\n",
    "    action_dim=1,\n",
    "    n_latent_var=64,\n",
    "    lr=0.002,\n",
    "    epochs=50,\n",
    "    update_interval=500\n",
    ")\n",
    "\n",
    "agent = PPO(env, ActorCriticBoxDiscrete, **ppo_params)\n",
    "rewards = agent.learn(10000)\n",
    "plt.scatter(np.arange(len(rewards)), rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quadcopter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from systems.quadcopter import (Quadcopter, QuadcopterSupervisorEnv, Controller, plot_quadcopter,\n",
    "                                QUADPARAMS, CONTROLLER_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTROLLER_PARAMS = {\n",
    "    'Motor_limits': [4000, 9000],\n",
    "    'Tilt_limits': [-10, 10],            # degrees\n",
    "    'Yaw_Control_Limits': [-900, 900],\n",
    "    'Z_XY_offset': 500,\n",
    "    'Linear_To_Angular_Scaler': [1, 1, 0],\n",
    "    'Yaw_Rate_Scaler': 0.18,\n",
    "    'Linear_PID': {\n",
    "        'P':[300, 300, 0],\n",
    "        'I':[0.04, 0.04, 0],\n",
    "        'D':[450, 450, 0]},\n",
    "    'Angular_PID':{\n",
    "        'P':[22000, 22000, 1500],\n",
    "        'I':[0, 0, 1.2],\n",
    "        'D':[12000, 12000, 0]},\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = QuadcopterSupervisorEnv(Controller(Quadcopter(), params=CONTROLLER_PARAMS), deterministic_reset=True)\n",
    "env.reset(position=(0,0,0), target=(0,0,5), linear_rate=(0,0,0), orientation=(0,0,0), angular_rate=(0,0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 5000\n",
    "label_ = 'No fault'\n",
    "pos_ = np.zeros((T, 3))\n",
    "env.reset()\n",
    "env.ctrl.quadcopter.set_motor_faults([0, 0, 0, 0])\n",
    "pos_[0] = env.start\n",
    "R = 0.\n",
    "rewards = []\n",
    "for t in trange(1, T, leave=False):\n",
    "    _, r, done, _ = env.step(0.)\n",
    "    rewards.append(r)\n",
    "    R += r\n",
    "    pos_[t] = env.ctrl.quadcopter.state[:3]\n",
    "    if done:\n",
    "        pos_ = pos_[:t+1]\n",
    "        break\n",
    "print('Reward:', R)\n",
    "# plt.plot(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8), constrained_layout=True)\n",
    "gs = fig.add_gridspec(3, 1)\n",
    "ax = fig.add_subplot(gs[0:2, 0], projection='3d')\n",
    "ax.plot(pos_[::10, 0], pos_[::10, 1], pos_[::10, 2], 'r.-', label=label_)\n",
    "ax.text(*env.start, \"start\")\n",
    "ax.text(*env.end, \"end\")\n",
    "ax_lims = np.asarray([ax.get_xlim3d(), ax.get_ylim3d(), ax.get_zlim3d()])\n",
    "ax.set_box_aspect(np.ptp(ax_lims, axis=1))\n",
    "ax.legend()\n",
    "\n",
    "ax = fig.add_subplot(gs[2:, 0])\n",
    "ax.plot(pos_[:, 0], 'r:', label='x')\n",
    "ax.plot(pos_[:, 1], 'r-', label='y')\n",
    "ax.plot(pos_[:, 2], 'r--', label='z')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_params = dict(\n",
    "    state_dim=env.observation_space.shape[0],\n",
    "    action_dim=env.action_space.shape[0],\n",
    "    n_latent_var=64,\n",
    "    activation=torch.nn.Tanh,  # final layer activation\n",
    "    lr=0.02,\n",
    "    epochs=25,\n",
    "    update_interval=2000\n",
    ")\n",
    "\n",
    "REWARDS = []\n",
    "AGENTS = []\n",
    "LABELS = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn supervision - no fault\n",
    "env = QuadcopterSupervisorEnv(Controller(Quadcopter()), seed=0, deterministic_reset=False)\n",
    "env.reset(position=(0,0,5), target=(3,3,5), linear_rate=(0,0,0), orientation=(0,0,0), angular_rate=(0,0,0))\n",
    "agent = PPO(env, ActorCriticBox, **ppo_params)\n",
    "for t in agent.policy.parameters():\n",
    "    torch.nn.init.normal_(t, 0., 0.01)\n",
    "AGENTS.append(agent)\n",
    "REWARDS.append(agent.learn(25000))\n",
    "LABELS.append('No fault')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn supervision\n",
    "env = QuadcopterSupervisorEnv(Controller(Quadcopter()), seed=0, deterministic_reset=False)\n",
    "env.ctrl.quadcopter.set_motor_faults([0, 0, 0, 0.20])\n",
    "env.reset(position=(0,0,5), target=(3,3,5), linear_rate=(0,0,0), orientation=(0,0,0), angular_rate=(0,0,0))\n",
    "agent = PPO(env, ActorCriticBox, **ppo_params)\n",
    "for t in agent.policy.parameters():\n",
    "    torch.nn.init.normal_(t, 0., 0.01)\n",
    "AGENTS.append(agent)\n",
    "REWARDS.append(agent.learn(25000))\n",
    "LABELS.append('M4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for (rewards, label) in zip(REWARDS, LABELS):\n",
    "    plt.scatter(np.arange(len(rewards)), rewards, label=label)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = QuadcopterSupervisorEnv(Controller(Quadcopter()), deterministic_reset=True)\n",
    "env.reset(position=(0,0,25), target=(10,10,10), linear_rate=(0,0,0), orientation=(0,0,0), angular_rate=(0,0,0))\n",
    "env.ctrl.quadcopter.set_motor_faults([0, 0, 0, 0])\n",
    "positions = plot_quadcopter(env, *AGENTS, labels=LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PPO(env, ActorCriticBox, **ppo_params)\n",
    "for t in agent.policy.parameters():\n",
    "    pass\n",
    "    torch.nn.init.normal_(t, 0., 1e-9)\n",
    "    print(t)\n",
    "s = env.reset()\n",
    "for t in range(20):\n",
    "    a, _ = agent.predict(s)\n",
    "    env.step(a)\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = QuadcopterSupervisorEnv(Controller(Quadcopter(), params=CONTROLLER_PARAMS), deterministic_reset=True)\n",
    "env.reset(position=(0,0,0), target=(0,0,5), linear_rate=(0,0,0), orientation=(0,0,0), angular_rate=(0,0,0))\n",
    "\n",
    "class PIDAgent(PPO):\n",
    "    def __init__(self, env, **kwargs):\n",
    "        self.env = env\n",
    "        self.ctrl = env.ctrl\n",
    "        self.gamma = 0.99\n",
    "    def predict(self, state):\n",
    "        return self.ctrl.get_control(), 1.0\n",
    "\n",
    "pida = PIDAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each fault, \n",
    "faults = [\n",
    "    [0, 0, 0, 0.20],\n",
    "    [0, 0, 0.1, 0.20],\n",
    "    [0, 0.2, 0, 0.20],\n",
    "    [0.1, 0, 0, 0.20],\n",
    "    [0.1, 0, 0.2, 0.20],\n",
    "]\n",
    "env.reset()\n",
    "for fault in faults:\n",
    "    env.ctrl.quadcopter.set_motor_faults(fault)\n",
    "    a = PIDAgent(env, **ppo_params)\n",
    "    m = Memory()\n",
    "    a.experience(m, ppo_params['update_interval'], env, a)\n",
    "\n",
    "    ret = torch.tensor(returns(m.rewards, m.is_terminals, a.gamma)).float().to(DEVICE)\n",
    "    ret = (ret - ret.mean()) / (ret.std() + 1e-5)\n",
    "    states = torch.tensor(m.states).float().to(DEVICE).detach()\n",
    "    actions = torch.tensor(m.actions).float().to(DEVICE).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, distances, _ = prune_library(library, len(library), m, **ppo_params)\n",
    "affinities = np.exp(-distances / distances.std())\n",
    "print(distances)\n",
    "\n",
    "_, expected_returns = rank_policies(m, library, **ppo_params)\n",
    "print(expected_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN, SpectralClustering, AffinityPropagation\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "\n",
    "# normalized metrics\n",
    "distancen = distances / distances.max()\n",
    "affinityn = affinities / affinities.max()\n",
    "\n",
    "clusterers = [\n",
    "    (DBSCAN(eps=0.5, min_samples=2, metric='precomputed'), distancen, 'DBSCAN'),\n",
    "    (SpectralClustering(n_clusters=2, affinity='precomputed'), affinityn, 'Spectral'),\n",
    "    (AffinityPropagation(affinity='precomputed', random_state=SEED), affinityn, 'Affinity')\n",
    "]\n",
    "\n",
    "res = dict(expected_returns=expected_returns)\n",
    "for clusterer, data, name in clusterers:\n",
    "    labels = clusterer.fit_predict(data)\n",
    "    res[name] = labels\n",
    "res = pd.DataFrame(res)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stable baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO as PPO2\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_fn():\n",
    "    env = QuadcopterSupervisorEnv(Controller(Quadcopter()), seed=i, deterministic_reset=False)\n",
    "#     env.ctrl.quadcopter.set_motor_faults([0, 0, 0, 0.25])\n",
    "    return env\n",
    "envs = SubprocVecEnv([lambda: QuadcopterSupervisorEnv(Controller(Quadcopter()), seed=i, deterministic_reset=False) for i in range(4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = PPO2(MlpPolicy, envs, verbose=1)\n",
    "a.learn(total_timesteps=50000)\n",
    "envs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGENTS.append(a)\n",
    "REWARDS.append([])\n",
    "LABELS.append('Baselines')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
