{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.distributions import Bernoulli, MultivariateNormal, Categorical\n",
    "import gym\n",
    "\n",
    "import notebook_setup\n",
    "from tqdm.auto import tqdm, trange\n",
    "from systems import CartPoleEnv\n",
    "from systems import CartPoleContinuousEnv\n",
    "from ppo import ActorCriticDiscrete, ActorCriticMultiBinary, ActorCriticBox, PPO, DEVICE, Memory, returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ppo_params = dict(\n",
    "    state_dim=4,\n",
    "    action_dim=2,\n",
    "    n_latent_var=32,\n",
    "    lr=0.02,\n",
    "    epochs=5,\n",
    "    update_interval=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "agent = PPO(CartPoleEnv(), ActorCriticDiscrete, **ppo_params)\n",
    "rewards = agent.learn(30000)\n",
    "plt.scatter(np.arange(len(rewards)), rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ActorCriticDiscrete(state_dim=4, action_dim=2, n_latent_var=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### LunarLander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "\n",
    "ppo_params = dict(\n",
    "    state_dim=env.observation_space.shape[0],\n",
    "    action_dim=env.action_space.shape[0],\n",
    "    n_latent_var=64,\n",
    "    lr=0.0003,\n",
    "    epochs=75,\n",
    "    update_interval=3000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "agent = PPO(env, ActorCriticBox, **ppo_params)\n",
    "rewards = agent.learn(3000)\n",
    "plt.scatter(np.arange(len(rewards)), rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### CartPole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env = CartPoleContinuousEnv()\n",
    "\n",
    "ppo_params = dict(\n",
    "    state_dim=env.observation_space.shape[0],\n",
    "    action_dim=env.action_space.shape[0],\n",
    "    n_latent_var=32,\n",
    "    lr=0.02,\n",
    "    epochs=25,\n",
    "    update_interval=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "agent = PPO(env, ActorCriticBox, **ppo_params)\n",
    "rewards = agent.learn(10000)\n",
    "plt.scatter(np.arange(len(rewards)), rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Discretized Continuous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class ActorCriticBoxDiscrete(ActorCriticBox):\n",
    "    \n",
    "    def predict(self, state):\n",
    "        action, logprob = super().predict(state)\n",
    "        return int(np.round(np.clip(action.item(), 0, 1))), logprob\n",
    "\n",
    "    def evaluate(self, state, action):\n",
    "        action_logprobs, state_value, dist_entropy = \\\n",
    "            super().evaluate(state, action)\n",
    "        return action_logprobs, state_value, dist_entropy\n",
    "\n",
    "env = CartPoleEnv()\n",
    "\n",
    "ppo_params = dict(\n",
    "    state_dim=4,\n",
    "    action_dim=1,\n",
    "    n_latent_var=64,\n",
    "    lr=0.002,\n",
    "    epochs=50,\n",
    "    update_interval=500\n",
    ")\n",
    "\n",
    "agent = PPO(env, ActorCriticBoxDiscrete, **ppo_params)\n",
    "rewards = agent.learn(10000)\n",
    "plt.scatter(np.arange(len(rewards)), rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quadcopter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from systems.quadcopter import (Quadcopter, QuadcopterSupervisorEnv, Controller,\n",
    "                                plot_quadcopter, QUADPARAMS, CONTROLLER_PARAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/quadcopter_schematic.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUADPARAMS = {\n",
    "    'position': [0, 0, 0],      # (metres), optional, defalts to 0\n",
    "    'orientation': [0, 0, 0],   # (degrees), optional, defaults to 0\n",
    "    'ground_level': -np.inf,    # (metres), optional, location of ground plane\n",
    "    'r': 0.1,                   # (metres), Radius of sphere representing centre of quadcopter\n",
    "    'L': 0.3,                   # (metres), length of arm\n",
    "    'prop_size': [10, 4.5],     # (inches), diameter & pitch of rotors\n",
    "    'mass': 1.2                 # (kilograms)\n",
    "    }\n",
    "\n",
    "CONTROLLER_PARAMS = {\n",
    "    'Motor_limits': [4000, 9000],        # rpm\n",
    "    'Tilt_limits': [-10, 10],            # degrees\n",
    "    'Yaw_Control_Limits': [-900, 900],\n",
    "    'Z_XY_offset': 500,\n",
    "    'Linear_To_Angular_Scaler': [1, 1, 0],\n",
    "    'Yaw_Rate_Scaler': 0.18,\n",
    "    'Linear_PID': {\n",
    "        'P':[300, 300, 7000],\n",
    "        'I':[0.04, 0.04, 4.5],\n",
    "        'D':[450, 450, 5000]},\n",
    "    'Angular_PID':{\n",
    "        'P':[22000, 22000, 1500],\n",
    "        'I':[0, 0, 1.2],\n",
    "        'D':[12000, 12000, 0]},\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = QuadcopterSupervisorEnv(\n",
    "            Controller(\n",
    "                Quadcopter(turbulence=0.1),\n",
    "                params=CONTROLLER_PARAMS,\n",
    "                ignore_yaw=False),\n",
    "            deterministic_reset=True,\n",
    "            dt=5e-2)\n",
    "\n",
    "env.reset(position=(0,0,0),\n",
    "          target=(5,5,5),\n",
    "          linear_rate=(0,0,0),\n",
    "          orientation=(0,0,0),\n",
    "          angular_rate=(0,0,0))\n",
    "\n",
    "env.max_n = 1000\n",
    "env.ctrl.quadcopter.setNormalWind(np.asarray([0, 0, 0]))\n",
    "\n",
    "T = 5000\n",
    "label_ = ''\n",
    "pos_ = np.zeros((T, 3))\n",
    "env.reset()\n",
    "env.ctrl.quadcopter.set_motor_faults([0, 0, 0, 0])\n",
    "pos_[0] = env.start\n",
    "R = 0.\n",
    "rewards = []\n",
    "for t in trange(1, T, leave=False):\n",
    "    _, r, done, _ = env.step(0)\n",
    "    rewards.append(r)\n",
    "    R += r\n",
    "    pos_[t] = env.ctrl.quadcopter.state[:3]\n",
    "    if done:\n",
    "        pos_ = pos_[:t+1]\n",
    "        break\n",
    "print('Total reward:', R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_quadcopter(env);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8), constrained_layout=True)\n",
    "gs = fig.add_gridspec(3, 1)\n",
    "ax = fig.add_subplot(gs[0:2, 0], projection='3d')\n",
    "ax.plot(pos_[::10, 0], pos_[::10, 1], pos_[::10, 2], 'r.-', label=label_)\n",
    "ax.text(*env.start, \"start\")\n",
    "ax.text(*env.end, \"end\")\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z')\n",
    "ax_lims = np.asarray([ax.get_xlim3d(), ax.get_ylim3d(), ax.get_zlim3d()])\n",
    "ax.set_box_aspect(np.ptp(ax_lims, axis=1))\n",
    "ax.legend()\n",
    "ax.view_init(90, 90)\n",
    "\n",
    "ax = fig.add_subplot(gs[2:, 0])\n",
    "ax.plot(pos_[:, 0], ':', label='x')\n",
    "ax.plot(pos_[:, 1], '-', label='y')\n",
    "ax.plot(pos_[:, 2], '--', label='z')\n",
    "ax.set_xlabel('Simulation steps')\n",
    "ax.set_ylabel('Distance /m')\n",
    "ax.legend()\n",
    "print('Final z', pos_[-1, 2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RL interface\n",
    "\n",
    "# Actions\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# States\n",
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advance simulation\n",
    "state, reward, done, extra = env.step(action=0.)\n",
    "print('state\\n', state)\n",
    "print('reward', reward)\n",
    "print('done?', done)\n",
    "print('extra', extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Core loop\n",
    "done = False\n",
    "env.reset()\n",
    "while not done:\n",
    "    state, reward, done, extra = env.step(action=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ppo_params = dict(\n",
    "    state_dim=12,\n",
    "    action_dim=4,\n",
    "    n_latent_var=6,\n",
    "    activation=torch.nn.Tanh,  # final layer activation,\n",
    "    action_std=0.01,\n",
    "    lr=0.05,\n",
    "    epochs=25,\n",
    "    update_interval=2000,\n",
    ")\n",
    "\n",
    "REWARDS = {}\n",
    "AGENTS = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Learn supervision - no fault\n",
    "env = QuadcopterSupervisorEnv(Controller(Quadcopter()), seed=0, deterministic_reset=True)\n",
    "env.reset(position=(0,0,0), target=(3,4,5), linear_rate=(0,0,0), orientation=(0,0,0), angular_rate=(0,0,0))\n",
    "agent = PPO(env, ActorCriticBox, **ppo_params)\n",
    "# for t in agent.policy.parameters():\n",
    "#     torch.nn.init.normal_(t, 0., 0.01)\n",
    "AGENTS['No fault'] = agent\n",
    "REWARDS['No fault'] = agent.learn(25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "REWARDS['No fault'].extend(AGENTS['No fault'].learn(25000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Learn supervision\n",
    "motor, fault = 3, 0.3\n",
    "faults = np.zeros(4)\n",
    "faults[motor] = fault\n",
    "env = QuadcopterSupervisorEnv(Controller(Quadcopter()), seed=0, deterministic_reset=False)\n",
    "env.ctrl.quadcopter.set_motor_faults(faults)\n",
    "env.reset(position=(0,0,0), target=(3,3,5), linear_rate=(0,0,0), orientation=(0,0,0), angular_rate=(0,0,0))\n",
    "agent = PPO(env, ActorCriticBox, **ppo_params)\n",
    "for t in agent.policy.parameters():\n",
    "    torch.nn.init.normal_(t, 0., 0.01)\n",
    "AGENTS[f'm{motor}({fault})'] = agent\n",
    "REWARDS[f'm{motor}({fault})'] = agent.learn(30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot rewards\n",
    "for (label, rewards) in REWARDS.items():\n",
    "    if rewards is None: continue\n",
    "    plt.scatter(np.arange(len(rewards)), rewards, label=label)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "motor, fault = 3, 0.0\n",
    "faults = np.zeros(4)\n",
    "faults[motor] = fault\n",
    "env = QuadcopterSupervisorEnv(Controller(Quadcopter()), deterministic_reset=True)\n",
    "env.ctrl.quadcopter.set_motor_faults(faults)\n",
    "env.reset(position=(0,0,0), target=(3,3,5), linear_rate=(0,0,0), orientation=(0,0,0), angular_rate=(0,0,0))\n",
    "\n",
    "positions, velocities, actions, rewards = plot_quadcopter(env,\n",
    "                                                          *[*AGENTS.values(), None],\n",
    "                                                          labels=[*AGENTS.keys(), 'Unsupervised'])\n",
    "plt.suptitle(f'Env - m{motor}({fault})')\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "env = QuadcopterSupervisorEnv(Controller(Quadcopter(), params=CONTROLLER_PARAMS), deterministic_reset=True)\n",
    "env.reset(position=(0,0,0), target=(0,0,5), linear_rate=(0,0,0), orientation=(0,0,0), angular_rate=(0,0,0))\n",
    "\n",
    "class PIDAgent(PPO):\n",
    "    def __init__(self, env, **kwargs):\n",
    "        self.env = env\n",
    "        self.ctrl = env.ctrl\n",
    "        self.gamma = 0.99\n",
    "    def predict(self, state):\n",
    "        return self.ctrl.get_control(), 1.0\n",
    "\n",
    "pida = PIDAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# For each fault, \n",
    "faults = [\n",
    "    [0, 0, 0, 0.20],\n",
    "    [0, 0, 0.1, 0.20],\n",
    "    [0, 0.2, 0, 0.20],\n",
    "    [0.1, 0, 0, 0.20],\n",
    "    [0.1, 0, 0.2, 0.20],\n",
    "]\n",
    "env.reset()\n",
    "for fault in faults:\n",
    "    env.ctrl.quadcopter.set_motor_faults(fault)\n",
    "    a = PIDAgent(env, **ppo_params)\n",
    "    m = Memory()\n",
    "    a.experience(m, ppo_params['update_interval'], env, a)\n",
    "\n",
    "    ret = torch.tensor(returns(m.rewards, m.is_terminals, a.gamma)).float().to(DEVICE)\n",
    "    ret = (ret - ret.mean()) / (ret.std() + 1e-5)\n",
    "    states = torch.tensor(m.states).float().to(DEVICE).detach()\n",
    "    actions = torch.tensor(m.actions).float().to(DEVICE).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_, distances, _ = prune_library(library, len(library), m, **ppo_params)\n",
    "affinities = np.exp(-distances / distances.std())\n",
    "print(distances)\n",
    "\n",
    "_, expected_returns = rank_policies(m, library, **ppo_params)\n",
    "print(expected_returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN, SpectralClustering, AffinityPropagation\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "\n",
    "# normalized metrics\n",
    "distancen = distances / distances.max()\n",
    "affinityn = affinities / affinities.max()\n",
    "\n",
    "clusterers = [\n",
    "    (DBSCAN(eps=0.5, min_samples=2, metric='precomputed'), distancen, 'DBSCAN'),\n",
    "    (SpectralClustering(n_clusters=2, affinity='precomputed'), affinityn, 'Spectral'),\n",
    "    (AffinityPropagation(affinity='precomputed', random_state=SEED), affinityn, 'Affinity')\n",
    "]\n",
    "\n",
    "res = dict(expected_returns=expected_returns)\n",
    "for clusterer, data, name in clusterers:\n",
    "    labels = clusterer.fit_predict(data)\n",
    "    res[name] = labels\n",
    "res = pd.DataFrame(res)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Stable baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO as PPO2\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def env_fn():\n",
    "    env = QuadcopterSupervisorEnv(Controller(Quadcopter()), seed=i, deterministic_reset=False)\n",
    "#     env.ctrl.quadcopter.set_motor_faults([0, 0, 0, 0.25])\n",
    "    return env\n",
    "envs = SubprocVecEnv([lambda: QuadcopterSupervisorEnv(Controller(Quadcopter()), seed=i, deterministic_reset=True) for i in range(4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = PPO2(MlpPolicy, envs, verbose=1)\n",
    "a.learn(total_timesteps=50000)\n",
    "envs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "AGENTS.append(a)\n",
    "REWARDS.append([])\n",
    "LABELS.append('Baselines')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
